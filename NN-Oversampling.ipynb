{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fd33643"
      },
      "source": [
        "# Program Synthesis for Dataset Augmentation with Symbolic Regression and Genetic Programming"
      ],
      "id": "0fd33643"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2649232b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4c98d9-d546-4713-d57c-4aa07838e9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gplearn in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gplearn) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from gplearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Ruchi Bhalani, rb44675\n",
        "!pip install gplearn"
      ],
      "id": "2649232b"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8ZMR0CqzTxX",
        "outputId": "1b88b492-ff6c-4833-8161-8c7d22ef491c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "A8ZMR0CqzTxX"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ab1fe226"
      },
      "outputs": [],
      "source": [
        "# headers\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from collections import OrderedDict\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "ab1fe226"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cc9c4481"
      },
      "outputs": [],
      "source": [
        "# function to calculate adjusted r2\n",
        "def get_adj_r2(r2, n, p):\n",
        "    return (1-(1-r2)*((n-1)/(n-p-1)))"
      ],
      "id": "cc9c4481"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7c7806"
      },
      "source": [
        "# Assignment 2: Regression and KNN classifier"
      ],
      "id": "7a7c7806"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bb47879"
      },
      "source": [
        "**Data Prep**\n"
      ],
      "id": "8bb47879"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "589c05de",
        "outputId": "a628a36a-e595-497c-a828-c3c2b45dcb4c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n",
            "ONLY MINORITY:  442\n",
            "ONLY MINORITY:  50\n",
            "0    284315\n",
            "1       492\n",
            "Name: Class, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Time         V1         V2         V3         V4         V5  \\\n",
              "15166   26523.0 -18.474868  11.586381 -21.402917   6.038515 -14.451158   \n",
              "119714  75556.0  -0.734303   0.435519  -0.530866  -0.471120   0.643214   \n",
              "9035    12597.0  -2.589617   7.016714 -13.705407  10.343228  -2.954461   \n",
              "42007   40918.0  -3.140260   3.367342  -2.778931   3.859701  -1.159518   \n",
              "42756   41233.0 -10.645800   5.918307 -11.671043   8.807369  -7.975501   \n",
              "\n",
              "              V6         V7         V8        V9  ...       V21       V22  \\\n",
              "15166  -4.146524 -14.856124  12.431140 -4.053353  ...  1.741136 -1.251138   \n",
              "119714  0.713832  -1.234572  -2.551412 -2.057724  ... -1.004877  1.150354   \n",
              "9035   -3.055116  -9.301289   3.349573 -5.654212  ...  1.887738  0.333998   \n",
              "42007  -0.721552  -4.195342  -0.598346 -2.870145  ...  2.452339 -0.292963   \n",
              "42756  -3.586806 -13.616797   6.428169 -7.368451  ...  2.571970  0.206809   \n",
              "\n",
              "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
              "15166  -0.396219  0.095706  1.322751 -0.217955  1.628793  0.482248   99.99   \n",
              "119714 -0.152555 -1.386745  0.004716  0.219146 -0.058257  0.158048   29.95   \n",
              "9035    0.287659 -1.186406 -0.690273  0.631704  1.934221  0.789687    1.00   \n",
              "42007  -0.189330 -0.166482  0.038040 -0.015477  0.776691  0.397557    0.76   \n",
              "42756  -1.667801  0.558419 -0.027898  0.354254  0.273329 -0.152908    0.00   \n",
              "\n",
              "        Class  \n",
              "15166       1  \n",
              "119714      1  \n",
              "9035        1  \n",
              "42007       1  \n",
              "42756       1  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cff28d6e-a2ca-4aa9-9269-42bc064017df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15166</th>\n",
              "      <td>26523.0</td>\n",
              "      <td>-18.474868</td>\n",
              "      <td>11.586381</td>\n",
              "      <td>-21.402917</td>\n",
              "      <td>6.038515</td>\n",
              "      <td>-14.451158</td>\n",
              "      <td>-4.146524</td>\n",
              "      <td>-14.856124</td>\n",
              "      <td>12.431140</td>\n",
              "      <td>-4.053353</td>\n",
              "      <td>...</td>\n",
              "      <td>1.741136</td>\n",
              "      <td>-1.251138</td>\n",
              "      <td>-0.396219</td>\n",
              "      <td>0.095706</td>\n",
              "      <td>1.322751</td>\n",
              "      <td>-0.217955</td>\n",
              "      <td>1.628793</td>\n",
              "      <td>0.482248</td>\n",
              "      <td>99.99</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119714</th>\n",
              "      <td>75556.0</td>\n",
              "      <td>-0.734303</td>\n",
              "      <td>0.435519</td>\n",
              "      <td>-0.530866</td>\n",
              "      <td>-0.471120</td>\n",
              "      <td>0.643214</td>\n",
              "      <td>0.713832</td>\n",
              "      <td>-1.234572</td>\n",
              "      <td>-2.551412</td>\n",
              "      <td>-2.057724</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.004877</td>\n",
              "      <td>1.150354</td>\n",
              "      <td>-0.152555</td>\n",
              "      <td>-1.386745</td>\n",
              "      <td>0.004716</td>\n",
              "      <td>0.219146</td>\n",
              "      <td>-0.058257</td>\n",
              "      <td>0.158048</td>\n",
              "      <td>29.95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9035</th>\n",
              "      <td>12597.0</td>\n",
              "      <td>-2.589617</td>\n",
              "      <td>7.016714</td>\n",
              "      <td>-13.705407</td>\n",
              "      <td>10.343228</td>\n",
              "      <td>-2.954461</td>\n",
              "      <td>-3.055116</td>\n",
              "      <td>-9.301289</td>\n",
              "      <td>3.349573</td>\n",
              "      <td>-5.654212</td>\n",
              "      <td>...</td>\n",
              "      <td>1.887738</td>\n",
              "      <td>0.333998</td>\n",
              "      <td>0.287659</td>\n",
              "      <td>-1.186406</td>\n",
              "      <td>-0.690273</td>\n",
              "      <td>0.631704</td>\n",
              "      <td>1.934221</td>\n",
              "      <td>0.789687</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42007</th>\n",
              "      <td>40918.0</td>\n",
              "      <td>-3.140260</td>\n",
              "      <td>3.367342</td>\n",
              "      <td>-2.778931</td>\n",
              "      <td>3.859701</td>\n",
              "      <td>-1.159518</td>\n",
              "      <td>-0.721552</td>\n",
              "      <td>-4.195342</td>\n",
              "      <td>-0.598346</td>\n",
              "      <td>-2.870145</td>\n",
              "      <td>...</td>\n",
              "      <td>2.452339</td>\n",
              "      <td>-0.292963</td>\n",
              "      <td>-0.189330</td>\n",
              "      <td>-0.166482</td>\n",
              "      <td>0.038040</td>\n",
              "      <td>-0.015477</td>\n",
              "      <td>0.776691</td>\n",
              "      <td>0.397557</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42756</th>\n",
              "      <td>41233.0</td>\n",
              "      <td>-10.645800</td>\n",
              "      <td>5.918307</td>\n",
              "      <td>-11.671043</td>\n",
              "      <td>8.807369</td>\n",
              "      <td>-7.975501</td>\n",
              "      <td>-3.586806</td>\n",
              "      <td>-13.616797</td>\n",
              "      <td>6.428169</td>\n",
              "      <td>-7.368451</td>\n",
              "      <td>...</td>\n",
              "      <td>2.571970</td>\n",
              "      <td>0.206809</td>\n",
              "      <td>-1.667801</td>\n",
              "      <td>0.558419</td>\n",
              "      <td>-0.027898</td>\n",
              "      <td>0.354254</td>\n",
              "      <td>0.273329</td>\n",
              "      <td>-0.152908</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cff28d6e-a2ca-4aa9-9269-42bc064017df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cff28d6e-a2ca-4aa9-9269-42bc064017df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cff28d6e-a2ca-4aa9-9269-42bc064017df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# NOTE: THESE VALUES ARE MEANT TO BE CUSTOMIZABLE BY THE USER\n",
        "# read in data\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/medical-charges.txt\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/breast_cancer.csv\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/diabetes.csv\"\n",
        "USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/creditcard.csv\"\n",
        "EXPECTED_DISTINCT_PERCENTAGE = 0.85\n",
        "\n",
        "# How many entries do we want to augment this dataset by?\n",
        "# TODO: Let the user pass this in as a percentage as well\n",
        "NEW_EXAMPLES = 10\n",
        "\n",
        "# data = pd.read_csv(USER_INPUT_DATASET, header = 0)\n",
        "df = pd.read_csv(USER_INPUT_DATASET, header=\"infer\")\n",
        "print(df.info())\n",
        "# print(df)\n",
        "\n",
        "\n",
        "# DATA PREPROCESSING\n",
        "df = df.dropna(axis=1, how='all')\n",
        "# df = df.dropna()\n",
        "df = df.fillna(0)\n",
        "\n",
        "# df = df.sample(1000)\n",
        "# original_df = df\n",
        "\n",
        "# Split the data into train and test sets\n",
        "minority_class = df['Class'].value_counts().idxmin()\n",
        "minority_df = df[df['Class'] == minority_class]\n",
        "majority_df = df[df['Class'] != minority_class]\n",
        "\n",
        "minority_train, minority_test  = train_test_split(minority_df, test_size=0.1)\n",
        "\n",
        "#concatenate majority train with the generated stuff (df)\n",
        "majority_train, majority_test  = train_test_split(majority_df, test_size=0.1)\n",
        "\n",
        "# this is the original test and train data\n",
        "test_data = pd.concat([minority_test, majority_test])\n",
        "train_df = pd.concat([minority_train, majority_train])\n",
        "\n",
        "# Split the train set into two dataframes based on the class labels\n",
        "train_data = train_df[train_df['Class'] == minority_class]\n",
        "train_majority_df = train_df[train_df['Class'] != minority_class]\n",
        "\n",
        "print(\"ONLY MINORITY: \", len(train_data))\n",
        "print(\"ONLY MINORITY: \", len(minority_test))\n",
        "print(df['Class'].value_counts())\n",
        "\n",
        "# train_data, test_data = train_test_split(df, test_size=0.2, shuffle=False)\n",
        "# this contains the augmented oversampled data\n",
        "df = train_data\n",
        "\n",
        "# Separate them into numerical, categorical, and proper noun columns (regex generation)\n",
        "numerical_cols = list()\n",
        "categorical_cols = list()\n",
        "proper_noun_cols = list()\n",
        "for (index, colname) in enumerate(df):\n",
        "    if colname in df.select_dtypes(include='object').columns:\n",
        "      unique_vals = df[colname].unique()\n",
        "      if len(unique_vals) >= EXPECTED_DISTINCT_PERCENTAGE * df.shape[0]:\n",
        "        proper_noun_cols.append(colname)\n",
        "      else:\n",
        "        categorical_cols.append(colname)\n",
        "    else:\n",
        "      numerical_cols.append(colname)\n",
        "\n",
        "\n",
        "# print(categorical_cols)\n",
        "# print(numerical_cols)\n",
        "# print(\"FINAL STEP: proper nouns --\", proper_noun_cols)\n",
        "\n",
        "df.head()\n"
      ],
      "id": "589c05de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3043bb91"
      },
      "source": [
        "There are several categorical columns. We need to transform these to be able to do regression. "
      ],
      "id": "3043bb91"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "0484464d",
        "outputId": "8f4309ed-0c28-4652-eddf-82c9420c7d98",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Time         V1         V2         V3         V4         V5  \\\n",
              "15166   26523.0 -18.474868  11.586381 -21.402917   6.038515 -14.451158   \n",
              "119714  75556.0  -0.734303   0.435519  -0.530866  -0.471120   0.643214   \n",
              "9035    12597.0  -2.589617   7.016714 -13.705407  10.343228  -2.954461   \n",
              "42007   40918.0  -3.140260   3.367342  -2.778931   3.859701  -1.159518   \n",
              "42756   41233.0 -10.645800   5.918307 -11.671043   8.807369  -7.975501   \n",
              "\n",
              "              V6         V7         V8        V9  ...       V21       V22  \\\n",
              "15166  -4.146524 -14.856124  12.431140 -4.053353  ...  1.741136 -1.251138   \n",
              "119714  0.713832  -1.234572  -2.551412 -2.057724  ... -1.004877  1.150354   \n",
              "9035   -3.055116  -9.301289   3.349573 -5.654212  ...  1.887738  0.333998   \n",
              "42007  -0.721552  -4.195342  -0.598346 -2.870145  ...  2.452339 -0.292963   \n",
              "42756  -3.586806 -13.616797   6.428169 -7.368451  ...  2.571970  0.206809   \n",
              "\n",
              "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
              "15166  -0.396219  0.095706  1.322751 -0.217955  1.628793  0.482248   99.99   \n",
              "119714 -0.152555 -1.386745  0.004716  0.219146 -0.058257  0.158048   29.95   \n",
              "9035    0.287659 -1.186406 -0.690273  0.631704  1.934221  0.789687    1.00   \n",
              "42007  -0.189330 -0.166482  0.038040 -0.015477  0.776691  0.397557    0.76   \n",
              "42756  -1.667801  0.558419 -0.027898  0.354254  0.273329 -0.152908    0.00   \n",
              "\n",
              "        Class  \n",
              "15166       1  \n",
              "119714      1  \n",
              "9035        1  \n",
              "42007       1  \n",
              "42756       1  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-029f1fb1-8e50-4f21-8d0c-21ed05c00dd5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15166</th>\n",
              "      <td>26523.0</td>\n",
              "      <td>-18.474868</td>\n",
              "      <td>11.586381</td>\n",
              "      <td>-21.402917</td>\n",
              "      <td>6.038515</td>\n",
              "      <td>-14.451158</td>\n",
              "      <td>-4.146524</td>\n",
              "      <td>-14.856124</td>\n",
              "      <td>12.431140</td>\n",
              "      <td>-4.053353</td>\n",
              "      <td>...</td>\n",
              "      <td>1.741136</td>\n",
              "      <td>-1.251138</td>\n",
              "      <td>-0.396219</td>\n",
              "      <td>0.095706</td>\n",
              "      <td>1.322751</td>\n",
              "      <td>-0.217955</td>\n",
              "      <td>1.628793</td>\n",
              "      <td>0.482248</td>\n",
              "      <td>99.99</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119714</th>\n",
              "      <td>75556.0</td>\n",
              "      <td>-0.734303</td>\n",
              "      <td>0.435519</td>\n",
              "      <td>-0.530866</td>\n",
              "      <td>-0.471120</td>\n",
              "      <td>0.643214</td>\n",
              "      <td>0.713832</td>\n",
              "      <td>-1.234572</td>\n",
              "      <td>-2.551412</td>\n",
              "      <td>-2.057724</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.004877</td>\n",
              "      <td>1.150354</td>\n",
              "      <td>-0.152555</td>\n",
              "      <td>-1.386745</td>\n",
              "      <td>0.004716</td>\n",
              "      <td>0.219146</td>\n",
              "      <td>-0.058257</td>\n",
              "      <td>0.158048</td>\n",
              "      <td>29.95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9035</th>\n",
              "      <td>12597.0</td>\n",
              "      <td>-2.589617</td>\n",
              "      <td>7.016714</td>\n",
              "      <td>-13.705407</td>\n",
              "      <td>10.343228</td>\n",
              "      <td>-2.954461</td>\n",
              "      <td>-3.055116</td>\n",
              "      <td>-9.301289</td>\n",
              "      <td>3.349573</td>\n",
              "      <td>-5.654212</td>\n",
              "      <td>...</td>\n",
              "      <td>1.887738</td>\n",
              "      <td>0.333998</td>\n",
              "      <td>0.287659</td>\n",
              "      <td>-1.186406</td>\n",
              "      <td>-0.690273</td>\n",
              "      <td>0.631704</td>\n",
              "      <td>1.934221</td>\n",
              "      <td>0.789687</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42007</th>\n",
              "      <td>40918.0</td>\n",
              "      <td>-3.140260</td>\n",
              "      <td>3.367342</td>\n",
              "      <td>-2.778931</td>\n",
              "      <td>3.859701</td>\n",
              "      <td>-1.159518</td>\n",
              "      <td>-0.721552</td>\n",
              "      <td>-4.195342</td>\n",
              "      <td>-0.598346</td>\n",
              "      <td>-2.870145</td>\n",
              "      <td>...</td>\n",
              "      <td>2.452339</td>\n",
              "      <td>-0.292963</td>\n",
              "      <td>-0.189330</td>\n",
              "      <td>-0.166482</td>\n",
              "      <td>0.038040</td>\n",
              "      <td>-0.015477</td>\n",
              "      <td>0.776691</td>\n",
              "      <td>0.397557</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42756</th>\n",
              "      <td>41233.0</td>\n",
              "      <td>-10.645800</td>\n",
              "      <td>5.918307</td>\n",
              "      <td>-11.671043</td>\n",
              "      <td>8.807369</td>\n",
              "      <td>-7.975501</td>\n",
              "      <td>-3.586806</td>\n",
              "      <td>-13.616797</td>\n",
              "      <td>6.428169</td>\n",
              "      <td>-7.368451</td>\n",
              "      <td>...</td>\n",
              "      <td>2.571970</td>\n",
              "      <td>0.206809</td>\n",
              "      <td>-1.667801</td>\n",
              "      <td>0.558419</td>\n",
              "      <td>-0.027898</td>\n",
              "      <td>0.354254</td>\n",
              "      <td>0.273329</td>\n",
              "      <td>-0.152908</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-029f1fb1-8e50-4f21-8d0c-21ed05c00dd5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-029f1fb1-8e50-4f21-8d0c-21ed05c00dd5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-029f1fb1-8e50-4f21-8d0c-21ed05c00dd5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# we'll deal with the proper noun columns at the end\n",
        "regression_df = df.drop(columns=proper_noun_cols)\n",
        "regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "regression_df.head()"
      ],
      "id": "0484464d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38a13dc1"
      },
      "source": [
        "An interesting thing to check with regression problems is whether any of the individual features correlate very strongly with the label."
      ],
      "id": "38a13dc1"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad660fd",
        "outputId": "4915a0a3-74a0-471c-a6d0-a4ea58150a3a",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Time        V1        V2        V3        V4        V5        V6  \\\n",
            "Time    1.000000  0.283974 -0.261868  0.221733 -0.183578  0.337220  0.165930   \n",
            "V1      0.283974  1.000000 -0.828858  0.909019 -0.576283  0.898603  0.252016   \n",
            "V2     -0.261868 -0.828858  1.000000 -0.886474  0.645409 -0.840797 -0.174114   \n",
            "V3      0.221733  0.909019 -0.886474  1.000000 -0.729313  0.887049  0.410435   \n",
            "V4     -0.183578 -0.576283  0.645409 -0.729313  1.000000 -0.562645 -0.306189   \n",
            "V5      0.337220  0.898603 -0.840797  0.887049 -0.562645  1.000000  0.231766   \n",
            "V6      0.165930  0.252016 -0.174114  0.410435 -0.306189  0.231766  1.000000   \n",
            "V7      0.251377  0.898754 -0.868443  0.883155 -0.692188  0.845797  0.175529   \n",
            "V8     -0.205305 -0.106826 -0.013696 -0.210214  0.100499 -0.239239 -0.744850   \n",
            "V9      0.149949  0.637788 -0.700260  0.733561 -0.822482  0.666312  0.242068   \n",
            "V10     0.231540  0.709794 -0.749752  0.815773 -0.727467  0.766850  0.314013   \n",
            "V11    -0.356021 -0.411017  0.516903 -0.610780  0.720897 -0.451231 -0.468027   \n",
            "V12     0.293054  0.494487 -0.585094  0.658766 -0.777049  0.572077  0.441688   \n",
            "V13    -0.167851 -0.133948  0.124224 -0.181419  0.189974 -0.218863 -0.242452   \n",
            "V14     0.162164  0.228929 -0.388163  0.460434 -0.642719  0.277553  0.505694   \n",
            "V15    -0.142005  0.152488 -0.236753  0.195815 -0.210025  0.105545 -0.107269   \n",
            "V16     0.284078  0.568905 -0.546954  0.630926 -0.621955  0.673648  0.358297   \n",
            "V17     0.299928  0.621562 -0.567672  0.647639 -0.615901  0.732487  0.345539   \n",
            "V18     0.321099  0.657308 -0.577372  0.650410 -0.593308  0.750312  0.287541   \n",
            "V19    -0.103493 -0.271849  0.151119 -0.260522  0.245338 -0.401581 -0.202035   \n",
            "V20    -0.018284 -0.313023  0.365665 -0.363802  0.309375 -0.340592  0.015567   \n",
            "V21    -0.070679  0.109167 -0.071534  0.133361 -0.184658  0.109877 -0.000832   \n",
            "V22     0.142075 -0.059345  0.023577 -0.080513  0.197429 -0.115404  0.101324   \n",
            "V23     0.064345 -0.048118  0.181508 -0.046216  0.059698 -0.110505  0.433804   \n",
            "V24    -0.046539 -0.199116  0.093194 -0.080922 -0.018467 -0.291890 -0.131547   \n",
            "V25    -0.171098 -0.107737  0.142213 -0.121713 -0.078078 -0.122249 -0.207125   \n",
            "V26     0.003898  0.109811 -0.058354  0.020962  0.230264  0.122489 -0.027114   \n",
            "V27    -0.178371  0.241111 -0.235476  0.155556 -0.115302  0.230997 -0.206765   \n",
            "V28     0.053249  0.275622 -0.067707  0.236195 -0.231203  0.238588 -0.000210   \n",
            "Amount  0.066006  0.044805 -0.264083  0.086895 -0.132097 -0.049149  0.220925   \n",
            "Class        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "\n",
            "              V7        V8        V9  ...       V21       V22       V23  \\\n",
            "Time    0.251377 -0.205305  0.149949  ... -0.070679  0.142075  0.064345   \n",
            "V1      0.898754 -0.106826  0.637788  ...  0.109167 -0.059345 -0.048118   \n",
            "V2     -0.868443 -0.013696 -0.700260  ... -0.071534  0.023577  0.181508   \n",
            "V3      0.883155 -0.210214  0.733561  ...  0.133361 -0.080513 -0.046216   \n",
            "V4     -0.692188  0.100499 -0.822482  ... -0.184658  0.197429  0.059698   \n",
            "V5      0.845797 -0.239239  0.666312  ...  0.109877 -0.115404 -0.110505   \n",
            "V6      0.175529 -0.744850  0.242068  ... -0.000832  0.101324  0.433804   \n",
            "V7      1.000000  0.101213  0.764965  ...  0.172029 -0.185917 -0.088743   \n",
            "V8      0.101213  1.000000 -0.053345  ...  0.008786 -0.085814 -0.415127   \n",
            "V9      0.764965 -0.053345  1.000000  ...  0.310812 -0.345596 -0.101507   \n",
            "V10     0.861571 -0.034285  0.860793  ...  0.226620 -0.308356 -0.084666   \n",
            "V11    -0.537511  0.217307 -0.614115  ...  0.082224  0.019122 -0.041391   \n",
            "V12     0.644172 -0.188941  0.709262  ...  0.005192 -0.159785 -0.006273   \n",
            "V13    -0.088699  0.374716 -0.130555  ...  0.049296 -0.007783 -0.133663   \n",
            "V14     0.336159 -0.251532  0.526575  ... -0.211234  0.118790  0.015289   \n",
            "V15     0.248533  0.197312  0.200599  ...  0.204080 -0.124434 -0.086640   \n",
            "V16     0.687646 -0.205567  0.673153  ... -0.091924 -0.152850  0.002735   \n",
            "V17     0.713301 -0.248173  0.724999  ... -0.029928 -0.167941  0.014542   \n",
            "V18     0.737949 -0.204726  0.707062  ... -0.016994 -0.161580  0.026840   \n",
            "V19    -0.314786  0.243100 -0.292190  ...  0.108145  0.178765 -0.001614   \n",
            "V20    -0.420421 -0.129893 -0.438328  ... -0.644340  0.527228  0.202952   \n",
            "V21     0.172029  0.008786  0.310812  ...  1.000000 -0.828793  0.056943   \n",
            "V22    -0.185917 -0.085814 -0.345596  ... -0.828793  1.000000  0.090313   \n",
            "V23    -0.088743 -0.415127 -0.101507  ...  0.056943  0.090313  1.000000   \n",
            "V24    -0.165672  0.118902 -0.076479  ... -0.037442  0.100448 -0.051142   \n",
            "V25     0.077623  0.308886 -0.001056  ...  0.163636 -0.308931  0.137399   \n",
            "V26     0.065620  0.046008 -0.185893  ...  0.026053  0.048988  0.061485   \n",
            "V27     0.306121  0.318150  0.218819  ...  0.370041 -0.425094 -0.224276   \n",
            "V28     0.250886 -0.011718  0.271532  ...  0.297051 -0.289399  0.070468   \n",
            "Amount  0.182598  0.034826  0.126840  ...  0.013865 -0.007179 -0.163475   \n",
            "Class        NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
            "\n",
            "             V24       V25       V26       V27       V28    Amount  Class  \n",
            "Time   -0.046539 -0.171098  0.003898 -0.178371  0.053249  0.066006    NaN  \n",
            "V1     -0.199116 -0.107737  0.109811  0.241111  0.275622  0.044805    NaN  \n",
            "V2      0.093194  0.142213 -0.058354 -0.235476 -0.067707 -0.264083    NaN  \n",
            "V3     -0.080922 -0.121713  0.020962  0.155556  0.236195  0.086895    NaN  \n",
            "V4     -0.018467 -0.078078  0.230264 -0.115302 -0.231203 -0.132097    NaN  \n",
            "V5     -0.291890 -0.122249  0.122489  0.230997  0.238588 -0.049149    NaN  \n",
            "V6     -0.131547 -0.207125 -0.027114 -0.206765 -0.000210  0.220925    NaN  \n",
            "V7     -0.165672  0.077623  0.065620  0.306121  0.250886  0.182598    NaN  \n",
            "V8      0.118902  0.308886  0.046008  0.318150 -0.011718  0.034826    NaN  \n",
            "V9     -0.076479 -0.001056 -0.185893  0.218819  0.271532  0.126840    NaN  \n",
            "V10    -0.119727  0.043821 -0.002720  0.253740  0.269438  0.092826    NaN  \n",
            "V11    -0.122181  0.036298  0.231995  0.171917 -0.054528 -0.141916    NaN  \n",
            "V12    -0.056075  0.070050 -0.143593  0.056408  0.100748  0.125873    NaN  \n",
            "V13     0.094279  0.024264  0.084744  0.073191 -0.151807 -0.017413    NaN  \n",
            "V14     0.168811 -0.122861 -0.266372 -0.203844 -0.088647  0.175146    NaN  \n",
            "V15     0.039463 -0.015452  0.104165  0.212319  0.144568  0.117337    NaN  \n",
            "V16    -0.210901  0.094953 -0.041254  0.055782  0.113382  0.041360    NaN  \n",
            "V17    -0.252910  0.047732 -0.023923  0.078083  0.145793  0.028192    NaN  \n",
            "V18    -0.278454  0.082898  0.008043  0.126636  0.189559  0.015895    NaN  \n",
            "V19     0.259045 -0.210905  0.090816 -0.001556 -0.089541  0.079378    NaN  \n",
            "V20    -0.037509  0.028720  0.018196 -0.177395 -0.000243  0.063953    NaN  \n",
            "V21    -0.037442  0.163636  0.026053  0.370041  0.297051  0.013865    NaN  \n",
            "V22     0.100448 -0.308931  0.048988 -0.425094 -0.289399 -0.007179    NaN  \n",
            "V23    -0.051142  0.137399  0.061485 -0.224276  0.070468 -0.163475    NaN  \n",
            "V24     1.000000 -0.126679 -0.228653 -0.308429 -0.044750  0.078750    NaN  \n",
            "V25    -0.126679  1.000000  0.104471  0.264251  0.200177 -0.139264    NaN  \n",
            "V26    -0.228653  0.104471  1.000000  0.218785  0.016287 -0.088354    NaN  \n",
            "V27    -0.308429  0.264251  0.218785  1.000000  0.250029  0.089289    NaN  \n",
            "V28    -0.044750  0.200177  0.016287  0.250029  1.000000 -0.091602    NaN  \n",
            "Amount  0.078750 -0.139264 -0.088354  0.089289 -0.091602  1.000000    NaN  \n",
            "Class        NaN       NaN       NaN       NaN       NaN       NaN    NaN  \n",
            "\n",
            "[31 rows x 31 columns] \n",
            "\n",
            "\n",
            "=== SIGNIFICANTLY CORRELATED COLUMNS ===\n",
            "V1   V2   0.82886\n",
            "V1   V3   0.90902\n",
            "V1   V5   0.8986\n",
            "V1   V7   0.89875\n",
            "V1   V9   0.63779\n",
            "V1   V10   0.70979\n",
            "V1   V17   0.62156\n",
            "V1   V18   0.65731\n",
            "V2   V3   0.88647\n",
            "V2   V4   0.64541\n",
            "V2   V5   0.8408\n",
            "V2   V7   0.86844\n",
            "V2   V9   0.70026\n",
            "V2   V10   0.74975\n",
            "V3   V4   0.72931\n",
            "V3   V5   0.88705\n",
            "V3   V7   0.88315\n",
            "V3   V9   0.73356\n",
            "V3   V10   0.81577\n",
            "V3   V11   0.61078\n",
            "V3   V12   0.65877\n",
            "V3   V16   0.63093\n",
            "V3   V17   0.64764\n",
            "V3   V18   0.65041\n",
            "V4   V7   0.69219\n",
            "V4   V9   0.82248\n",
            "V4   V10   0.72747\n",
            "V4   V11   0.7209\n",
            "V4   V12   0.77705\n",
            "V4   V14   0.64272\n",
            "V4   V16   0.62195\n",
            "V4   V17   0.6159\n",
            "V5   V7   0.8458\n",
            "V5   V9   0.66631\n",
            "V5   V10   0.76685\n",
            "V5   V16   0.67365\n",
            "V5   V17   0.73249\n",
            "V5   V18   0.75031\n",
            "V6   V8   0.74485\n",
            "V7   V9   0.76497\n",
            "V7   V10   0.86157\n",
            "V7   V12   0.64417\n",
            "V7   V16   0.68765\n",
            "V7   V17   0.7133\n",
            "V7   V18   0.73795\n",
            "V9   V10   0.86079\n",
            "V9   V11   0.61412\n",
            "V9   V12   0.70926\n",
            "V9   V16   0.67315\n",
            "V9   V17   0.725\n",
            "V9   V18   0.70706\n",
            "V10   V11   0.72426\n",
            "V10   V12   0.83878\n",
            "V10   V16   0.81522\n",
            "V10   V17   0.81108\n",
            "V10   V18   0.79044\n",
            "V11   V12   0.88892\n",
            "V11   V14   0.86398\n",
            "V11   V16   0.75166\n",
            "V11   V17   0.70134\n",
            "V11   V18   0.61608\n",
            "V12   V14   0.80064\n",
            "V12   V16   0.8882\n",
            "V12   V17   0.84653\n",
            "V12   V18   0.79565\n",
            "V14   V16   0.6561\n",
            "V16   V17   0.96051\n",
            "V16   V18   0.94422\n",
            "V16   V19   0.68543\n",
            "V17   V18   0.97078\n",
            "V17   V19   0.62001\n",
            "V20   V21   0.64434\n",
            "V21   V22   0.82879\n",
            "['V1', 'V2', 'V3', 'V5', 'V7', 'V9', 'V10', 'V17', 'V18', 'V4', 'V11', 'V12', 'V16', 'V14', 'V6', 'V8', 'V19', 'V20', 'V21', 'V22']\n",
            "OrderedDict([(0.82886, ['V1', 'V2']), (0.90902, ['V1', 'V3']), (0.8986, ['V1', 'V5']), (0.89875, ['V1', 'V7']), (0.63779, ['V1', 'V9']), (0.70979, ['V1', 'V10']), (0.62156, ['V1', 'V17']), (0.65731, ['V1', 'V18']), (0.88647, ['V2', 'V3']), (0.64541, ['V2', 'V4']), (0.8408, ['V2', 'V5']), (0.86844, ['V2', 'V7']), (0.70026, ['V2', 'V9']), (0.74975, ['V2', 'V10']), (0.72931, ['V3', 'V4']), (0.88705, ['V3', 'V5']), (0.88315, ['V3', 'V7']), (0.73356, ['V3', 'V9']), (0.81577, ['V3', 'V10']), (0.61078, ['V3', 'V11']), (0.65877, ['V3', 'V12']), (0.63093, ['V3', 'V16']), (0.64764, ['V3', 'V17']), (0.65041, ['V3', 'V18']), (0.69219, ['V4', 'V7']), (0.82248, ['V4', 'V9']), (0.72747, ['V4', 'V10']), (0.7209, ['V4', 'V11']), (0.77705, ['V4', 'V12']), (0.64272, ['V4', 'V14']), (0.62195, ['V4', 'V16']), (0.6159, ['V4', 'V17']), (0.8458, ['V5', 'V7']), (0.66631, ['V5', 'V9']), (0.76685, ['V5', 'V10']), (0.67365, ['V5', 'V16']), (0.73249, ['V5', 'V17']), (0.75031, ['V5', 'V18']), (0.74485, ['V6', 'V8']), (0.76497, ['V7', 'V9']), (0.86157, ['V7', 'V10']), (0.64417, ['V7', 'V12']), (0.68765, ['V7', 'V16']), (0.7133, ['V7', 'V17']), (0.73795, ['V7', 'V18']), (0.86079, ['V9', 'V10']), (0.61412, ['V9', 'V11']), (0.70926, ['V9', 'V12']), (0.67315, ['V9', 'V16']), (0.725, ['V9', 'V17']), (0.70706, ['V9', 'V18']), (0.72426, ['V10', 'V11']), (0.83878, ['V10', 'V12']), (0.81522, ['V10', 'V16']), (0.81108, ['V10', 'V17']), (0.79044, ['V10', 'V18']), (0.88892, ['V11', 'V12']), (0.86398, ['V11', 'V14']), (0.75166, ['V11', 'V16']), (0.70134, ['V11', 'V17']), (0.61608, ['V11', 'V18']), (0.80064, ['V12', 'V14']), (0.8882, ['V12', 'V16']), (0.84653, ['V12', 'V17']), (0.79565, ['V12', 'V18']), (0.6561, ['V14', 'V16']), (0.96051, ['V16', 'V17']), (0.94422, ['V16', 'V18']), (0.68543, ['V16', 'V19']), (0.97078, ['V17', 'V18']), (0.62001, ['V17', 'V19']), (0.64434, ['V20', 'V21']), (0.82879, ['V21', 'V22'])])\n"
          ]
        }
      ],
      "source": [
        "print(regression_df.corr(), \"\\n\\n\")\n",
        "\n",
        "# Indexing with numbers on a numpy matrix will probably be faster\n",
        "\n",
        "print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "rows, cols = regression_df.shape\n",
        "corr = regression_df.corr().values\n",
        "fields = list(regression_df.columns)\n",
        "correlated_columns = list()\n",
        "nodes = []\n",
        "correlations = OrderedDict()\n",
        "\n",
        "# data structure that organizes correlations\n",
        "correlations_per_column = list()\n",
        "for i in range(cols):\n",
        "    for j in range(i+1, cols):\n",
        "      corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "      if corr[i,j] > 0.6:\n",
        "          print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "          correlated_columns.append([fields[i], fields[j], corr[i, j]])\n",
        "          correlations[corr[i, j]] = [fields[i], fields[j]]\n",
        "          if fields[i] not in nodes:\n",
        "            nodes.append(fields[i])\n",
        "          if fields[j] not in nodes:\n",
        "            nodes.append(fields[j])\n",
        "\n",
        "print(nodes)\n",
        "print(correlations)"
      ],
      "id": "6ad660fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l74DGBikGfow"
      },
      "source": [
        "We can visualize these attributes and their correlations as a weighted graph."
      ],
      "id": "l74DGBikGfow"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ggXpUuAfWqG2",
        "outputId": "32d441e3-78e0-4725-a261-b7a9ae3df30b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "for key,value in correlations.items():\n",
        "  G.add_edge(value[0], value[1], weight=key)\n",
        "\n",
        "\n",
        "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0.5]\n",
        "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.5]\n",
        "\n",
        "pos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n",
        "\n",
        "# nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=700)\n",
        "\n",
        "# edges\n",
        "nx.draw_networkx_edges(G, pos, edgelist=elarge, width=6)\n",
        "nx.draw_networkx_edges(\n",
        "    G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=\"b\", style=\"dashed\"\n",
        ")\n",
        "\n",
        "# node labels\n",
        "nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
        "# edge weight labels\n",
        "edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.margins(0.15)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ggXpUuAfWqG2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBaK36IxauAO"
      },
      "source": [
        "Now, let's organize them by pair and start generating values."
      ],
      "id": "lBaK36IxauAO"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkLcoIVuar-l",
        "outputId": "e4eed91b-a9d5-44cc-d223-50131500cb76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([(97.078, ['V17', 'V18']), (96.051, ['V16', 'V17']), (94.422, ['V16', 'V18']), (90.902, ['V1', 'V3']), (89.875, ['V1', 'V7']), (89.86, ['V1', 'V5']), (88.89200000000001, ['V11', 'V12']), (88.82, ['V12', 'V16']), (88.705, ['V3', 'V5']), (88.64699999999999, ['V2', 'V3']), (88.315, ['V3', 'V7']), (86.844, ['V2', 'V7']), (86.398, ['V11', 'V14']), (86.157, ['V7', 'V10']), (86.07900000000001, ['V9', 'V10']), (84.653, ['V12', 'V17']), (84.58, ['V5', 'V7']), (84.08, ['V2', 'V5']), (83.878, ['V10', 'V12']), (82.88600000000001, ['V1', 'V2']), (82.879, ['V21', 'V22']), (82.248, ['V4', 'V9']), (81.577, ['V3', 'V10']), (81.52199999999999, ['V10', 'V16']), (81.108, ['V10', 'V17']), (80.06400000000001, ['V12', 'V14']), (79.565, ['V12', 'V18']), (79.044, ['V10', 'V18']), (77.705, ['V4', 'V12']), (76.685, ['V5', 'V10']), (76.497, ['V7', 'V9']), (75.166, ['V11', 'V16']), (75.031, ['V5', 'V18']), (74.97500000000001, ['V2', 'V10']), (74.485, ['V6', 'V8']), (73.795, ['V7', 'V18']), (73.356, ['V3', 'V9']), (73.249, ['V5', 'V17']), (72.931, ['V3', 'V4']), (72.747, ['V4', 'V10']), (72.5, ['V9', 'V17']), (72.426, ['V10', 'V11']), (72.09, ['V4', 'V11']), (71.33, ['V7', 'V17']), (70.979, ['V1', 'V10']), (70.926, ['V9', 'V12']), (70.706, ['V9', 'V18']), (70.134, ['V11', 'V17']), (70.026, ['V2', 'V9']), (69.219, ['V4', 'V7']), (68.765, ['V7', 'V16']), (68.54299999999999, ['V16', 'V19']), (67.365, ['V5', 'V16']), (67.315, ['V9', 'V16']), (66.631, ['V5', 'V9']), (65.877, ['V3', 'V12']), (65.731, ['V1', 'V18']), (65.61, ['V14', 'V16']), (65.04100000000001, ['V3', 'V18']), (64.764, ['V3', 'V17']), (64.541, ['V2', 'V4']), (64.434, ['V20', 'V21']), (64.417, ['V7', 'V12']), (64.27199999999999, ['V4', 'V14']), (63.778999999999996, ['V1', 'V9']), (63.092999999999996, ['V3', 'V16']), (62.195, ['V4', 'V16']), (62.156, ['V1', 'V17']), (62.001, ['V17', 'V19']), (61.608, ['V11', 'V18']), (61.59, ['V4', 'V17']), (61.412, ['V9', 'V11']), (61.077999999999996, ['V3', 'V11'])])\n"
          ]
        }
      ],
      "source": [
        "# weight all the correlations so that they're iterated through correctly\n",
        "weighted_correlations = OrderedDict()\n",
        "for key,value in correlations.items():\n",
        "  mult_factor = 1\n",
        "  if value[0] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  if value[1] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  weighted_correlations[mult_factor * key] = value\n",
        "\n",
        "weighted_correlations = OrderedDict(sorted(weighted_correlations.items(), reverse=True))\n",
        "print(weighted_correlations)\n"
      ],
      "id": "FkLcoIVuar-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numerical Data Generation: Symbolic Regression\n",
        "Now, let's augment the dataset with numerical data.\n",
        "\n",
        "The code is implementing a technique for data augmentation using symbolic regression, kernel density estimation, and K-nearest neighbor regression. The goal is to generate new samples of data that are similar to the original dataset, but not identical, to increase the size and diversity of the dataset for training machine learning models. The code takes in a pandas DataFrame df and the number of new samples to generate n_samples as inputs.\n",
        "\n",
        "The code first scales each column of the input dataset to the range [0, 1], fits a symbolic regressor to the scaled data, and generates new values using the regressor. It then applies kernel density estimation and K-nearest neighbor regression to the original data and generates new values using these methods. Finally, it performs a grid search to find the best kernel and bandwidth for the kernel density estimator, and generates new values using the best estimator.\n",
        "\n",
        "The new values generated by each method are clipped to the range of the original column data, and the new values for all columns are combined to create a new DataFrame new_df. The new_df is then concatenated with the original DataFrame df to create an augmented dataset, which is returned as output.\n",
        "\n"
      ],
      "metadata": {
        "id": "dJCaBCe3meJ0"
      },
      "id": "dJCaBCe3meJ0"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YqEGUD4oDrP",
        "outputId": "2a7644e5-851d-445e-b05a-fa936675e289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n",
            "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}\n",
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0    48.81          4596.07        7       0.00636966        0.0071038     16.00m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-7.46683681e+08            -inf            -inf -1.84117525e+05\n",
            "            -inf -1.86671275e+08            -inf            -inf\n",
            " -9.23057913e+04            -inf -8.29652986e+07            -inf\n",
            "            -inf -6.17171430e+04            -inf -4.66682197e+07\n",
            "            -inf            -inf -4.64303545e+04            -inf\n",
            " -2.98678654e+07            -inf            -inf -3.72626914e+04\n",
            "            -inf -2.07417524e+07            -inf            -inf\n",
            " -3.11538383e+04            -inf -1.52389980e+07            -inf\n",
            "            -inf -2.67924028e+04            -inf -1.16675017e+07\n",
            "            -inf            -inf -2.35228977e+04            -inf\n",
            " -9.21889792e+06            -inf            -inf -2.09811468e+04\n",
            "            -inf -7.46742790e+06            -inf            -inf\n",
            " -1.89486988e+04            -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "<ipython-input-22-b5e94a70d864>:81: RuntimeWarning: underflow encountered in exp\n",
            "  probs = np.exp(log_dens - log_dens.max())\n",
            "<ipython-input-22-b5e94a70d864>:82: RuntimeWarning: underflow encountered in true_divide\n",
            "  probs /= probs.sum()\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          693.517        7        0.0106299        0.0105704     17.83m\n",
            "   1    15.71          3.75748        1                0                0      4.25m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-309.09676215          -inf          -inf -270.85464024          -inf\n",
            " -267.2593202           -inf          -inf -261.06627691          -inf\n",
            " -260.98265644          -inf          -inf -258.88938571          -inf\n",
            " -258.84341583          -inf          -inf -258.22078521          -inf\n",
            " -258.03256208          -inf          -inf -258.11105373          -inf\n",
            " -257.86012536          -inf          -inf -258.26365729          -inf\n",
            " -257.99690418 -257.99872239 -261.89245354 -258.55789356 -262.24310741\n",
            " -258.27361212 -256.72698226 -259.14021916 -258.938691   -259.58104306\n",
            " -258.6076053  -256.46053433 -258.04393574 -259.378861   -258.51451721\n",
            " -258.96413375 -256.65587942 -257.45999929 -259.86381686 -257.89565923]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          1575.53        7       0.00534834       0.00520872      7.89m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-329.90341429          -inf          -inf -250.83811746          -inf\n",
            " -257.0285738           -inf          -inf -240.44733927          -inf\n",
            " -243.77497577          -inf          -inf -237.46509649          -inf\n",
            " -239.29907881          -inf          -inf -236.22268145          -inf\n",
            " -237.34077085          -inf          -inf -235.68552808          -inf\n",
            " -236.38593727          -inf          -inf -235.51897334          -inf\n",
            " -235.94337958          -inf          -inf -235.57615095          -inf\n",
            " -235.79815535          -inf          -inf -235.78001526          -inf\n",
            " -235.84162846          -inf          -inf -236.08597622          -inf\n",
            " -236.01492157          -inf          -inf -236.4667284           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          1148.02        7       0.00972342       0.00969265      6.37m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-306.52466572          -inf          -inf -284.60289212          -inf\n",
            " -280.08995862          -inf          -inf -276.54737866          -inf\n",
            " -275.32401423          -inf          -inf -274.06673462          -inf\n",
            " -273.41925527          -inf          -inf -272.85060573          -inf\n",
            " -272.34000869          -inf          -inf -272.18766957          -inf\n",
            " -271.68360919 -272.43874106 -275.49688953 -271.83854903 -276.47939606\n",
            " -271.30277152 -273.09425343 -274.05411351 -271.69102059 -274.85369569\n",
            " -271.11386095 -272.81513889 -273.57238227 -271.68314058 -274.19893229\n",
            " -271.06119258 -272.45458225 -273.10364184 -271.77748629 -273.66535068\n",
            " -271.10602114 -271.41306114 -272.48288066 -271.94998591 -273.05898806]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          2834.07        7       0.00582693       0.00627007      7.59m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-214.75874529          -inf          -inf -214.2339346           -inf\n",
            " -214.75761062 -214.21178417 -215.68907853 -214.23728277 -215.59518079\n",
            " -215.16496567 -215.81385079 -214.60717408 -214.55871169 -214.39352538\n",
            " -215.24885209 -216.25767077 -215.06351044 -214.89952838 -214.63201651\n",
            " -215.34953712 -216.39522277 -215.51608797 -215.27290837 -214.96954671\n",
            " -215.5291566  -216.88053134 -215.78970309 -215.68726957 -215.21002604\n",
            " -215.77353369 -215.75361483 -215.89321216 -216.14459244 -215.35602625\n",
            " -216.06664172 -215.32916147 -215.59712466 -216.64423439 -215.22054293\n",
            " -216.39585502 -215.33979022 -215.36350954 -217.18466949 -215.09188259\n",
            " -216.7532247  -215.66064989 -215.32728814 -217.7641944  -215.07955324]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          849.922        7       0.00766559       0.00768992      7.96m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-320.07233157          -inf          -inf -266.37313008          -inf\n",
            " -266.6495146           -inf          -inf -256.34444974          -inf\n",
            " -257.48096282          -inf          -inf -253.52376778          -inf\n",
            " -254.19041126          -inf          -inf -252.14073468          -inf\n",
            " -252.51636475          -inf          -inf -251.32283273          -inf\n",
            " -251.52228751          -inf          -inf -250.82493631          -inf\n",
            " -250.88892814          -inf          -inf -250.54580842          -inf\n",
            " -250.48038461          -inf          -inf -250.43310333          -inf\n",
            " -250.22987236          -inf          -inf -250.45498028          -inf\n",
            " -250.10058516          -inf          -inf -250.58898337          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          1190.77        7       0.00522359       0.00523388      6.44m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-185.86739118          -inf          -inf -177.42536976          -inf\n",
            " -175.20344979          -inf          -inf -173.892468            -inf\n",
            " -173.53024332          -inf          -inf -173.12879514          -inf\n",
            " -173.1233983           -inf          -inf -173.08121177          -inf\n",
            " -173.15658658          -inf          -inf -173.36956386          -inf\n",
            " -173.38146827          -inf          -inf -173.89197462          -inf\n",
            " -173.71715511 -172.833014   -173.42376877 -174.61182582 -173.59245304\n",
            " -174.1518342  -173.31832491 -173.14232271 -175.5037448  -173.22368836\n",
            " -174.69414752 -173.60039112 -173.09071874 -176.54183655 -173.08596453\n",
            " -175.35150933 -173.35174641 -173.1217595  -177.70051524 -173.04528936]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          459.321        7         0.010364         0.010365      7.76m\n",
            "   1    15.93          1.74594        1                0                0      4.30m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-826.51750643          -inf          -inf -307.52636387          -inf\n",
            " -405.33226589          -inf          -inf -285.46729593          -inf\n",
            " -328.43211031          -inf          -inf -278.89975834          -inf\n",
            " -301.80882053          -inf          -inf -275.82355256          -inf\n",
            " -289.50705042          -inf          -inf -274.07748698          -inf\n",
            " -282.86042141          -inf          -inf -272.9868099           -inf\n",
            " -278.92669896          -inf          -inf -272.27903577          -inf\n",
            " -276.4498364           -inf          -inf -271.82597455          -inf\n",
            " -274.81435624          -inf          -inf -271.55902139          -inf\n",
            " -273.69667891          -inf          -inf -271.43667637          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          1500.68        7       0.00923772       0.00910131      7.59m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-1484.53845622           -inf           -inf  -276.5879762\n",
            "           -inf  -531.80916523           -inf           -inf\n",
            "  -244.70476162           -inf  -356.85142407           -inf\n",
            "           -inf  -235.2001829            -inf  -296.34214787\n",
            "           -inf           -inf  -231.15286701           -inf\n",
            "  -268.95251596           -inf           -inf  -229.27788193\n",
            "           -inf  -254.63648169           -inf           -inf\n",
            "  -228.47920541           -inf  -246.5103896            -inf\n",
            "           -inf  -228.28993218           -inf  -241.69210788\n",
            "           -inf           -inf  -228.47876292           -inf\n",
            "  -238.79954383           -inf           -inf  -228.91793505\n",
            "           -inf  -237.10231931           -inf           -inf\n",
            "  -229.53088372           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          457.479        7       0.00872995       0.00871846      8.84m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-253.43428741          -inf          -inf -208.67943084          -inf\n",
            " -214.20809617          -inf          -inf -204.31515072          -inf\n",
            " -207.55440114          -inf          -inf -203.20582486          -inf\n",
            " -205.08971275          -inf          -inf -202.76431126          -inf\n",
            " -203.89751751          -inf          -inf -202.6353641           -inf\n",
            " -203.32014514          -inf          -inf -202.71436184          -inf\n",
            " -203.06967534          -inf          -inf -202.95321027          -inf\n",
            " -203.00981801          -inf          -inf -203.32267842          -inf\n",
            " -203.07838225          -inf          -inf -203.80222916          -inf\n",
            " -203.24831193          -inf          -inf -204.37632676          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          782.281        7       0.00890649       0.00886813      9.95m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-318.79789615          -inf          -inf -263.52052394          -inf\n",
            " -265.75170955          -inf          -inf -254.71965269          -inf\n",
            " -256.20749261          -inf          -inf -252.49452078          -inf\n",
            " -253.23732928          -inf          -inf -251.74421929          -inf\n",
            " -252.21796783          -inf          -inf -251.52381101          -inf\n",
            " -251.91698768          -inf          -inf -251.54314869          -inf\n",
            " -251.90194811          -inf          -inf -251.69319214          -inf\n",
            " -252.01006479          -inf          -inf -251.92720677          -inf\n",
            " -252.18141201          -inf          -inf -252.2225433           -inf\n",
            " -252.39282424          -inf          -inf -252.56660192          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          3298.01        7       0.00539364       0.00556779      7.37m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-214.15144126          -inf          -inf -212.8912684           -inf\n",
            " -211.49117801          -inf          -inf -211.14957048          -inf\n",
            " -211.30313605          -inf          -inf -210.9677871           -inf\n",
            " -211.263233   -212.19819462 -212.10527327 -211.00149342 -212.10343651\n",
            " -211.2096385  -211.13126553 -211.38216384 -211.11490138 -211.34825247\n",
            " -211.19409119 -212.36817118 -211.4858795  -211.29080012 -211.30839012\n",
            " -211.23023505 -212.43477935 -211.65768226 -211.52873873 -211.40447688\n",
            " -211.31574848 -211.73053448 -211.69036346 -211.83052467 -211.44073142\n",
            " -211.44559246 -210.83254463 -211.4865459  -212.19739943 -211.31976152\n",
            " -211.61704268 -211.14248418 -211.27663291 -212.62940378 -211.18003877]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          1897.23        7       0.00833949       0.00815147      7.92m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-258.12264281          -inf          -inf -254.84364184          -inf\n",
            " -252.65363391          -inf          -inf -251.85520078          -inf\n",
            " -251.65686486          -inf          -inf -251.3982649           -inf\n",
            " -251.38443083 -253.94251402 -253.43880676 -251.48709871 -253.54788122\n",
            " -251.51848699 -252.18208237 -252.8033138  -251.78853435 -252.7201181\n",
            " -251.82992274 -251.56701217 -251.84453767 -252.18490088 -251.88943319\n",
            " -252.21230211 -251.03704684 -251.51119547 -252.6238746  -251.55257225\n",
            " -252.62955109 -251.61493764 -251.31612047 -253.08128232 -251.33554701\n",
            " -253.06244092 -251.86594783 -251.28910734 -253.54647054 -251.25653363\n",
            " -253.49509654 -252.12526397 -251.32842489 -254.01534118 -251.23528937]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81           643.71        7       0.00679316        0.0069428      7.85m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-133.33178046          -inf          -inf -132.84934208          -inf\n",
            " -133.12406291          -inf          -inf -132.80400148          -inf\n",
            " -133.06396381 -134.17736259 -133.5207589  -133.25115805 -133.2934356\n",
            " -133.30320958 -133.3948938  -133.43657636 -134.15110371 -133.23069238\n",
            " -133.82637102 -133.13037321 -133.19182024 -135.48085223 -133.08162001\n",
            " -134.64229087 -132.98167727 -133.02804595 -137.16884818 -132.9536528\n",
            " -135.73501319 -133.74711304 -133.03545706 -139.13514799 -132.93719509\n",
            " -137.07974996 -133.80103497 -133.22662615 -141.30793182 -133.06233212\n",
            " -138.65348637 -134.06461597 -133.38270925 -143.62775186 -133.17826036\n",
            " -140.43384284 -134.53578572 -133.56534866 -146.04734365 -133.3184568 ]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          2136.86        7       0.00733373       0.00691176      6.31m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-296.90327482          -inf          -inf -260.55302572          -inf\n",
            " -261.86980773          -inf          -inf -254.31531407          -inf\n",
            " -255.41847324          -inf          -inf -252.62782487          -inf\n",
            " -253.28651255          -inf          -inf -252.00475368          -inf\n",
            " -252.40527911          -inf          -inf -251.79115996          -inf\n",
            " -252.01840337          -inf          -inf -251.77829031          -inf\n",
            " -251.8785506           -inf          -inf -251.87738861          -inf\n",
            " -251.8860705           -inf          -inf -252.0441451           -inf\n",
            " -251.98831856          -inf          -inf -252.25470641          -inf\n",
            " -252.15216305          -inf          -inf -252.49594109          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          334.921        7       0.00849113       0.00869166      7.93m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-140.7859828           -inf          -inf -132.55612371          -inf\n",
            " -132.25845006          -inf          -inf -130.66897255          -inf\n",
            " -130.76171326          -inf          -inf -130.53235033          -inf\n",
            " -130.55201192          -inf          -inf -131.14184012          -inf\n",
            " -130.89105337          -inf          -inf -132.30194985          -inf\n",
            " -131.60689488          -inf          -inf -133.90347026          -inf\n",
            " -132.66094758          -inf          -inf -135.84821965          -inf\n",
            " -134.02939146          -inf          -inf -138.04761145          -inf\n",
            " -135.68304051 -131.25724344 -130.62094325 -140.42718952 -130.55788105\n",
            " -137.5873283  -131.69501536 -130.6693469  -142.92771039 -130.50115036]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          2402.08        7       0.00775759       0.00761987      7.53m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-254.09164763          -inf          -inf -246.10910594          -inf\n",
            " -242.00559359          -inf          -inf -240.5855526           -inf\n",
            " -239.66184669          -inf          -inf -239.05444992          -inf\n",
            " -238.7545046           -inf          -inf -238.49135032          -inf\n",
            " -238.33620114          -inf          -inf -238.34207611          -inf\n",
            " -238.16159414          -inf          -inf -238.41619451          -inf\n",
            " -238.14148278 -239.0216202  -239.38708257 -238.6259939  -239.76775576\n",
            " -238.23646776 -238.64800561 -239.05014408 -238.92465806 -239.31547877\n",
            " -238.42156259 -238.43078509 -238.69435221 -239.28525161 -238.91918845\n",
            " -238.67579691 -238.14832109 -238.43800742 -239.69161409 -238.62417438]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          971.367        7       0.00783145       0.00771789      7.95m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-334.49362218          -inf          -inf -297.37270275          -inf\n",
            " -297.80868544          -inf          -inf -290.33803103          -inf\n",
            " -291.53846937          -inf          -inf -288.76519033          -inf\n",
            " -289.67386915          -inf          -inf -288.28510764          -inf\n",
            " -289.09196862          -inf          -inf -288.17688065          -inf\n",
            " -288.93076783          -inf          -inf -288.22982339          -inf\n",
            " -288.92549941          -inf          -inf -288.36422435          -inf\n",
            " -288.992877            -inf          -inf -288.54536454          -inf\n",
            " -289.10106979          -inf          -inf -288.75633761          -inf\n",
            " -289.23452669          -inf          -inf -288.98789589          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          2900.91        7       0.00731624       0.00721471      7.73m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-211.72843076          -inf          -inf -210.0023142           -inf\n",
            " -209.17688159          -inf          -inf -208.80637065          -inf\n",
            " -209.48750746 -209.98871502 -209.63319173 -209.10456521 -209.86013996\n",
            " -209.81553547 -210.47561046 -209.22873633 -209.61685069 -209.09336924\n",
            " -210.18705426 -211.10393558 -209.72489149 -210.21605366 -209.31158745\n",
            " -210.64415164 -210.56015447 -209.99254358 -210.87640533 -209.50115956\n",
            " -211.16893058 -210.49828803 -209.96704267 -211.5881755  -209.52377218\n",
            " -211.73726772 -210.79146486 -210.07076883 -212.34392973 -209.62457623\n",
            " -212.33424872 -210.97372735 -210.21294233 -213.13683875 -209.75170154\n",
            " -212.95281644 -211.08009069 -210.33522447 -213.96110434 -209.8683258 ]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81            620.5        7       0.00663619       0.00675316      8.11m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-170.22193004          -inf          -inf -166.02372715          -inf\n",
            " -163.30596729          -inf          -inf -163.0772942           -inf\n",
            " -162.08369429          -inf          -inf -162.31085633          -inf\n",
            " -161.86335782 -161.35813825 -164.22277332 -162.23781834 -165.00038992\n",
            " -161.95894874 -161.28401934 -162.21281876 -162.56850833 -162.81968466\n",
            " -162.21774527 -161.76913393 -161.82727188 -163.20492223 -162.28720238\n",
            " -162.60606117 -161.64578943 -161.66759369 -164.09353249 -162.00866054\n",
            " -163.11976345 -162.37035059 -161.72258961 -165.19373351 -161.94014706\n",
            " -163.76053434 -162.47605559 -161.79817279 -166.46988737 -161.91817378\n",
            " -164.5312473  -162.9868488  -161.99044569 -167.88984213 -162.00086219]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          1582.49        7       0.00399213       0.00396699      6.75m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-230.62242571          -inf          -inf -136.80908511          -inf\n",
            " -153.0300418           -inf          -inf -131.93647816          -inf\n",
            " -139.45132663          -inf          -inf -131.22629566          -inf\n",
            " -135.43754531          -inf          -inf -131.72927962          -inf\n",
            " -134.39848908          -inf          -inf -132.89798131          -inf\n",
            " -134.65868619          -inf          -inf -134.52037386          -inf\n",
            " -135.65453305          -inf          -inf -136.48003594          -inf\n",
            " -137.13953414          -inf          -inf -138.69543099          -inf\n",
            " -138.97290123          -inf          -inf -141.09824129          -inf\n",
            " -141.05939112          -inf          -inf -143.62932164          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          548.446        7       0.00630637       0.00647519      8.63m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-556.64887853          -inf          -inf -175.67673479          -inf\n",
            " -249.86161707          -inf          -inf -162.14101996          -inf\n",
            " -195.14736808          -inf          -inf -159.50436757          -inf\n",
            " -177.34065407          -inf          -inf -159.49062213          -inf\n",
            " -170.30374682          -inf          -inf -160.48234106          -inf\n",
            " -167.55554676          -inf          -inf -161.92044908          -inf\n",
            " -166.82024915          -inf          -inf -163.58867122          -inf\n",
            " -167.124087            -inf          -inf -165.40533814          -inf\n",
            " -168.00555822          -inf          -inf -167.32990584          -inf\n",
            " -169.2388116           -inf          -inf -169.33121201          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          374.366       31       0.00682929        0.0124924      8.73m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-178.5340161           -inf          -inf -130.89344818          -inf\n",
            " -137.1041049           -inf          -inf -127.02504626          -inf\n",
            " -130.10093919          -inf          -inf -126.65033771          -inf\n",
            " -128.4226334           -inf          -inf -127.48142172          -inf\n",
            " -128.50599324          -inf          -inf -129.04226877          -inf\n",
            " -129.45065601          -inf          -inf -131.10906762          -inf\n",
            " -130.94050184          -inf          -inf -133.53380713          -inf\n",
            " -132.83650738          -inf          -inf -136.20178648          -inf\n",
            " -135.05866282          -inf          -inf -139.02071442          -inf\n",
            " -137.54458722          -inf          -inf -141.91812189          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          188.814        7        0.0104583        0.0105699      6.76m\n",
            "   1    18.01          3.87391        1                0                0      5.50m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-365.32846233          -inf          -inf -120.2304674           -inf\n",
            " -167.06011805          -inf          -inf -111.04426635          -inf\n",
            " -132.24360343          -inf          -inf -110.00241153          -inf\n",
            " -121.94789422          -inf          -inf -111.37266121          -inf\n",
            " -118.97298604          -inf          -inf -113.91685754          -inf\n",
            " -119.1024099           -inf          -inf -117.05518059          -inf\n",
            " -120.84283122          -inf          -inf -120.4687023           -inf\n",
            " -123.50472431          -inf          -inf -123.99053851          -inf\n",
            " -126.7018905           -inf          -inf -127.53461067          -inf\n",
            " -130.18830661          -inf          -inf -131.05608043          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          936.027        7       0.00831128       0.00873329      6.97m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          230.603        7       0.00931409       0.00918496      6.88m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-126.44601783          -inf          -inf -102.72644881          -inf\n",
            " -104.49973989          -inf          -inf -100.21853844          -inf\n",
            " -101.51563221          -inf          -inf -100.88172827          -inf\n",
            " -101.86331367          -inf          -inf -102.76485927          -inf\n",
            " -103.43849171          -inf          -inf -105.39666262          -inf\n",
            " -105.66734113          -inf          -inf -108.52733961          -inf\n",
            " -108.36111629          -inf          -inf -111.97355718          -inf\n",
            " -111.4294426           -inf          -inf -115.59753227          -inf\n",
            " -114.78815532          -inf          -inf -119.30111009          -inf\n",
            " -118.35059128          -inf          -inf -123.01782965          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          2930.33        7       0.00419994       0.00402595      8.04m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ -80.83363201          -inf          -inf  -58.11313282          -inf\n",
            "  -62.62125001          -inf          -inf  -58.64561962          -inf\n",
            "  -61.6425429           -inf          -inf  -62.47065116          -inf\n",
            "  -64.16435572          -inf          -inf  -67.76458885          -inf\n",
            "  -68.44667232          -inf          -inf  -73.65315827          -inf\n",
            "  -73.75937716          -inf          -inf  -79.68781107          -inf\n",
            "  -79.6276931           -inf          -inf  -85.64785631          -inf\n",
            "  -85.7324293           -inf          -inf  -91.42895095          -inf\n",
            "  -91.8678064           -inf          -inf  -96.98595897          -inf\n",
            "  -97.90851396          -inf          -inf -102.30405324          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          243.422        7       0.00972554       0.00941162      7.06m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-129.05062008          -inf          -inf -125.32470198          -inf\n",
            " -125.94317315          -inf          -inf -125.41402838          -inf\n",
            " -126.99788339          -inf          -inf -127.13895682          -inf\n",
            " -128.67752328          -inf          -inf -129.26654578          -inf\n",
            " -130.64194842 -127.5593589  -126.64823028 -131.61764018 -126.03346257\n",
            " -132.74291564 -128.87930268 -126.97530308 -134.12500179 -126.24922062\n",
            " -134.95496884 -130.86701542 -127.79159232 -136.73756238 -126.8655869\n",
            " -137.2955996  -131.87549263 -128.75896671 -139.4149923  -127.63247481\n",
            " -139.75893269 -133.36038813 -129.63937756 -142.12695247 -128.34564352\n",
            " -142.31752593 -135.22378057 -130.60918746 -144.85124078 -129.11485758]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          654.997        7       0.00710093       0.00675298      8.04m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ -60.93314751          -inf          -inf  -60.58109364          -inf\n",
            "  -62.40323428          -inf          -inf  -63.29667901          -inf\n",
            "  -65.63861965  -62.71884619  -61.71259821  -67.52108709  -61.26193449\n",
            "  -69.93652139  -64.19445034  -62.28376981  -72.62671126  -61.70282773\n",
            "  -74.78373915  -67.22115006  -63.5057058   -78.17524501  -62.60602484\n",
            "  -79.95006918  -70.46860728  -65.35344175  -83.86098129  -63.99550237\n",
            "  -85.34005757  -74.03814938  -67.46321962  -89.50211509  -65.60972284\n",
            "  -90.86314931  -77.59950543  -69.69245458  -95.00103272  -67.34951681\n",
            "  -96.42238144  -81.57334532  -72.09970881 -100.31022406  -69.24437081\n",
            " -101.9353555   -85.3938044   -74.66987859 -105.41041857  -71.28673482]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81           351324        7       0.00070842       0.00105368      9.37m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-4.45788556e+06            -inf            -inf -3.44231350e+03\n",
            "            -inf -1.11469386e+06            -inf            -inf\n",
            " -1.87521876e+03            -inf -4.95600685e+05            -inf\n",
            "            -inf -1.36206020e+03            -inf -2.78925213e+05\n",
            "            -inf            -inf -1.10948737e+03            -inf\n",
            " -1.78639167e+05            -inf            -inf -9.60311270e+02\n",
            "            -inf -1.24165434e+05            -inf            -inf\n",
            " -8.62460828e+02            -inf -9.13215965e+04            -inf\n",
            "            -inf -7.93719657e+02            -inf -7.00063812e+04\n",
            "            -inf            -inf -7.43027878e+02            -inf\n",
            " -5.53940921e+04            -inf            -inf -7.04269501e+02\n",
            "            -inf -4.49430925e+04            -inf            -inf\n",
            " -6.73792620e+02            -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          15.6559       31                0                0      8.11m\n",
            "(442, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KernelDensity, KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from gplearn.genetic import SymbolicRegressor\n",
        "\n",
        "def augment_dataset(df, n_samples):\n",
        "    # Get column names and data types\n",
        "    col_names = df.columns.tolist()\n",
        "    dtypes = df.dtypes.tolist()\n",
        "\n",
        "    # Create a list to store new data samples\n",
        "    new_data = []\n",
        "\n",
        "    # Loop through each column and generate new values\n",
        "    for col_name, dtype in zip(col_names, dtypes):\n",
        "        # Get the data from the column\n",
        "        col_data = df[col_name].values\n",
        "\n",
        "        # Scale the column data to the range [0, 1]\n",
        "        scaler = MinMaxScaler()\n",
        "        col_data_scaled = scaler.fit_transform(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Fit a symbolic regressor to the column data\n",
        "        est_gp = SymbolicRegressor(population_size=5000, tournament_size=50,\n",
        "                                    generations=50, stopping_criteria=0.01,\n",
        "                                    p_crossover=0.7, p_subtree_mutation=0.1,\n",
        "                                    p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
        "                                    max_samples=0.9, verbose=1,\n",
        "                                    parsimony_coefficient=0.001, random_state=0, warm_start=True)\n",
        "        est_gp.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the symbolic regressor\n",
        "        new_col_data = est_gp.predict(np.random.rand(n_samples, 1))\n",
        "\n",
        "        # Reshape the new column data to be a 2D array\n",
        "        new_col_data = new_col_data.reshape(-1, 1)\n",
        "\n",
        "        # Scale the new column data using the scaler fitted to the original column data\n",
        "        new_col_data_scaled = scaler.transform(new_col_data)\n",
        "\n",
        "        # Inverse transform the scaled new column data to get the original scale\n",
        "        new_col_data = scaler.inverse_transform(new_col_data_scaled)\n",
        "\n",
        "        # Fit a kernel density estimator to the original column data\n",
        "        kde = KernelDensity(kernel='gaussian', bandwidth=0.1)\n",
        "        kde.fit(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Generate new values using the kernel density estimator\n",
        "        kde_samples = kde.sample(n_samples)\n",
        "        kde_samples = np.squeeze(kde_samples)\n",
        "\n",
        "        # Fit a KNN regressor to the original column data\n",
        "        knn = KNeighborsRegressor(n_neighbors=5)\n",
        "        knn.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the KNN regressor\n",
        "        knn_samples = knn.predict(np.random.rand(n_samples, 1))\n",
        "        knn_samples = np.squeeze(knn_samples)\n",
        "\n",
        "        # Use a grid search to find the best kernel and bandwidth for the kernel density estimator\n",
        "        params = {'kernel': ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear'],\n",
        "                  'bandwidth': np.linspace(0.1, 1.0, 10)}\n",
        "        grid = GridSearchCV(KernelDensity(), params, cv=5)\n",
        "        print(\"DATA SHAPE HERE: \", print(col_data.reshape(-1, 1).shape))\n",
        "        grid.fit(col_data.reshape(-1, 1))\n",
        "        # grid.fit(col_data)\n",
        "        kde_best = grid.best_estimator_\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        if kde_best.kernel in [\"gaussian\", \"tophat\"]:\n",
        "            kde_best_samples = kde_best.sample(n_samples)\n",
        "        else:\n",
        "            # Evaluate log density for a grid of points\n",
        "            x_grid = np.linspace(col_data.min(), col_data.max(), 1000).reshape(-1, 1)\n",
        "            log_dens = kde_best.score_samples(x_grid)\n",
        "\n",
        "            # Sample from the grid according to the log densities\n",
        "            probs = np.exp(log_dens - log_dens.max())\n",
        "            probs /= probs.sum()\n",
        "            kde_best_samples = np.random.choice(x_grid.flatten(), size=n_samples, p=probs)\n",
        "            \n",
        "        kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        # kde_best_samples = kde_best.sample(n_samples)\n",
        "        # kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Clip the new values to the range of the original column data\n",
        "        kde_best_samples = np.clip(kde_best_samples, np.min(col_data), np.max(col_data))\n",
        "\n",
        "        # Add new values to the list of new data\n",
        "        new_data.append(kde_best_samples.flatten())\n",
        "\n",
        "    # Transpose the list of new data to match the shape of the original dataset\n",
        "    new_data = np.array(new_data).T\n",
        "\n",
        "    # Convert the new data to a dataframe and append it to the original dataset\n",
        "    new_df = pd.DataFrame(new_data, columns=col_names)\n",
        "    augmented_df = pd.concat([df, new_df], ignore_index=True)\n",
        "\n",
        "    # return augmented_df\n",
        "    return new_df\n",
        "\n",
        "print(np.seterr())\n",
        "np.seterr(invalid='warn')\n",
        "np.seterr(under='warn')\n",
        "print(np.seterr())\n",
        "new_df = augment_dataset(regression_df, df.shape[0]*10)"
      ],
      "id": "4YqEGUD4oDrP"
    },
    {
      "cell_type": "code",
      "source": [
        "# def reverse_one_hot_encode(df):\n",
        "#     # Get a list of all columns with '_'\n",
        "#     oh_cols = [col for col in df.columns if '_' in col]\n",
        "\n",
        "#     # Get the original column names and values for each one-hot encoded column\n",
        "#     col_vals = {}\n",
        "#     for col in oh_cols:\n",
        "#         col_name = col.split('_')[0]\n",
        "#         col_val = col.split('_')[1]\n",
        "#         if col_name not in col_vals:\n",
        "#             col_vals[col_name] = {}\n",
        "#         col_vals[col_name][col_val] = col\n",
        "\n",
        "#     # Create a new dataframe to store the reverse one-hot encoded values\n",
        "#     new_df = pd.DataFrame(columns=col_vals.keys())\n",
        "\n",
        "#     # Iterate through each row of the original dataframe\n",
        "#     for index, row in df.iterrows():\n",
        "#         # Initialize a dictionary to store the values for this row\n",
        "#         new_row = {}\n",
        "\n",
        "#         # Iterate through each original column\n",
        "#         for col_name in df.columns:\n",
        "#             # If this is a one-hot encoded column, find the closest value to 1 and use that\n",
        "#             if col_name in oh_cols:\n",
        "#                 col_val = col_name.split('_')[1]\n",
        "#                 col_key = col_vals[col_name.split('_')[0]][col_val]\n",
        "                \n",
        "#                 # Get the numeric values for the one-hot encoded column\n",
        "#                 numeric_values = pd.to_numeric(row[col_key], errors='coerce')\n",
        "#                 # Set values < 0.5 to 0 and values >= 0.5 to the numeric value\n",
        "#                 numeric_values = np.where(numeric_values >= 0.5, numeric_values, 0)\n",
        "#                 # Set the value closest to 1 to 1, and the rest to 0\n",
        "#                 numeric_values = np.where(numeric_values == np.max(numeric_values), 1, 0)\n",
        "#                 # Combine the column name and the value (if it's 1) and add it to the new_row dictionary\n",
        "#                 new_row[col_name.split('_')[0]] = col_val if np.sum(numeric_values) > 0 else np.nan\n",
        "#             # If this is not a one-hot encoded column, use the original value\n",
        "#             else:\n",
        "#                 new_row[col_name] = row[col_name]\n",
        "\n",
        "#         # Append the values for this row to the new dataframe\n",
        "#         new_df = new_df.append(new_row, ignore_index=True)\n",
        "\n",
        "#     # Return the new dataframe\n",
        "#     return new_df"
      ],
      "metadata": {
        "id": "er5lGyrRnbUt"
      },
      "id": "er5lGyrRnbUt",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x0qeIOSFMs8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d83dfc-f63a-4b40-e2df-903aa082545f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Old:  442\n",
            "New:  4420\n",
            "            Time        V1        V2        V3        V4        V5        V6  \\\n",
            "Time    0.000000  0.256754 -0.584525  0.190153  0.156236  0.282025  0.132080   \n",
            "V1      0.256754  0.000000  0.811608  0.899049  0.562569  0.894954  0.242762   \n",
            "V2     -0.584525  0.811608  0.000000  0.871643  0.640047  0.830020  0.099833   \n",
            "V3      0.190153  0.899049  0.871643  0.000000  0.009895 -0.002537 -0.033100   \n",
            "V4      0.156236  0.562569  0.640047  0.009895  0.000000  0.012138 -0.013559   \n",
            "V5      0.282025  0.894954  0.830020 -0.002537  0.012138  0.000000 -0.057528   \n",
            "V6      0.132080  0.242762  0.099833 -0.033100 -0.013559 -0.057528  0.000000   \n",
            "V7      0.247857  0.869294  0.860417  0.863345  0.654886  0.830063  0.171098   \n",
            "V8     -0.418271  0.089929 -0.821332  0.196216  0.075569  0.213816  0.729056   \n",
            "V9     -0.742385  0.610714 -0.195634  0.719693  0.806611  0.637280  0.214735   \n",
            "V10    -0.631443  0.682112 -0.111762  0.804161  0.710183  0.742817  0.288622   \n",
            "V11     0.329376  0.410842  0.473794  0.033646 -0.006693  0.042797  0.044126   \n",
            "V12     0.279472  0.488199  0.564596 -0.007314  0.004106 -0.014967 -0.028063   \n",
            "V13    -0.777006  0.107914 -0.815892  0.170685  0.172614  0.188005  0.194360   \n",
            "V14    -0.768524  0.202426 -0.541543  0.452424  0.626876  0.250278  0.461104   \n",
            "V15    -0.771593  0.126101 -0.688403  0.184265  0.193796  0.076597  0.070728   \n",
            "V16     0.271986  0.552213  0.543541 -0.009884  0.016317 -0.023324 -0.032948   \n",
            "V17    -0.627140  0.593839 -0.321476  0.634529  0.599221  0.701772  0.306868   \n",
            "V18    -0.608215  0.629269 -0.301309  0.634167  0.575099  0.717072  0.254279   \n",
            "V19     0.049945  0.245500  0.123572  0.025388 -0.003687  0.034810  0.042137   \n",
            "V20    -0.767546  0.292147 -0.583941  0.335352  0.305521  0.329057 -0.048383   \n",
            "V21    -0.492706  0.090188 -0.704244  0.098678  0.180364  0.082667 -0.037113   \n",
            "V22    -0.644665  0.038300 -0.898384  0.077459  0.193513  0.101834  0.050242   \n",
            "V23     0.061553  0.009992  0.179389  0.031191  0.034832  0.094756  0.422702   \n",
            "V24    -0.881654  0.172377 -0.821579  0.067659  0.003354  0.260379  0.095460   \n",
            "V25    -0.694147  0.084004 -0.790977  0.110885  0.065981  0.094395  0.172952   \n",
            "V26    -0.922192  0.084582 -0.906789  0.020530  0.216879  0.099884 -0.026937   \n",
            "V27    -0.590354  0.216226 -0.609140  0.149113  0.106747  0.213275  0.193317   \n",
            "V28    -0.801850  0.251855 -0.848039  0.233433  0.219103  0.222422 -0.033626   \n",
            "Amount -0.550267  0.037758 -0.617486  0.047692  0.124322  0.030767  0.124535   \n",
            "Class        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "\n",
            "              V7        V8        V9  ...       V21       V22       V23  \\\n",
            "Time    0.247857 -0.418271 -0.742385  ... -0.492706 -0.644665  0.061553   \n",
            "V1      0.869294  0.089929  0.610714  ...  0.090188  0.038300  0.009992   \n",
            "V2      0.860417 -0.821332 -0.195634  ... -0.704244 -0.898384  0.179389   \n",
            "V3      0.863345  0.196216  0.719693  ...  0.098678  0.077459  0.031191   \n",
            "V4      0.654886  0.075569  0.806611  ...  0.180364  0.193513  0.034832   \n",
            "V5      0.830063  0.213816  0.637280  ...  0.082667  0.101834  0.094756   \n",
            "V6      0.171098  0.729056  0.214735  ... -0.037113  0.050242  0.422702   \n",
            "V7      0.000000  0.083501  0.759889  ...  0.171194  0.184938 -0.728279   \n",
            "V8      0.083501  0.000000 -0.793872  ... -0.864308 -0.796824  0.401896   \n",
            "V9      0.759889 -0.793872  0.000000  ... -0.458827 -0.567088  0.097834   \n",
            "V10     0.855633 -0.802762 -0.131289  ... -0.524911 -0.579365  0.081304   \n",
            "V11     0.509168  0.207322  0.590230  ...  0.070896 -0.004781  0.020428   \n",
            "V12     0.616817  0.162118  0.693063  ... -0.012909  0.142091 -0.015289   \n",
            "V13     0.083350 -0.415426 -0.847211  ... -0.674651 -0.897817  0.128448   \n",
            "V14     0.330975 -0.555300 -0.460965  ... -0.515211 -0.782411  0.010544   \n",
            "V15     0.242578 -0.640155 -0.795522  ... -0.553632 -0.792033  0.083351   \n",
            "V16     0.657675  0.163024  0.672448  ...  0.068150  0.146772 -0.023996   \n",
            "V17     0.708304 -0.527833 -0.264163  ... -0.673148 -0.710764  0.008989   \n",
            "V18     0.732763 -0.565632 -0.280265  ... -0.687947 -0.714618  0.021413   \n",
            "V19     0.294413  0.196712  0.263887  ...  0.054049  0.134089 -0.012219   \n",
            "V20     0.415904 -0.721865 -0.454220  ... -0.258517 -0.433409  0.192817   \n",
            "V21     0.171194 -0.864308 -0.458827  ...  0.000000 -0.088904  0.044819   \n",
            "V22     0.184938 -0.796824 -0.567088  ... -0.088904  0.000000  0.080507   \n",
            "V23    -0.728279  0.401896  0.097834  ...  0.044819  0.080507  0.000000   \n",
            "V24     0.161715 -0.694799 -0.917941  ... -0.702686 -0.808250  0.046085   \n",
            "V25     0.071212 -0.576382 -0.988482  ... -0.645890 -0.635860  0.134730   \n",
            "V26     0.061495 -0.763713 -0.774144  ... -0.749884 -0.882164  0.055884   \n",
            "V27     0.300027 -0.592671 -0.750277  ... -0.443122 -0.472097  0.224165   \n",
            "V28     0.242192 -0.850506 -0.717948  ... -0.497768 -0.632323  0.068813   \n",
            "Amount  0.173342 -0.597238 -0.487720  ... -0.671851 -0.767806  0.157503   \n",
            "Class        NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
            "\n",
            "             V24       V25       V26       V27       V28    Amount  Class  \n",
            "Time   -0.881654 -0.694147 -0.922192 -0.590354 -0.801850 -0.550267    NaN  \n",
            "V1      0.172377  0.084004  0.084582  0.216226  0.251855  0.037758    NaN  \n",
            "V2     -0.821579 -0.790977 -0.906789 -0.609140 -0.848039 -0.617486    NaN  \n",
            "V3      0.067659  0.110885  0.020530  0.149113  0.233433  0.047692    NaN  \n",
            "V4      0.003354  0.065981  0.216879  0.106747  0.219103  0.124322    NaN  \n",
            "V5      0.260379  0.094395  0.099884  0.213275  0.222422  0.030767    NaN  \n",
            "V6      0.095460  0.172952 -0.026937  0.193317 -0.033626  0.124535    NaN  \n",
            "V7      0.161715  0.071212  0.061495  0.300027  0.242192  0.173342    NaN  \n",
            "V8     -0.694799 -0.576382 -0.763713 -0.592671 -0.850506 -0.597238    NaN  \n",
            "V9     -0.917941 -0.988482 -0.774144 -0.750277 -0.717948 -0.487720    NaN  \n",
            "V10    -0.860738 -0.931211 -0.931142 -0.716229 -0.714392 -0.472216    NaN  \n",
            "V11     0.092828  0.007009  0.201355  0.153050  0.026383  0.097964    NaN  \n",
            "V12     0.035084  0.051848  0.125952  0.048244  0.081483  0.104647    NaN  \n",
            "V13    -0.898585 -0.946963 -0.904118 -0.830918 -0.816199 -0.693026    NaN  \n",
            "V14    -0.827738 -0.855954 -0.715051 -0.724367 -0.889614 -0.502944    NaN  \n",
            "V15    -0.958840 -0.974916 -0.873134 -0.739564 -0.843529 -0.547593    NaN  \n",
            "V16     0.207756  0.094082  0.039976  0.046962  0.107780  0.024487    NaN  \n",
            "V17    -0.741787 -0.921100 -0.940264 -0.849512 -0.829777 -0.588153    NaN  \n",
            "V18    -0.713985 -0.880923 -0.952909 -0.796075 -0.782380 -0.590798    NaN  \n",
            "V19     0.230088  0.184071  0.053615 -0.024345  0.071695  0.071902    NaN  \n",
            "V20    -0.856461 -0.893476 -0.931014 -0.686211 -0.923561 -0.795159    NaN  \n",
            "V21    -0.702686 -0.645890 -0.749884 -0.443122 -0.497768 -0.671851    NaN  \n",
            "V22    -0.808250 -0.635860 -0.882164 -0.472097 -0.632323 -0.767806    NaN  \n",
            "V23     0.046085  0.134730  0.055884  0.224165  0.068813  0.157503    NaN  \n",
            "V24     0.000000 -0.857419 -0.747309 -0.632542 -0.935612 -0.569721    NaN  \n",
            "V25    -0.857419  0.000000 -0.860053 -0.706173 -0.789236 -0.546892    NaN  \n",
            "V26    -0.747309 -0.860053  0.000000 -0.672934 -0.943942 -0.693046    NaN  \n",
            "V27    -0.632542 -0.706173 -0.672934  0.000000 -0.719671 -0.460416    NaN  \n",
            "V28    -0.935612 -0.789236 -0.943942 -0.719671  0.000000 -0.583852    NaN  \n",
            "Amount -0.569721 -0.546892 -0.693046 -0.460416 -0.583852  0.000000    NaN  \n",
            "Class        NaN       NaN       NaN       NaN       NaN       NaN    NaN  \n",
            "\n",
            "[31 rows x 31 columns]\n",
            "               Time         V1         V2         V3         V4         V5  \\\n",
            "0     146021.967968   0.090199   5.349005 -25.366025  11.129726 -16.655750   \n",
            "1      61646.360360  -5.960571   2.696342  -3.361092   1.935818  -2.744530   \n",
            "2       7550.708709  -3.656272   0.440055  -1.928025   4.590600   0.601609   \n",
            "3       7550.708709  -8.391091   0.257112  -2.243027   3.715844  -1.747432   \n",
            "4     170348.000000  -1.864098  13.398463  -4.512782   5.008683  -2.478585   \n",
            "...             ...        ...        ...        ...        ...        ...   \n",
            "4415  146021.967968  -1.719215   6.416168  -3.313677   5.621959  -2.088975   \n",
            "4416  142279.501502   1.997554   3.946447   1.131966   2.620690  -0.638343   \n",
            "4417   61646.360360   0.827265   2.787813 -20.451522  12.069715  -6.913932   \n",
            "4418    7550.708709 -15.251818   0.409564 -20.584549   6.051630 -15.849594   \n",
            "4419    7550.708709 -22.787413  -0.230734  -2.365807   2.804271  -0.357598   \n",
            "\n",
            "            V6         V7         V8        V9  ...       V21       V22  \\\n",
            "0     3.948718 -15.690459   2.284560 -0.696334  ...  1.476885  0.730294   \n",
            "1    -1.726801  -2.053524   0.695632 -2.242339  ...  0.626027  0.039644   \n",
            "2    -0.494693  -3.338162  -1.259970 -4.897433  ... -0.475084 -0.875469   \n",
            "3    -1.326695  -1.954705  -1.504420 -5.183108  ... -0.625235 -0.944534   \n",
            "4    -0.671165  -5.314530  10.901434  1.118540  ...  3.028451  1.869868   \n",
            "...        ...        ...        ...       ...  ...       ...       ...   \n",
            "4415 -0.951025   4.270853   2.895685 -0.360247  ...  1.727138  0.885691   \n",
            "4416  0.816416  -2.646434   1.490096 -1.284488  ...  1.076482  0.454034   \n",
            "4417 -1.705000  -1.707659   0.756745 -2.141512  ...  0.676078  0.074176   \n",
            "4418 -2.604122  -2.349979  -1.321083 -4.947847  ... -0.525134 -0.892735   \n",
            "4419 -2.105530 -15.097549  -2.359997 -5.972915  ... -0.975589 -1.151729   \n",
            "\n",
            "           V23       V24       V25       V26       V27       V28      Amount  \\\n",
            "0    -0.967549  0.310789  0.521978  0.409710  0.842558  0.394173  108.527898   \n",
            "1     0.170735 -0.057675  0.074182  0.012004  0.357228  0.140042   17.023984   \n",
            "2    -0.076718 -0.610372 -0.639493 -0.397400 -0.499843 -0.407592    0.000000   \n",
            "3     0.170735 -0.654088 -0.702464 -0.424693 -0.830281 -0.472019    0.000000   \n",
            "4    -0.299426  0.713602  1.445557  0.889297  1.916479  0.920330  721.391321   \n",
            "...        ...       ...       ...       ...       ...       ...         ...   \n",
            "4415  5.070305  0.385731  0.633927  0.491591  1.049081  0.458600  172.367838   \n",
            "4416  0.047009  0.164028  0.340061  0.249848  0.605056  0.293952   97.887908   \n",
            "4417  0.244971 -0.035817  0.102169  0.035398  0.377881  0.154359   19.151982   \n",
            "4418  0.096499 -0.619740 -0.646490 -0.401299 -0.530822 -0.418330    0.000000   \n",
            "4419 -0.893313 -0.772746 -0.919366 -0.502675 -1.429198 -0.668881    0.000000   \n",
            "\n",
            "      Class  \n",
            "0       1.0  \n",
            "1       1.0  \n",
            "2       1.0  \n",
            "3       1.0  \n",
            "4       1.0  \n",
            "...     ...  \n",
            "4415    1.0  \n",
            "4416    1.0  \n",
            "4417    1.0  \n",
            "4418    1.0  \n",
            "4419    1.0  \n",
            "\n",
            "[4420 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# reprocess the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# just brings columns back to normal and replaces values with NaN\n",
        "def reverse_one_hot_encode(df):\n",
        "    oh_cols = [col for col in df.columns if \"_\" in col and col.split(\"_\")[0] in categorical_cols]\n",
        "    print(oh_cols)\n",
        "    new_df = df.drop(columns=oh_cols).copy()\n",
        "    for col in oh_cols:\n",
        "        col_name, val = col.split(\"_\")\n",
        "        mask = df[col] == 1\n",
        "        new_df[col_name] = np.nan\n",
        "        new_df.loc[mask, col_name] = val\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_significantly_correlated_colmns(df):\n",
        "  print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "  rows, cols = df.shape\n",
        "  corr = df.corr().values\n",
        "  fields = list(df.columns)\n",
        "  for i in range(cols):\n",
        "      for j in range(i+1, cols-1):\n",
        "        corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "        if corr[i,j] > 0.75:\n",
        "            print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "\n",
        "new_df = reverse_one_hot_encode(new_df)\n",
        "corr1 = regression_df.corr()\n",
        "corr2 = new_df.corr()\n",
        "# print a matrix to see the difference in correlations\n",
        "diff = np.abs(corr1) - np.abs(corr2)\n",
        "\n",
        "print(\"Old: \", len(regression_df))\n",
        "print(\"New: \", len(new_df))\n",
        "print(diff)\n",
        "\n",
        "# print(print_significantly_correlated_colmns(regression_df))\n",
        "# print(print_significantly_correlated_colmns(new_df))\n",
        "new_df.head()\n",
        "print(new_df)"
      ],
      "id": "x0qeIOSFMs8x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIh2Sn5ohWnz"
      },
      "source": [
        "## Categorical Variable Generation: Hill Climbing Algorithm\n",
        "In this implementation, we use a hill-climbing algorithm to search for the best categorical variable distribution that matches the original distribution and is correlated with the numerical variables in the generated data. The algorithm starts with the original distribution and iteratively perturbs it by swapping two values, then computes the correlation with the numerical variables and updates the distribution if the correlation improves. The algorithm repeats this process for a fixed number of iterations (in this case, 10), then moves on to the next categorical variable.\n",
        "\n",
        "This is an example of an explicit search or optimization technique for program synthesis, as we are searching for the best program (i.e., categorical variable distribution) that satisfies certain constraints (i.e., matching the original distribution and being correlated with the numerical variables)."
      ],
      "id": "cIh2Sn5ohWnz"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Hp-B6m8qSRXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9b30456-6544-401a-b8d0-ec55bfb76d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Time        V1        V2        V3        V4        V5        V6  \\\n",
            "Time    1.000000  0.283974 -0.261868  0.221733 -0.183578  0.337220  0.165930   \n",
            "V1      0.283974  1.000000 -0.828858  0.909019 -0.576283  0.898603  0.252016   \n",
            "V2     -0.261868 -0.828858  1.000000 -0.886474  0.645409 -0.840797 -0.174114   \n",
            "V3      0.221733  0.909019 -0.886474  1.000000 -0.729313  0.887049  0.410435   \n",
            "V4     -0.183578 -0.576283  0.645409 -0.729313  1.000000 -0.562645 -0.306189   \n",
            "V5      0.337220  0.898603 -0.840797  0.887049 -0.562645  1.000000  0.231766   \n",
            "V6      0.165930  0.252016 -0.174114  0.410435 -0.306189  0.231766  1.000000   \n",
            "V7      0.251377  0.898754 -0.868443  0.883155 -0.692188  0.845797  0.175529   \n",
            "V8     -0.205305 -0.106826 -0.013696 -0.210214  0.100499 -0.239239 -0.744850   \n",
            "V9      0.149949  0.637788 -0.700260  0.733561 -0.822482  0.666312  0.242068   \n",
            "V10     0.231540  0.709794 -0.749752  0.815773 -0.727467  0.766850  0.314013   \n",
            "V11    -0.356021 -0.411017  0.516903 -0.610780  0.720897 -0.451231 -0.468027   \n",
            "V12     0.293054  0.494487 -0.585094  0.658766 -0.777049  0.572077  0.441688   \n",
            "V13    -0.167851 -0.133948  0.124224 -0.181419  0.189974 -0.218863 -0.242452   \n",
            "V14     0.162164  0.228929 -0.388163  0.460434 -0.642719  0.277553  0.505694   \n",
            "V15    -0.142005  0.152488 -0.236753  0.195815 -0.210025  0.105545 -0.107269   \n",
            "V16     0.284078  0.568905 -0.546954  0.630926 -0.621955  0.673648  0.358297   \n",
            "V17     0.299928  0.621562 -0.567672  0.647639 -0.615901  0.732487  0.345539   \n",
            "V18     0.321099  0.657308 -0.577372  0.650410 -0.593308  0.750312  0.287541   \n",
            "V19    -0.103493 -0.271849  0.151119 -0.260522  0.245338 -0.401581 -0.202035   \n",
            "V20    -0.018284 -0.313023  0.365665 -0.363802  0.309375 -0.340592  0.015567   \n",
            "V21    -0.070679  0.109167 -0.071534  0.133361 -0.184658  0.109877 -0.000832   \n",
            "V22     0.142075 -0.059345  0.023577 -0.080513  0.197429 -0.115404  0.101324   \n",
            "V23     0.064345 -0.048118  0.181508 -0.046216  0.059698 -0.110505  0.433804   \n",
            "V24    -0.046539 -0.199116  0.093194 -0.080922 -0.018467 -0.291890 -0.131547   \n",
            "V25    -0.171098 -0.107737  0.142213 -0.121713 -0.078078 -0.122249 -0.207125   \n",
            "V26     0.003898  0.109811 -0.058354  0.020962  0.230264  0.122489 -0.027114   \n",
            "V27    -0.178371  0.241111 -0.235476  0.155556 -0.115302  0.230997 -0.206765   \n",
            "V28     0.053249  0.275622 -0.067707  0.236195 -0.231203  0.238588 -0.000210   \n",
            "Amount  0.066006  0.044805 -0.264083  0.086895 -0.132097 -0.049149  0.220925   \n",
            "Class        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "\n",
            "              V7        V8        V9  ...       V21       V22       V23  \\\n",
            "Time    0.251377 -0.205305  0.149949  ... -0.070679  0.142075  0.064345   \n",
            "V1      0.898754 -0.106826  0.637788  ...  0.109167 -0.059345 -0.048118   \n",
            "V2     -0.868443 -0.013696 -0.700260  ... -0.071534  0.023577  0.181508   \n",
            "V3      0.883155 -0.210214  0.733561  ...  0.133361 -0.080513 -0.046216   \n",
            "V4     -0.692188  0.100499 -0.822482  ... -0.184658  0.197429  0.059698   \n",
            "V5      0.845797 -0.239239  0.666312  ...  0.109877 -0.115404 -0.110505   \n",
            "V6      0.175529 -0.744850  0.242068  ... -0.000832  0.101324  0.433804   \n",
            "V7      1.000000  0.101213  0.764965  ...  0.172029 -0.185917 -0.088743   \n",
            "V8      0.101213  1.000000 -0.053345  ...  0.008786 -0.085814 -0.415127   \n",
            "V9      0.764965 -0.053345  1.000000  ...  0.310812 -0.345596 -0.101507   \n",
            "V10     0.861571 -0.034285  0.860793  ...  0.226620 -0.308356 -0.084666   \n",
            "V11    -0.537511  0.217307 -0.614115  ...  0.082224  0.019122 -0.041391   \n",
            "V12     0.644172 -0.188941  0.709262  ...  0.005192 -0.159785 -0.006273   \n",
            "V13    -0.088699  0.374716 -0.130555  ...  0.049296 -0.007783 -0.133663   \n",
            "V14     0.336159 -0.251532  0.526575  ... -0.211234  0.118790  0.015289   \n",
            "V15     0.248533  0.197312  0.200599  ...  0.204080 -0.124434 -0.086640   \n",
            "V16     0.687646 -0.205567  0.673153  ... -0.091924 -0.152850  0.002735   \n",
            "V17     0.713301 -0.248173  0.724999  ... -0.029928 -0.167941  0.014542   \n",
            "V18     0.737949 -0.204726  0.707062  ... -0.016994 -0.161580  0.026840   \n",
            "V19    -0.314786  0.243100 -0.292190  ...  0.108145  0.178765 -0.001614   \n",
            "V20    -0.420421 -0.129893 -0.438328  ... -0.644340  0.527228  0.202952   \n",
            "V21     0.172029  0.008786  0.310812  ...  1.000000 -0.828793  0.056943   \n",
            "V22    -0.185917 -0.085814 -0.345596  ... -0.828793  1.000000  0.090313   \n",
            "V23    -0.088743 -0.415127 -0.101507  ...  0.056943  0.090313  1.000000   \n",
            "V24    -0.165672  0.118902 -0.076479  ... -0.037442  0.100448 -0.051142   \n",
            "V25     0.077623  0.308886 -0.001056  ...  0.163636 -0.308931  0.137399   \n",
            "V26     0.065620  0.046008 -0.185893  ...  0.026053  0.048988  0.061485   \n",
            "V27     0.306121  0.318150  0.218819  ...  0.370041 -0.425094 -0.224276   \n",
            "V28     0.250886 -0.011718  0.271532  ...  0.297051 -0.289399  0.070468   \n",
            "Amount  0.182598  0.034826  0.126840  ...  0.013865 -0.007179 -0.163475   \n",
            "Class        NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
            "\n",
            "             V24       V25       V26       V27       V28    Amount  Class  \n",
            "Time   -0.046539 -0.171098  0.003898 -0.178371  0.053249  0.066006    NaN  \n",
            "V1     -0.199116 -0.107737  0.109811  0.241111  0.275622  0.044805    NaN  \n",
            "V2      0.093194  0.142213 -0.058354 -0.235476 -0.067707 -0.264083    NaN  \n",
            "V3     -0.080922 -0.121713  0.020962  0.155556  0.236195  0.086895    NaN  \n",
            "V4     -0.018467 -0.078078  0.230264 -0.115302 -0.231203 -0.132097    NaN  \n",
            "V5     -0.291890 -0.122249  0.122489  0.230997  0.238588 -0.049149    NaN  \n",
            "V6     -0.131547 -0.207125 -0.027114 -0.206765 -0.000210  0.220925    NaN  \n",
            "V7     -0.165672  0.077623  0.065620  0.306121  0.250886  0.182598    NaN  \n",
            "V8      0.118902  0.308886  0.046008  0.318150 -0.011718  0.034826    NaN  \n",
            "V9     -0.076479 -0.001056 -0.185893  0.218819  0.271532  0.126840    NaN  \n",
            "V10    -0.119727  0.043821 -0.002720  0.253740  0.269438  0.092826    NaN  \n",
            "V11    -0.122181  0.036298  0.231995  0.171917 -0.054528 -0.141916    NaN  \n",
            "V12    -0.056075  0.070050 -0.143593  0.056408  0.100748  0.125873    NaN  \n",
            "V13     0.094279  0.024264  0.084744  0.073191 -0.151807 -0.017413    NaN  \n",
            "V14     0.168811 -0.122861 -0.266372 -0.203844 -0.088647  0.175146    NaN  \n",
            "V15     0.039463 -0.015452  0.104165  0.212319  0.144568  0.117337    NaN  \n",
            "V16    -0.210901  0.094953 -0.041254  0.055782  0.113382  0.041360    NaN  \n",
            "V17    -0.252910  0.047732 -0.023923  0.078083  0.145793  0.028192    NaN  \n",
            "V18    -0.278454  0.082898  0.008043  0.126636  0.189559  0.015895    NaN  \n",
            "V19     0.259045 -0.210905  0.090816 -0.001556 -0.089541  0.079378    NaN  \n",
            "V20    -0.037509  0.028720  0.018196 -0.177395 -0.000243  0.063953    NaN  \n",
            "V21    -0.037442  0.163636  0.026053  0.370041  0.297051  0.013865    NaN  \n",
            "V22     0.100448 -0.308931  0.048988 -0.425094 -0.289399 -0.007179    NaN  \n",
            "V23    -0.051142  0.137399  0.061485 -0.224276  0.070468 -0.163475    NaN  \n",
            "V24     1.000000 -0.126679 -0.228653 -0.308429 -0.044750  0.078750    NaN  \n",
            "V25    -0.126679  1.000000  0.104471  0.264251  0.200177 -0.139264    NaN  \n",
            "V26    -0.228653  0.104471  1.000000  0.218785  0.016287 -0.088354    NaN  \n",
            "V27    -0.308429  0.264251  0.218785  1.000000  0.250029  0.089289    NaN  \n",
            "V28    -0.044750  0.200177  0.016287  0.250029  1.000000 -0.091602    NaN  \n",
            "Amount  0.078750 -0.139264 -0.088354  0.089289 -0.091602  1.000000    NaN  \n",
            "Class        NaN       NaN       NaN       NaN       NaN       NaN    NaN  \n",
            "\n",
            "[31 rows x 31 columns]\n",
            "            Time        V1        V2        V3        V4        V5        V6  \\\n",
            "Time    1.000000  0.061038  0.695768  0.056384 -0.047832  0.091763 -0.007902   \n",
            "V1      0.061038  1.000000 -0.124386  0.142019 -0.084480  0.144781  0.033426   \n",
            "V2      0.695768 -0.124386  1.000000 -0.159270  0.103154 -0.129590 -0.090610   \n",
            "V3      0.056384  0.142019 -0.159270  1.000000 -0.721044  0.889170  0.438176   \n",
            "V4     -0.047832 -0.084480  0.103154 -0.721044  1.000000 -0.552488 -0.317531   \n",
            "V5      0.091763  0.144781 -0.129590  0.889170 -0.552488  1.000000  0.280039   \n",
            "V6     -0.007902  0.033426 -0.090610  0.438176 -0.317531  0.280039  1.000000   \n",
            "V7      0.036490  0.175860 -0.139811  0.163683 -0.147126  0.153326  0.025197   \n",
            "V8      0.508629 -0.004149  0.690243 -0.023788 -0.003659 -0.018945 -0.137102   \n",
            "V9      0.790491  0.128955  0.629191  0.132664 -0.149857  0.133666  0.016591   \n",
            "V10     0.773837  0.143439  0.587615  0.146646 -0.137291  0.148102  0.030905   \n",
            "V11    -0.023667 -0.068200  0.121715 -0.582649  0.726484 -0.415411 -0.431033   \n",
            "V12     0.026446  0.086603 -0.113568  0.664887 -0.773606  0.584620  0.465233   \n",
            "V13     0.794533 -0.000539  0.804244 -0.020882  0.016965 -0.010012 -0.079662   \n",
            "V14     0.824563  0.060456  0.708427  0.083054 -0.120483  0.068565  0.045560   \n",
            "V15     0.775112  0.046843  0.736247  0.041133 -0.047538  0.041179 -0.047752   \n",
            "V16     0.046982  0.106193 -0.091679  0.639200 -0.608231  0.693235  0.386023   \n",
            "V17     0.838473  0.128297  0.641976  0.119463 -0.117749  0.147714  0.024931   \n",
            "V18     0.843223  0.134616  0.631661  0.122519 -0.115198  0.152772  0.019844   \n",
            "V19    -0.059861 -0.066623  0.001827 -0.239257  0.248418 -0.372395 -0.166654   \n",
            "V20     0.675308 -0.035210  0.851316 -0.084218  0.048562 -0.065967 -0.050885   \n",
            "V21     0.462102  0.035944  0.614001 -0.003173 -0.031279 -0.001637 -0.031008   \n",
            "V22     0.686987  0.006474  0.757615 -0.016998  0.038951 -0.009538 -0.023923   \n",
            "V23     0.006602  0.022867  0.030464  0.004280 -0.009941 -0.006276  0.084258   \n",
            "V24     0.793667 -0.011315  0.775968 -0.002462 -0.015676 -0.022187 -0.051808   \n",
            "V25     0.726477  0.002043  0.802360 -0.010829 -0.022944  0.003456 -0.062074   \n",
            "V26     0.800803  0.039325  0.794592  0.003807  0.026977  0.038955 -0.049662   \n",
            "V27     0.634290  0.062353  0.657206  0.031979 -0.026955  0.054052 -0.046134   \n",
            "V28     0.745493  0.065819  0.751319  0.041315 -0.048518  0.052705 -0.028343   \n",
            "Amount  0.544757  0.013154  0.697764 -0.019244 -0.014508 -0.023236 -0.046756   \n",
            "Class        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "\n",
            "              V7        V8        V9  ...       V21       V22       V23  \\\n",
            "Time    0.036490  0.508629  0.790491  ...  0.462102  0.686987  0.006602   \n",
            "V1      0.175860 -0.004149  0.128955  ...  0.035944  0.006474  0.022867   \n",
            "V2     -0.139811  0.690243  0.629191  ...  0.614001  0.757615  0.030464   \n",
            "V3      0.163683 -0.023788  0.132664  ... -0.003173 -0.016998  0.004280   \n",
            "V4     -0.147126 -0.003659 -0.149857  ... -0.031279  0.038951 -0.009941   \n",
            "V5      0.153326 -0.018945  0.133666  ... -0.001637 -0.009538 -0.006276   \n",
            "V6      0.025197 -0.137102  0.016591  ... -0.031008 -0.023923  0.084258   \n",
            "V7      1.000000  0.032058  0.132816  ...  0.033372 -0.033173  0.655197   \n",
            "V8      0.032058  1.000000  0.694164  ...  0.705511  0.702829 -0.064035   \n",
            "V9      0.132816  0.694164  1.000000  ...  0.681625  0.683652 -0.020932   \n",
            "V10     0.152257  0.686436  0.969823  ...  0.649697  0.666498 -0.017946   \n",
            "V11    -0.113290  0.028366 -0.081552  ...  0.006207  0.023028 -0.024533   \n",
            "V12     0.129601 -0.009352  0.102922  ...  0.015668 -0.043119  0.016691   \n",
            "V13    -0.010377  0.719750  0.793906  ...  0.595709  0.739625 -0.027822   \n",
            "V14     0.061114  0.626106  0.910438  ...  0.547442  0.757890 -0.001161   \n",
            "V15     0.045608  0.731482  0.867249  ...  0.654139  0.731479 -0.017609   \n",
            "V16     0.137375  0.001559  0.109485  ...  0.002504 -0.031951  0.022586   \n",
            "V17     0.125877  0.599310  0.944489  ...  0.561656  0.685380 -0.001931   \n",
            "V18     0.130247  0.602149  0.939944  ...  0.565623  0.684546  0.000392   \n",
            "V19    -0.069038  0.002015 -0.071509  ... -0.023873 -0.004730 -0.011695   \n",
            "V20    -0.067405  0.683717  0.669381  ...  0.607401  0.880721  0.027811   \n",
            "V21     0.033372  0.705511  0.681625  ...  1.000000  0.558350  0.001680   \n",
            "V22    -0.033173  0.702829  0.683652  ...  0.558350  1.000000  0.009529   \n",
            "V23     0.655197 -0.064035 -0.020932  ...  0.001680  0.009529  1.000000   \n",
            "V24    -0.024823  0.694333  0.814188  ...  0.590656  0.759855 -0.013267   \n",
            "V25     0.018271  0.788249  0.826324  ...  0.687215  0.718664  0.021843   \n",
            "V26     0.014472  0.680272  0.769830  ...  0.633362  0.770727  0.006250   \n",
            "V27     0.058485  0.806272  0.839354  ...  0.725552  0.647986 -0.041184   \n",
            "V28     0.049451  0.713606  0.869906  ...  0.699390  0.701099  0.011119   \n",
            "Amount  0.037284  0.534491  0.536625  ...  0.561926  0.637583 -0.032602   \n",
            "Class        NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
            "\n",
            "             V24       V25       V26       V27       V28    Amount  Class  \n",
            "Time    0.793667  0.726477  0.800803  0.634290  0.745493  0.544757    NaN  \n",
            "V1     -0.011315  0.002043  0.039325  0.062353  0.065819  0.013154    NaN  \n",
            "V2      0.775968  0.802360  0.794592  0.657206  0.751319  0.697764    NaN  \n",
            "V3     -0.002462 -0.010829  0.003807  0.031979  0.041315 -0.019244    NaN  \n",
            "V4     -0.015676 -0.022944  0.026977 -0.026955 -0.048518 -0.014508    NaN  \n",
            "V5     -0.022187  0.003456  0.038955  0.054052  0.052705 -0.023236    NaN  \n",
            "V6     -0.051808 -0.062074 -0.049662 -0.046134 -0.028343 -0.046756    NaN  \n",
            "V7     -0.024823  0.018271  0.014472  0.058485  0.049451  0.037284    NaN  \n",
            "V8      0.694333  0.788249  0.680272  0.806272  0.713606  0.534491    NaN  \n",
            "V9      0.814188  0.826324  0.769830  0.839354  0.869906  0.536625    NaN  \n",
            "V10     0.792110  0.818881  0.775679  0.844068  0.862763  0.488200    NaN  \n",
            "V11     0.004035  0.030438  0.063832  0.045123  0.014477  0.014496    NaN  \n",
            "V12    -0.026821 -0.003860 -0.038273  0.002832  0.000450  0.001928    NaN  \n",
            "V13     0.842127  0.815737  0.839292  0.760892  0.782128  0.594527    NaN  \n",
            "V14     0.856565  0.796403  0.773289  0.731602  0.799699  0.597277    NaN  \n",
            "V15     0.841205  0.829171  0.836272  0.827196  0.851374  0.579863    NaN  \n",
            "V16    -0.037127  0.014472 -0.005585  0.016699  0.013657 -0.007839    NaN  \n",
            "V17     0.781504  0.814685  0.797618  0.778534  0.835224  0.520813    NaN  \n",
            "V18     0.775293  0.816405  0.800337  0.783052  0.839623  0.510773    NaN  \n",
            "V19     0.018737 -0.056667 -0.016289 -0.021747 -0.029589  0.006177    NaN  \n",
            "V20     0.736048  0.773883  0.793517  0.682355  0.768761  0.731000    NaN  \n",
            "V21     0.590656  0.687215  0.633362  0.725552  0.699390  0.561926    NaN  \n",
            "V22     0.759855  0.718664  0.770727  0.647986  0.701099  0.637583    NaN  \n",
            "V23    -0.013267  0.021843  0.006250 -0.041184  0.011119 -0.032602    NaN  \n",
            "V24     1.000000  0.799026  0.773780  0.722688  0.807730  0.556353    NaN  \n",
            "V25     0.799026  1.000000  0.823214  0.849439  0.859295  0.555650    NaN  \n",
            "V26     0.773780  0.823214  1.000000  0.775650  0.803450  0.642830    NaN  \n",
            "V27     0.722688  0.849439  0.775650  1.000000  0.845164  0.473136    NaN  \n",
            "V28     0.807730  0.859295  0.803450  0.845164  1.000000  0.552817    NaN  \n",
            "Amount  0.556353  0.555650  0.642830  0.473136  0.552817  1.000000    NaN  \n",
            "Class        NaN       NaN       NaN       NaN       NaN       NaN    NaN  \n",
            "\n",
            "[31 rows x 31 columns]\n",
            "170348.0    440\n",
            "41233.0     424\n",
            "7891.0      415\n",
            "146022.0    411\n",
            "406.0       408\n",
            "           ... \n",
            "25198.0       1\n",
            "143456.0      1\n",
            "54846.0       1\n",
            "41203.0       1\n",
            "42254.0       1\n",
            "Name: Time, Length: 450, dtype: int64\n",
            "1.0    4862\n",
            "Name: Class, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Time         V1         V2         V3         V4         V5  \\\n",
              "15166   26523.0 -18.474868  11.586381 -21.402917   6.038515 -14.451158   \n",
              "119714  75556.0  -0.734303   0.435519  -0.530866  -0.471120   0.643214   \n",
              "9035    12597.0  -2.589617   7.016714 -13.705407  10.343228  -2.954461   \n",
              "42007   40918.0  -3.140260   3.367342  -2.778931   3.859701  -1.159518   \n",
              "42756   41233.0 -10.645800   5.918307 -11.671043   8.807369  -7.975501   \n",
              "\n",
              "              V6         V7         V8        V9  ...       V21       V22  \\\n",
              "15166  -4.146524 -14.856124  12.431140 -4.053353  ...  1.741136 -1.251138   \n",
              "119714  0.713832  -1.234572  -2.551412 -2.057724  ... -1.004877  1.150354   \n",
              "9035   -3.055116  -9.301289   3.349573 -5.654212  ...  1.887738  0.333998   \n",
              "42007  -0.721552  -4.195342  -0.598346 -2.870145  ...  2.452339 -0.292963   \n",
              "42756  -3.586806 -13.616797   6.428169 -7.368451  ...  2.571970  0.206809   \n",
              "\n",
              "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
              "15166  -0.396219  0.095706  1.322751 -0.217955  1.628793  0.482248   99.99   \n",
              "119714 -0.152555 -1.386745  0.004716  0.219146 -0.058257  0.158048   29.95   \n",
              "9035    0.287659 -1.186406 -0.690273  0.631704  1.934221  0.789687    1.00   \n",
              "42007  -0.189330 -0.166482  0.038040 -0.015477  0.776691  0.397557    0.76   \n",
              "42756  -1.667801  0.558419 -0.027898  0.354254  0.273329 -0.152908    0.00   \n",
              "\n",
              "        Class  \n",
              "15166     1.0  \n",
              "119714    1.0  \n",
              "9035      1.0  \n",
              "42007     1.0  \n",
              "42756     1.0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8fb0f08-13b9-41d8-bf8a-1a45b8b83ad7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15166</th>\n",
              "      <td>26523.0</td>\n",
              "      <td>-18.474868</td>\n",
              "      <td>11.586381</td>\n",
              "      <td>-21.402917</td>\n",
              "      <td>6.038515</td>\n",
              "      <td>-14.451158</td>\n",
              "      <td>-4.146524</td>\n",
              "      <td>-14.856124</td>\n",
              "      <td>12.431140</td>\n",
              "      <td>-4.053353</td>\n",
              "      <td>...</td>\n",
              "      <td>1.741136</td>\n",
              "      <td>-1.251138</td>\n",
              "      <td>-0.396219</td>\n",
              "      <td>0.095706</td>\n",
              "      <td>1.322751</td>\n",
              "      <td>-0.217955</td>\n",
              "      <td>1.628793</td>\n",
              "      <td>0.482248</td>\n",
              "      <td>99.99</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119714</th>\n",
              "      <td>75556.0</td>\n",
              "      <td>-0.734303</td>\n",
              "      <td>0.435519</td>\n",
              "      <td>-0.530866</td>\n",
              "      <td>-0.471120</td>\n",
              "      <td>0.643214</td>\n",
              "      <td>0.713832</td>\n",
              "      <td>-1.234572</td>\n",
              "      <td>-2.551412</td>\n",
              "      <td>-2.057724</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.004877</td>\n",
              "      <td>1.150354</td>\n",
              "      <td>-0.152555</td>\n",
              "      <td>-1.386745</td>\n",
              "      <td>0.004716</td>\n",
              "      <td>0.219146</td>\n",
              "      <td>-0.058257</td>\n",
              "      <td>0.158048</td>\n",
              "      <td>29.95</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9035</th>\n",
              "      <td>12597.0</td>\n",
              "      <td>-2.589617</td>\n",
              "      <td>7.016714</td>\n",
              "      <td>-13.705407</td>\n",
              "      <td>10.343228</td>\n",
              "      <td>-2.954461</td>\n",
              "      <td>-3.055116</td>\n",
              "      <td>-9.301289</td>\n",
              "      <td>3.349573</td>\n",
              "      <td>-5.654212</td>\n",
              "      <td>...</td>\n",
              "      <td>1.887738</td>\n",
              "      <td>0.333998</td>\n",
              "      <td>0.287659</td>\n",
              "      <td>-1.186406</td>\n",
              "      <td>-0.690273</td>\n",
              "      <td>0.631704</td>\n",
              "      <td>1.934221</td>\n",
              "      <td>0.789687</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42007</th>\n",
              "      <td>40918.0</td>\n",
              "      <td>-3.140260</td>\n",
              "      <td>3.367342</td>\n",
              "      <td>-2.778931</td>\n",
              "      <td>3.859701</td>\n",
              "      <td>-1.159518</td>\n",
              "      <td>-0.721552</td>\n",
              "      <td>-4.195342</td>\n",
              "      <td>-0.598346</td>\n",
              "      <td>-2.870145</td>\n",
              "      <td>...</td>\n",
              "      <td>2.452339</td>\n",
              "      <td>-0.292963</td>\n",
              "      <td>-0.189330</td>\n",
              "      <td>-0.166482</td>\n",
              "      <td>0.038040</td>\n",
              "      <td>-0.015477</td>\n",
              "      <td>0.776691</td>\n",
              "      <td>0.397557</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42756</th>\n",
              "      <td>41233.0</td>\n",
              "      <td>-10.645800</td>\n",
              "      <td>5.918307</td>\n",
              "      <td>-11.671043</td>\n",
              "      <td>8.807369</td>\n",
              "      <td>-7.975501</td>\n",
              "      <td>-3.586806</td>\n",
              "      <td>-13.616797</td>\n",
              "      <td>6.428169</td>\n",
              "      <td>-7.368451</td>\n",
              "      <td>...</td>\n",
              "      <td>2.571970</td>\n",
              "      <td>0.206809</td>\n",
              "      <td>-1.667801</td>\n",
              "      <td>0.558419</td>\n",
              "      <td>-0.027898</td>\n",
              "      <td>0.354254</td>\n",
              "      <td>0.273329</td>\n",
              "      <td>-0.152908</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8fb0f08-13b9-41d8-bf8a-1a45b8b83ad7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8fb0f08-13b9-41d8-bf8a-1a45b8b83ad7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8fb0f08-13b9-41d8-bf8a-1a45b8b83ad7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import random\n",
        "import copy\n",
        "\n",
        "def generate_categorical_variables(original_data, generated_data):\n",
        "    # Extract the original categorical variables\n",
        "    original_categorical_data = original_data.select_dtypes(include=['object'])\n",
        "    \n",
        "    # Extract the generated numerical variables\n",
        "    generated_numerical_data = generated_data.select_dtypes(include=['int64', 'float64'])\n",
        "    \n",
        "    # Generate new categorical variables for the generated data using hill-climbing algorithm\n",
        "    new_categorical_data = copy.deepcopy(original_categorical_data)\n",
        "    for col in new_categorical_data.columns:\n",
        "        current_distribution = original_categorical_data[col].value_counts(normalize=True)\n",
        "        for _ in range(10):\n",
        "            # Perturb the distribution by swapping two values\n",
        "            i, j = random.sample(range(len(current_distribution)), 2)\n",
        "            new_distribution = current_distribution.copy()\n",
        "            new_distribution[i], new_distribution[j] = new_distribution[j], new_distribution[i]\n",
        "            new_distribution /= new_distribution.sum()\n",
        "\n",
        "            # Compute the correlation with the numerical variables\n",
        "            new_correlation = abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))\n",
        "            current_correlation=abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))*-1\n",
        "            # Update the categorical variable if the correlation improves\n",
        "            if new_correlation.sum() > current_correlation.sum():\n",
        "                current_correlation = new_correlation\n",
        "                new_categorical_data[col] = np.random.choice(new_distribution.index, size=len(new_categorical_data), p=new_distribution.values)\n",
        "    \n",
        "    # Replace the NaNs in the generated data with the new categorical data\n",
        "    generated_data.update(new_categorical_data)\n",
        "    \n",
        "    return generated_data\n",
        "\n",
        "\n",
        "final_df = generate_categorical_variables(df, new_df)\n",
        "# final_df.head()\n",
        "\n",
        "final_df = pd.concat([df, final_df], axis=0)\n",
        "final_df_normalized = pd.get_dummies(final_df, columns=categorical_cols)\n",
        "complete_encoded = pd.concat([final_df_normalized, regression_df], axis=0)\n",
        "print(regression_df.corr())\n",
        "print(complete_encoded.corr())\n",
        "\n",
        "# make sure that all columns are rounded appropriately\n",
        "for col in final_df.columns:\n",
        "  if col in numerical_cols:\n",
        "    # print(col)\n",
        "    # all(print(x) for x in col)\n",
        "    if all((x*1.0).is_integer() for x in df[col]):\n",
        "      final_df[col] = np.round(final_df[col], 0)\n",
        "      # print(final_df[col])\n",
        "      print(final_df[col].value_counts())\n",
        "\n",
        "# print(final_df)\n",
        "final_df.head()\n"
      ],
      "id": "Hp-B6m8qSRXx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVkqlquMVDX"
      },
      "source": [
        "Let's view both distributions to evaluate the quality of these generated values."
      ],
      "id": "LkVkqlquMVDX"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0o2tCRcLOTao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "f52900cd-ade1-4c73-a13f-d1e6743b0e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The distributions are not similar.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def compare_distributions(list1, list2):\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n",
        "\n",
        "    # Plot histograms\n",
        "    ax1.hist(list1, density=True, alpha=0.5)\n",
        "    ax2.hist(list2, density=True, alpha=0.5)\n",
        "\n",
        "    # Fit normal distributions\n",
        "    mu1, std1 = norm.fit(list1)\n",
        "    mu2, std2 = norm.fit(list2)\n",
        "\n",
        "    # Plot normal distribution curves\n",
        "    x1 = np.linspace(min(list1), max(list1), 100)\n",
        "    ax1.plot(x1, norm.pdf(x1, mu1, std1))\n",
        "    ax1.set_title('Original Dataset')\n",
        "    ax2.set_title('Augmented Dataset')\n",
        "    x2 = np.linspace(min(list2), max(list2), 100)\n",
        "    ax2.plot(x2, norm.pdf(x2, mu2, std2))\n",
        "\n",
        "    # Check if distributions are similar\n",
        "    if np.allclose(mu1, mu2) and np.allclose(std1, std2):\n",
        "        print('The distributions are similar.')\n",
        "    else:\n",
        "        print('The distributions are not similar.')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "compare_distributions(list(df[numerical_cols[2]]), list(new_df[numerical_cols[2]]))"
      ],
      "id": "0o2tCRcLOTao"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8av1-IvPy-t"
      },
      "source": [
        "It seems like the criteria for determining if the distributions are similar is too strict, especially given that we are working off a small dataset."
      ],
      "id": "_8av1-IvPy-t"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jtmfU3j2P9sA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "7baffe0d-37ce-4578-88c0-554b5f6e8389"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL DIVERGENCE:  0.007366654229956551\n",
            "The distributions are similar\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_distributions_tolerance(list1, list2, tolerance=0.07):\n",
        "    \"\"\"Compares the distributions of two lists and plots them side by side.\n",
        "\n",
        "    Args:\n",
        "        list1 (list): The first list to compare.\n",
        "        list2 (list): The second list to compare.\n",
        "        tolerance (float): The tolerance for accepting similarity between the distributions.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the distributions are similar, False otherwise.\n",
        "    \"\"\"\n",
        "    # Calculate the histograms for the two lists\n",
        "    hist1, bins1 = np.histogram(list1, density=True)\n",
        "    hist2, bins2 = np.histogram(list2, density=True)\n",
        "\n",
        "    # Normalize the histograms to have unit area\n",
        "    hist1 = hist1 / np.sum(hist1)\n",
        "    hist2 = hist2 / np.sum(hist2)\n",
        "\n",
        "    # Calculate the KL divergence between the two histograms\n",
        "    kl_div = stats.entropy(hist2, hist1)\n",
        "\n",
        "    # Plot the histograms side by side\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    ax[0].bar(bins1[:-1], hist1, width=np.diff(bins1), align='edge')\n",
        "    ax[1].bar(bins2[:-1], hist2, width=np.diff(bins2), align='edge')\n",
        "    ax[0].set_title('Original Dataset')\n",
        "    ax[1].set_title('Augmented Dataset')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Check if the KL divergence is within the tolerance\n",
        "    print(\"KL DIVERGENCE: \", kl_div)\n",
        "    similar = True if kl_div <= tolerance else False\n",
        "\n",
        "    if similar:\n",
        "      print(\"The distributions are similar\")\n",
        "    else:\n",
        "      print(\"The distrubutions are NOT similar\")\n",
        "    return similar\n",
        "\n",
        "\n",
        "compare_distributions_tolerance(list(df[numerical_cols[2]]), list(new_df[numerical_cols[2]]))\n",
        "# new_df.to_csv('augmented_dataset.csv', index=False)"
      ],
      "id": "jtmfU3j2P9sA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PUT IT ALL TOGETHER"
      ],
      "metadata": {
        "id": "W3ptvFuiWa7k"
      },
      "id": "W3ptvFuiWa7k"
    },
    {
      "cell_type": "code",
      "source": [
        "# def prepare_for_regression(df, categorical_cols, proper_noun_cols=None):\n",
        "#   regression_df = df.drop(columns=proper_noun_cols)\n",
        "#   regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "#   return regression_df\n",
        "\n",
        "# def separate_columns(df):\n",
        "#   numerical_cols = list()\n",
        "#   categorical_cols = list()\n",
        "#   proper_noun_cols = list()\n",
        "#   for (index, colname) in enumerate(df):\n",
        "#       if colname in df.select_dtypes(include='object').columns:\n",
        "#         unique_vals = df[colname].unique()\n",
        "#         if len(unique_vals) >= 0.85 * df.shape[0]:\n",
        "#           proper_noun_cols.append(colname)\n",
        "#         else:\n",
        "#           categorical_cols.append(colname)\n",
        "#       else:\n",
        "#         numerical_cols.append(colname)\n",
        "#   return numerical_cols, categorical_cols, proper_noun_cols\n",
        "\n",
        "# def run_data_augmenter(dataset, num_samples=None, percentage_augmentation=None):\n",
        "#     df = pd.read_csv(dataset, header=\"infer\")\n",
        "#     print(df.info())\n",
        "#     print(df)\n",
        "\n",
        "\n",
        "#     # DATA PREPROCESSING\n",
        "#     df = df.dropna(axis=1, how='all')\n",
        "#     # df = df.dropna()\n",
        "#     df = df.fillna(0)\n",
        "#     new_data = 0\n",
        "#     if num_samples != None:\n",
        "#       new_data = num_samples\n",
        "#     else:\n",
        "#       new_data = int(df.shape[0] * percentage_augmentation)\n",
        "    \n",
        "#     numerical_cols, categorical_cols, _ = separate_columns(df)\n",
        "#     regression_df = prepare_for_regression(df, categorical_cols)\n",
        "    \n",
        "#     np.seterr(invalid='warn') \n",
        "#     np.seterr(under='warn')\n",
        "    \n",
        "#     new_df = augment_dataset(regression_df, new_data)\n",
        "#     new_df = reverse_one_hot_encode(new_df, categorical_cols)\n",
        "\n",
        "    \n",
        "#     num_cat_generated = generate_categorical_variables(df, new_df)\n",
        "#     # final_df.head()\n",
        "\n",
        "#     final_df = pd.concat([df, num_cat_generated], axis=0)\n",
        "\n",
        "#     # make sure that all columns are rounded appropriately\n",
        "#     for col in final_df.columns:\n",
        "#       if col in numerical_cols:\n",
        "#         # print(col)\n",
        "#         # all(print(x) for x in col)\n",
        "#         if all((x*1.0).is_integer() for x in df[col]):\n",
        "#           final_df[col] = np.round(final_df[col], 0)\n",
        "#           # print(final_df[col])\n",
        "#           print(final_df[col].value_counts())\n",
        "    \n",
        "#     return final_df"
      ],
      "metadata": {
        "id": "3wxGi_QLUdV6"
      },
      "id": "3wxGi_QLUdV6",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkDfuWoz7FJ"
      },
      "source": [
        "##Downstream ML task\n",
        "\n",
        "To demonstrate the usefulness of this dataset augmentation, we're going to be predicting the diabetes diagnoses of patients."
      ],
      "id": "okkDfuWoz7FJ"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8j2Gng6G0RcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fbc98d-7219-4b4d-9b01-1991f1df2507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "2564/2564 [==============================] - 8s 2ms/step - loss: 83.4890 - accuracy: 0.9827 - precision_3: 0.0096 - recall_3: 0.0882\n",
            "Epoch 2/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 1.0751 - accuracy: 0.9974 - precision_3: 0.1715 - recall_3: 0.1335\n",
            "Epoch 3/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 1.4330 - accuracy: 0.9971 - precision_3: 0.1141 - recall_3: 0.0973\n",
            "Epoch 4/150\n",
            "2564/2564 [==============================] - 9s 4ms/step - loss: 1.1534 - accuracy: 0.9971 - precision_3: 0.1742 - recall_3: 0.1833\n",
            "Epoch 5/150\n",
            "2564/2564 [==============================] - 6s 3ms/step - loss: 1.2712 - accuracy: 0.9971 - precision_3: 0.1850 - recall_3: 0.2059\n",
            "Epoch 6/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.7361 - accuracy: 0.9973 - precision_3: 0.2438 - recall_3: 0.2647\n",
            "Epoch 7/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 1.0316 - accuracy: 0.9972 - precision_3: 0.2463 - recall_3: 0.3032\n",
            "Epoch 8/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.5220 - accuracy: 0.9977 - precision_3: 0.3594 - recall_3: 0.4163\n",
            "Epoch 9/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.6126 - accuracy: 0.9977 - precision_3: 0.3517 - recall_3: 0.4050\n",
            "Epoch 10/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.5905 - accuracy: 0.9976 - precision_3: 0.3026 - recall_3: 0.3190\n",
            "Epoch 11/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.5059 - accuracy: 0.9979 - precision_3: 0.3759 - recall_3: 0.3597\n",
            "Epoch 12/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.2058 - accuracy: 0.9984 - precision_3: 0.5562 - recall_3: 0.4480\n",
            "Epoch 13/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.2339 - accuracy: 0.9979 - precision_3: 0.2353 - recall_3: 0.0995\n",
            "Epoch 14/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0178 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 15/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0339 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 16/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0198 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 17/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0143 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 18/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 19/150\n",
            "2564/2564 [==============================] - 9s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 20/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 21/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 22/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0130 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 23/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 24/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 25/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 26/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 27/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 28/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0136 - accuracy: 0.9983 - precision_3: 0.1667 - recall_3: 0.0023\n",
            "Epoch 29/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 30/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 31/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 32/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 33/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 34/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 35/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0140 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 36/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0132 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 37/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 38/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 39/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 40/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0133 - accuracy: 0.9981 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 41/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 42/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0172 - accuracy: 0.9980 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 43/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 44/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 45/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 46/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0130 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 47/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 48/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0132 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 49/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 50/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 51/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 52/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 53/150\n",
            "2564/2564 [==============================] - 9s 3ms/step - loss: 0.0133 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 54/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 55/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0254 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 56/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 57/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 58/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 59/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 60/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 61/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 62/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 63/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0132 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 64/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 65/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0130 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 66/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 67/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 68/150\n",
            "2564/2564 [==============================] - 6s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 69/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 70/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 71/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0130 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 72/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 73/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 74/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0129 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 75/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 76/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 77/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0130 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 78/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 79/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 80/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 81/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 82/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0158 - accuracy: 0.9980 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 83/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 84/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 85/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0133 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 86/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0162 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 87/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 88/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 89/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 90/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0140 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 91/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0131 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 92/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 93/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 94/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 95/150\n",
            "2564/2564 [==============================] - 6s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 96/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0147 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 97/150\n",
            "2564/2564 [==============================] - 6s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 98/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 99/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 100/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0138 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 101/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 102/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 103/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 104/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 105/150\n",
            "2564/2564 [==============================] - 8s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 106/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 107/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0129 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 108/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0132 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 109/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 110/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 111/150\n",
            "2564/2564 [==============================] - 5s 2ms/step - loss: 0.0129 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 112/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 113/150\n",
            "2564/2564 [==============================] - 5s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 114/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0128 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 115/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 116/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0237 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 117/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 118/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0133 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 119/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0228 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 120/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0139 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 121/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 122/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 123/150\n",
            "2564/2564 [==============================] - 5s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 124/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0143 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 125/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 126/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 127/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 128/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 129/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0130 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 130/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0141 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 131/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 132/150\n",
            "2564/2564 [==============================] - 5s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 133/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0214 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 134/150\n",
            "2564/2564 [==============================] - 6s 3ms/step - loss: 0.0131 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 135/150\n",
            "2564/2564 [==============================] - 5s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 136/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 137/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 138/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 139/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 140/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 141/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0133 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 142/150\n",
            "2564/2564 [==============================] - 5s 2ms/step - loss: 0.0130 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 143/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0141 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 144/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 145/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 146/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 147/150\n",
            "2564/2564 [==============================] - 5s 2ms/step - loss: 0.0142 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 148/150\n",
            "2564/2564 [==============================] - 7s 3ms/step - loss: 0.0132 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 149/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Epoch 150/150\n",
            "2564/2564 [==============================] - 6s 2ms/step - loss: 0.0127 - accuracy: 0.9983 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "891/891 [==============================] - 2s 2ms/step - loss: 0.0562 - accuracy: 0.9982 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
            "Accuracy: 99.82%\n",
            "Precision: 0.00%\n",
            "Recall: 0.00%\n"
          ]
        }
      ],
      "source": [
        "original_train_data = train_df\n",
        "augmented_train_data = pd.concat([final_df, majority_train])\n",
        "\n",
        "# original dataset train data is stored in train data\n",
        "# original dataset test data is stored in test data\n",
        "\n",
        "# augmented dataset train data is stored in final_df\n",
        "# augmented dataset test data is stored in test data\n",
        "\n",
        "if 'id' in train_data.columns:\n",
        "  train_data = train_data.drop(columns=['id'])\n",
        "\n",
        "if 'id' in test_data.columns:\n",
        "  test_data = test_data.drop(columns=['id'])\n",
        "\n",
        "if 'id' in final_df.columns:\n",
        "  augmented_train_data = final_df.drop(columns=['id'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "experiments = []\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "train_percentages=[]\n",
        "test_percentages=[]\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras.metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "# data = pd.read_csv('path/to/dataset.csv')\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "## UNCOMMENT THIS\n",
        "# test_size = 0.2\n",
        "# train_data, test_data = train_test_split(eval_original_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "train_X, train_y = original_train_data.iloc[:, :-1], original_train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "# print(train_data)\n",
        "# # print(test_y)\n",
        "# print(train_y)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=100)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "experiments.append(\"Original dataset\")\n",
        "accuracies.append(accuracy)\n",
        "precisions.append(precision)\n",
        "recalls.append(recall)\n",
        "\n"
      ],
      "id": "8j2Gng6G0RcV"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "woXDJmn21s5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672b3a3c-f817-4b9c-ef4c-f052384844bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15166     1.0\n",
            "119714    1.0\n",
            "9035      1.0\n",
            "42007     1.0\n",
            "42756     1.0\n",
            "         ... \n",
            "273879    0.0\n",
            "108541    0.0\n",
            "134246    0.0\n",
            "212796    0.0\n",
            "163062    0.0\n",
            "Name: Class, Length: 260745, dtype: float64\n",
            "Epoch 1/150\n",
            "261/261 [==============================] - 3s 5ms/step - loss: 1870.7721 - accuracy: 0.7194 - precision_5: 0.0204 - recall_5: 0.2984\n",
            "Epoch 2/150\n",
            "261/261 [==============================] - 2s 9ms/step - loss: 5.8473 - accuracy: 0.9784 - precision_5: 0.3807 - recall_5: 0.2550\n",
            "Epoch 3/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0655 - accuracy: 0.9871 - precision_5: 0.6113 - recall_5: 0.8538\n",
            "Epoch 4/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0523 - accuracy: 0.9905 - precision_5: 0.6939 - recall_5: 0.8809\n",
            "Epoch 5/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0373 - accuracy: 0.9932 - precision_5: 0.7695 - recall_5: 0.9091\n",
            "Epoch 6/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0338 - accuracy: 0.9947 - precision_5: 0.8218 - recall_5: 0.9107\n",
            "Epoch 7/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0292 - accuracy: 0.9956 - precision_5: 0.8544 - recall_5: 0.9194\n",
            "Epoch 8/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0264 - accuracy: 0.9965 - precision_5: 0.8914 - recall_5: 0.9235\n",
            "Epoch 9/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0250 - accuracy: 0.9970 - precision_5: 0.9171 - recall_5: 0.9243\n",
            "Epoch 10/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0230 - accuracy: 0.9975 - precision_5: 0.9355 - recall_5: 0.9313\n",
            "Epoch 11/150\n",
            "261/261 [==============================] - 1s 2ms/step - loss: 6.7267 - accuracy: 0.9827 - precision_5: 0.5362 - recall_5: 0.5356\n",
            "Epoch 12/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 5.5404 - accuracy: 0.9804 - precision_5: 0.4773 - recall_5: 0.5261\n",
            "Epoch 13/150\n",
            "261/261 [==============================] - 1s 2ms/step - loss: 0.0440 - accuracy: 0.9952 - precision_5: 0.8169 - recall_5: 0.9550\n",
            "Epoch 14/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0380 - accuracy: 0.9959 - precision_5: 0.8442 - recall_5: 0.9587\n",
            "Epoch 15/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0340 - accuracy: 0.9965 - precision_5: 0.8664 - recall_5: 0.9605\n",
            "Epoch 16/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0305 - accuracy: 0.9974 - precision_5: 0.9019 - recall_5: 0.9646\n",
            "Epoch 17/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0293 - accuracy: 0.9979 - precision_5: 0.9243 - recall_5: 0.9669\n",
            "Epoch 18/150\n",
            "261/261 [==============================] - 1s 2ms/step - loss: 0.0276 - accuracy: 0.9982 - precision_5: 0.9378 - recall_5: 0.9650\n",
            "Epoch 19/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0264 - accuracy: 0.9984 - precision_5: 0.9494 - recall_5: 0.9657\n",
            "Epoch 20/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0271 - accuracy: 0.9984 - precision_5: 0.9497 - recall_5: 0.9630\n",
            "Epoch 21/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 7.5198 - accuracy: 0.9904 - precision_5: 0.7632 - recall_5: 0.7046\n",
            "Epoch 22/150\n",
            "261/261 [==============================] - 1s 5ms/step - loss: 7.3425 - accuracy: 0.9696 - precision_5: 0.1800 - recall_5: 0.1777\n",
            "Epoch 23/150\n",
            "261/261 [==============================] - 1s 5ms/step - loss: 4.6838 - accuracy: 0.9685 - precision_5: 0.1962 - recall_5: 0.2234\n",
            "Epoch 24/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.6730 - accuracy: 0.9901 - precision_5: 0.7397 - recall_5: 0.7242\n",
            "Epoch 25/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0276 - accuracy: 0.9960 - precision_5: 0.8804 - recall_5: 0.9085\n",
            "Epoch 26/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0258 - accuracy: 0.9964 - precision_5: 0.8953 - recall_5: 0.9165\n",
            "Epoch 27/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0249 - accuracy: 0.9967 - precision_5: 0.9098 - recall_5: 0.9148\n",
            "Epoch 28/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9971 - precision_5: 0.9209 - recall_5: 0.9220\n",
            "Epoch 29/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9973 - precision_5: 0.9341 - recall_5: 0.9210\n",
            "Epoch 30/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9976 - precision_5: 0.9434 - recall_5: 0.9292\n",
            "Epoch 31/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9978 - precision_5: 0.9536 - recall_5: 0.9266\n",
            "Epoch 32/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9980 - precision_5: 0.9629 - recall_5: 0.9278\n",
            "Epoch 33/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9981 - precision_5: 0.9667 - recall_5: 0.9305\n",
            "Epoch 34/150\n",
            "261/261 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9983 - precision_5: 0.9710 - recall_5: 0.9354\n",
            "Epoch 35/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 3.2762 - accuracy: 0.9915 - precision_5: 0.7872 - recall_5: 0.7433\n",
            "Epoch 36/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 7.2053 - accuracy: 0.9679 - precision_5: 0.1856 - recall_5: 0.2125\n",
            "Epoch 37/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 3.2638 - accuracy: 0.9753 - precision_5: 0.3325 - recall_5: 0.3233\n",
            "Epoch 38/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 1.8286 - accuracy: 0.9836 - precision_5: 0.5671 - recall_5: 0.5093\n",
            "Epoch 39/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0240 - accuracy: 0.9972 - precision_5: 0.8985 - recall_5: 0.9578\n",
            "Epoch 40/150\n",
            "261/261 [==============================] - 1s 5ms/step - loss: 0.0229 - accuracy: 0.9974 - precision_5: 0.9058 - recall_5: 0.9607\n",
            "Epoch 41/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0222 - accuracy: 0.9976 - precision_5: 0.9174 - recall_5: 0.9595\n",
            "Epoch 42/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0210 - accuracy: 0.9979 - precision_5: 0.9271 - recall_5: 0.9603\n",
            "Epoch 43/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9981 - precision_5: 0.9369 - recall_5: 0.9619\n",
            "Epoch 44/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9981 - precision_5: 0.9427 - recall_5: 0.9582\n",
            "Epoch 45/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9984 - precision_5: 0.9539 - recall_5: 0.9622\n",
            "Epoch 46/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0189 - accuracy: 0.9986 - precision_5: 0.9631 - recall_5: 0.9601\n",
            "Epoch 47/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0182 - accuracy: 0.9987 - precision_5: 0.9685 - recall_5: 0.9628\n",
            "Epoch 48/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9987 - precision_5: 0.9685 - recall_5: 0.9603\n",
            "Epoch 49/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9988 - precision_5: 0.9758 - recall_5: 0.9609\n",
            "Epoch 50/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 1.6774 - accuracy: 0.9937 - precision_5: 0.8063 - recall_5: 0.8688\n",
            "Epoch 51/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 5.3123 - accuracy: 0.9842 - precision_5: 0.5781 - recall_5: 0.5592\n",
            "Epoch 52/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9987 - precision_5: 0.9672 - recall_5: 0.9634\n",
            "Epoch 53/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0172 - accuracy: 0.9989 - precision_5: 0.9702 - recall_5: 0.9714\n",
            "Epoch 54/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9990 - precision_5: 0.9735 - recall_5: 0.9737\n",
            "Epoch 55/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0160 - accuracy: 0.9991 - precision_5: 0.9757 - recall_5: 0.9739\n",
            "Epoch 56/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0158 - accuracy: 0.9990 - precision_5: 0.9776 - recall_5: 0.9710\n",
            "Epoch 57/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0167 - accuracy: 0.9990 - precision_5: 0.9786 - recall_5: 0.9671\n",
            "Epoch 58/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0165 - accuracy: 0.9989 - precision_5: 0.9776 - recall_5: 0.9624\n",
            "Epoch 59/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0157 - accuracy: 0.9991 - precision_5: 0.9798 - recall_5: 0.9700\n",
            "Epoch 60/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 5.7296 - accuracy: 0.9747 - precision_5: 0.3078 - recall_5: 0.2849\n",
            "Epoch 61/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0170 - accuracy: 0.9990 - precision_5: 0.9736 - recall_5: 0.9726\n",
            "Epoch 62/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0151 - accuracy: 0.9991 - precision_5: 0.9770 - recall_5: 0.9766\n",
            "Epoch 63/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0149 - accuracy: 0.9992 - precision_5: 0.9806 - recall_5: 0.9755\n",
            "Epoch 64/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0144 - accuracy: 0.9992 - precision_5: 0.9820 - recall_5: 0.9776\n",
            "Epoch 65/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0145 - accuracy: 0.9992 - precision_5: 0.9818 - recall_5: 0.9743\n",
            "Epoch 66/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0146 - accuracy: 0.9991 - precision_5: 0.9815 - recall_5: 0.9720\n",
            "Epoch 67/150\n",
            "261/261 [==============================] - 1s 2ms/step - loss: 0.0140 - accuracy: 0.9992 - precision_5: 0.9826 - recall_5: 0.9763\n",
            "Epoch 68/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0140 - accuracy: 0.9992 - precision_5: 0.9836 - recall_5: 0.9733\n",
            "Epoch 69/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0140 - accuracy: 0.9991 - precision_5: 0.9833 - recall_5: 0.9706\n",
            "Epoch 70/150\n",
            "261/261 [==============================] - 1s 2ms/step - loss: 0.0146 - accuracy: 0.9990 - precision_5: 0.9810 - recall_5: 0.9675\n",
            "Epoch 71/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0135 - accuracy: 0.9992 - precision_5: 0.9834 - recall_5: 0.9735\n",
            "Epoch 72/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 6.2145 - accuracy: 0.9827 - precision_5: 0.5296 - recall_5: 0.6518\n",
            "Epoch 73/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 5.7888 - accuracy: 0.9779 - precision_5: 0.3582 - recall_5: 0.2334\n",
            "Epoch 74/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.3801 - accuracy: 0.9805 - precision_5: 0.4733 - recall_5: 0.4011\n",
            "Epoch 75/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0314 - accuracy: 0.9948 - precision_5: 0.9730 - recall_5: 0.7415\n",
            "Epoch 76/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.2275 - accuracy: 0.9897 - precision_5: 0.7707 - recall_5: 0.6345\n",
            "Epoch 77/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 2.6273 - accuracy: 0.9788 - precision_5: 0.4221 - recall_5: 0.3715\n",
            "Epoch 78/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0181 - accuracy: 0.9973 - precision_5: 0.9885 - recall_5: 0.8663\n",
            "Epoch 79/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0212 - accuracy: 0.9973 - precision_5: 0.9871 - recall_5: 0.8651\n",
            "Epoch 80/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0159 - accuracy: 0.9976 - precision_5: 0.9883 - recall_5: 0.8824\n",
            "Epoch 81/150\n",
            "261/261 [==============================] - 1s 2ms/step - loss: 0.0154 - accuracy: 0.9977 - precision_5: 0.9881 - recall_5: 0.8863\n",
            "Epoch 82/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0499 - accuracy: 0.9969 - precision_5: 0.9714 - recall_5: 0.8597\n",
            "Epoch 83/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9972 - precision_5: 0.9607 - recall_5: 0.8850\n",
            "Epoch 84/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.2957 - accuracy: 0.9923 - precision_5: 0.8228 - recall_5: 0.7452\n",
            "Epoch 85/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 2.8231 - accuracy: 0.9822 - precision_5: 0.5225 - recall_5: 0.5037\n",
            "Epoch 86/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0122 - accuracy: 0.9985 - precision_5: 0.9874 - recall_5: 0.9319\n",
            "Epoch 87/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0115 - accuracy: 0.9984 - precision_5: 0.9854 - recall_5: 0.9301\n",
            "Epoch 88/150\n",
            "261/261 [==============================] - 1s 2ms/step - loss: 0.0110 - accuracy: 0.9985 - precision_5: 0.9848 - recall_5: 0.9313\n",
            "Epoch 89/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0111 - accuracy: 0.9983 - precision_5: 0.9843 - recall_5: 0.9255\n",
            "Epoch 90/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0106 - accuracy: 0.9984 - precision_5: 0.9831 - recall_5: 0.9313\n",
            "Epoch 91/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0101 - accuracy: 0.9986 - precision_5: 0.9851 - recall_5: 0.9369\n",
            "Epoch 92/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0108 - accuracy: 0.9984 - precision_5: 0.9839 - recall_5: 0.9311\n",
            "Epoch 93/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0102 - accuracy: 0.9983 - precision_5: 0.9834 - recall_5: 0.9253\n",
            "Epoch 94/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0087 - accuracy: 0.9987 - precision_5: 0.9841 - recall_5: 0.9449\n",
            "Epoch 95/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0102 - accuracy: 0.9983 - precision_5: 0.9832 - recall_5: 0.9264\n",
            "Epoch 96/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 2.3574 - accuracy: 0.9774 - precision_5: 0.3760 - recall_5: 0.3246\n",
            "Epoch 97/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0149 - accuracy: 0.9985 - precision_5: 0.9740 - recall_5: 0.9463\n",
            "Epoch 98/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0115 - accuracy: 0.9988 - precision_5: 0.9838 - recall_5: 0.9506\n",
            "Epoch 99/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0103 - accuracy: 0.9988 - precision_5: 0.9846 - recall_5: 0.9488\n",
            "Epoch 100/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0091 - accuracy: 0.9988 - precision_5: 0.9859 - recall_5: 0.9473\n",
            "Epoch 101/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0083 - accuracy: 0.9987 - precision_5: 0.9873 - recall_5: 0.9436\n",
            "Epoch 102/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0079 - accuracy: 0.9988 - precision_5: 0.9884 - recall_5: 0.9447\n",
            "Epoch 103/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0077 - accuracy: 0.9987 - precision_5: 0.9886 - recall_5: 0.9424\n",
            "Epoch 104/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0077 - accuracy: 0.9987 - precision_5: 0.9888 - recall_5: 0.9420\n",
            "Epoch 105/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0080 - accuracy: 0.9986 - precision_5: 0.9883 - recall_5: 0.9381\n",
            "Epoch 106/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0075 - accuracy: 0.9987 - precision_5: 0.9886 - recall_5: 0.9428\n",
            "Epoch 107/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0070 - accuracy: 0.9989 - precision_5: 0.9889 - recall_5: 0.9496\n",
            "Epoch 108/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0072 - accuracy: 0.9988 - precision_5: 0.9884 - recall_5: 0.9455\n",
            "Epoch 109/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0069 - accuracy: 0.9989 - precision_5: 0.9878 - recall_5: 0.9504\n",
            "Epoch 110/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0068 - accuracy: 0.9989 - precision_5: 0.9880 - recall_5: 0.9519\n",
            "Epoch 111/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0065 - accuracy: 0.9989 - precision_5: 0.9876 - recall_5: 0.9539\n",
            "Epoch 112/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0065 - accuracy: 0.9990 - precision_5: 0.9877 - recall_5: 0.9560\n",
            "Epoch 113/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7396 - accuracy: 0.9899 - precision_5: 0.7394 - recall_5: 0.7106\n",
            "Epoch 114/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0062 - accuracy: 0.9991 - precision_5: 0.9861 - recall_5: 0.9640\n",
            "Epoch 115/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0060 - accuracy: 0.9991 - precision_5: 0.9882 - recall_5: 0.9640\n",
            "Epoch 116/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0062 - accuracy: 0.9991 - precision_5: 0.9863 - recall_5: 0.9630\n",
            "Epoch 117/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0060 - accuracy: 0.9991 - precision_5: 0.9869 - recall_5: 0.9628\n",
            "Epoch 118/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0058 - accuracy: 0.9991 - precision_5: 0.9882 - recall_5: 0.9646\n",
            "Epoch 119/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0058 - accuracy: 0.9991 - precision_5: 0.9878 - recall_5: 0.9648\n",
            "Epoch 120/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0058 - accuracy: 0.9991 - precision_5: 0.9874 - recall_5: 0.9646\n",
            "Epoch 121/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0056 - accuracy: 0.9991 - precision_5: 0.9874 - recall_5: 0.9634\n",
            "Epoch 122/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0057 - accuracy: 0.9991 - precision_5: 0.9878 - recall_5: 0.9652\n",
            "Epoch 123/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0057 - accuracy: 0.9991 - precision_5: 0.9869 - recall_5: 0.9626\n",
            "Epoch 124/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0058 - accuracy: 0.9991 - precision_5: 0.9869 - recall_5: 0.9636\n",
            "Epoch 125/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9991 - precision_5: 0.9888 - recall_5: 0.9634\n",
            "Epoch 126/150\n",
            "261/261 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9987 - precision_5: 0.9861 - recall_5: 0.9459\n",
            "Epoch 127/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9991 - precision_5: 0.9874 - recall_5: 0.9659\n",
            "Epoch 128/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9992 - precision_5: 0.9872 - recall_5: 0.9679\n",
            "Epoch 129/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9992 - precision_5: 0.9876 - recall_5: 0.9702\n",
            "Epoch 130/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9991 - precision_5: 0.9878 - recall_5: 0.9624\n",
            "Epoch 131/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0065 - accuracy: 0.9989 - precision_5: 0.9864 - recall_5: 0.9529\n",
            "Epoch 132/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9991 - precision_5: 0.9868 - recall_5: 0.9663\n",
            "Epoch 133/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0055 - accuracy: 0.9990 - precision_5: 0.9869 - recall_5: 0.9617\n",
            "Epoch 134/150\n",
            "261/261 [==============================] - 3s 12ms/step - loss: 0.0056 - accuracy: 0.9990 - precision_5: 0.9867 - recall_5: 0.9605\n",
            "Epoch 135/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.0053 - accuracy: 0.9991 - precision_5: 0.9870 - recall_5: 0.9661\n",
            "Epoch 136/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0076 - accuracy: 0.9990 - precision_5: 0.9858 - recall_5: 0.9589\n",
            "Epoch 137/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9991 - precision_5: 0.9869 - recall_5: 0.9630\n",
            "Epoch 138/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.1360 - accuracy: 0.9916 - precision_5: 0.8662 - recall_5: 0.6469\n",
            "Epoch 139/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0474 - accuracy: 0.9915 - precision_5: 0.9821 - recall_5: 0.5518\n",
            "Epoch 140/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0063 - accuracy: 0.9993 - precision_5: 0.9863 - recall_5: 0.9755\n",
            "Epoch 141/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0060 - accuracy: 0.9993 - precision_5: 0.9871 - recall_5: 0.9757\n",
            "Epoch 142/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0058 - accuracy: 0.9993 - precision_5: 0.9863 - recall_5: 0.9755\n",
            "Epoch 143/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0057 - accuracy: 0.9993 - precision_5: 0.9855 - recall_5: 0.9766\n",
            "Epoch 144/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9993 - precision_5: 0.9857 - recall_5: 0.9772\n",
            "Epoch 145/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9993 - precision_5: 0.9869 - recall_5: 0.9768\n",
            "Epoch 146/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0057 - accuracy: 0.9993 - precision_5: 0.9846 - recall_5: 0.9753\n",
            "Epoch 147/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9993 - precision_5: 0.9857 - recall_5: 0.9759\n",
            "Epoch 148/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0056 - accuracy: 0.9993 - precision_5: 0.9846 - recall_5: 0.9759\n",
            "Epoch 149/150\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9993 - precision_5: 0.9861 - recall_5: 0.9761\n",
            "Epoch 150/150\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.2258 - accuracy: 0.9837 - precision_5: 0.6414 - recall_5: 0.2818\n",
            "891/891 [==============================] - 3s 2ms/step - loss: 0.0368 - accuracy: 0.9983 - precision_5: 1.0000 - recall_5: 0.0600\n",
            "Accuracy: 99.83%\n",
            "Precision: 100.00%\n",
            "Recall: 6.00%\n"
          ]
        }
      ],
      "source": [
        "# test_size = 5/9.0\n",
        "# train_data, test_data = train_test_split(eval_augmented_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = augmented_train_data.iloc[:, 1:], augmented_train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "# last column\n",
        "train_X, train_y = augmented_train_data.iloc[:, :-1], augmented_train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_y)\n",
        "# print(test_data)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=1000)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "\n",
        "\n",
        "experiments.append(\"Augmented dataset\")\n",
        "accuracies.append(accuracy)\n",
        "precisions.append(precision)\n",
        "recalls.append(recall)\n"
      ],
      "id": "woXDJmn21s5Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experimental Results Summary"
      ],
      "metadata": {
        "id": "SYWi6swLnJY5"
      },
      "id": "SYWi6swLnJY5"
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"Accuracy\": accuracies,\n",
        "    \"Precision\": precisions,\n",
        "    \"Recall\": recalls\n",
        "}\n",
        "\n",
        "# Create a Pandas DataFrame from the dictionary\n",
        "results = pd.DataFrame(data, index=experiments)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(results)"
      ],
      "metadata": {
        "id": "3H15UjTM1h-C"
      },
      "id": "3H15UjTM1h-C",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}