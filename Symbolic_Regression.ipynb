{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fd33643"
      },
      "source": [
        "# Program Synthesis for Dataset Augmentation with Symbolic Regression and Genetic Programming"
      ],
      "id": "0fd33643"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "2649232b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea29e2e8-6833-4dbf-88ed-dd67d36147d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gplearn in /usr/local/lib/python3.9/dist-packages (0.4.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Ruchi Bhalani, rb44675\n",
        "!pip install gplearn"
      ],
      "id": "2649232b"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8ZMR0CqzTxX",
        "outputId": "e3954b76-56de-4daa-df8e-e582dd76dbd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "A8ZMR0CqzTxX"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "ab1fe226"
      },
      "outputs": [],
      "source": [
        "# headers\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from collections import OrderedDict"
      ],
      "id": "ab1fe226"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "cc9c4481"
      },
      "outputs": [],
      "source": [
        "# function to calculate adjusted r2\n",
        "def get_adj_r2(r2, n, p):\n",
        "    return (1-(1-r2)*((n-1)/(n-p-1)))"
      ],
      "id": "cc9c4481"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7c7806"
      },
      "source": [
        "# Assignment 2: Regression and KNN classifier"
      ],
      "id": "7a7c7806"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bb47879"
      },
      "source": [
        "**Data Prep**\n"
      ],
      "id": "8bb47879"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "589c05de",
        "outputId": "3f403244-30b8-4203-8823-4e5c935cfbb6",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    int64  \n",
            " 2   BloodPressure             768 non-null    int64  \n",
            " 3   SkinThickness             768 non-null    int64  \n",
            " 4   Insulin                   768 non-null    int64  \n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n",
            "None\n",
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "763           10      101             76             48      180  32.9   \n",
            "764            2      122             70             27        0  36.8   \n",
            "765            5      121             72             23      112  26.2   \n",
            "766            1      126             60              0        0  30.1   \n",
            "767            1       93             70             31        0  30.4   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "763                     0.171   63        0  \n",
            "764                     0.340   27        0  \n",
            "765                     0.245   30        0  \n",
            "766                     0.349   47        1  \n",
            "767                     0.315   23        0  \n",
            "\n",
            "[768 rows x 9 columns]\n",
            "[]\n",
            "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
            "FINAL STEP: proper nouns -- []\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a41225e-b0ac-45c6-b521-4d623b587dac\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a41225e-b0ac-45c6-b521-4d623b587dac')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a41225e-b0ac-45c6-b521-4d623b587dac button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a41225e-b0ac-45c6-b521-4d623b587dac');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "# NOTE: THESE VALUES ARE MEANT TO BE CUSTOMIZABLE BY THE USER\n",
        "# read in data\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/medical-charges.txt\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/breast_cancer.csv\"\n",
        "USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/diabetes.csv\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/disease.csv\"\n",
        "EXPECTED_DISTINCT_PERCENTAGE = 0.85\n",
        "\n",
        "# How many entries do we want to augment this dataset by?\n",
        "# TODO: Let the user pass this in as a percentage as well\n",
        "NEW_EXAMPLES = 10\n",
        "\n",
        "# data = pd.read_csv(USER_INPUT_DATASET, header = 0)\n",
        "df = pd.read_csv(USER_INPUT_DATASET, header=\"infer\")\n",
        "print(df.info())\n",
        "print(df)\n",
        "\n",
        "\n",
        "# DATA PREPROCESSING\n",
        "df = df.dropna(axis=1, how='all')\n",
        "# df = df.dropna()\n",
        "df = df.fillna(0)\n",
        "\n",
        "\n",
        "# Separate them into numerical, categorical, and proper noun columns (regex generation)\n",
        "numerical_cols = list()\n",
        "categorical_cols = list()\n",
        "proper_noun_cols = list()\n",
        "for (index, colname) in enumerate(df):\n",
        "    if colname in df.select_dtypes(include='object').columns:\n",
        "      unique_vals = df[colname].unique()\n",
        "      if len(unique_vals) >= EXPECTED_DISTINCT_PERCENTAGE * df.shape[0]:\n",
        "        proper_noun_cols.append(colname)\n",
        "      else:\n",
        "        categorical_cols.append(colname)\n",
        "    else:\n",
        "      numerical_cols.append(colname)\n",
        "\n",
        "\n",
        "print(categorical_cols)\n",
        "print(numerical_cols)\n",
        "print(\"FINAL STEP: proper nouns --\", proper_noun_cols)\n",
        "\n",
        "df.head()\n"
      ],
      "id": "589c05de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3043bb91"
      },
      "source": [
        "There are several categorical columns. We need to transform these to be able to do regression. "
      ],
      "id": "3043bb91"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0484464d",
        "outputId": "8bce00b1-9a7f-468c-e0f7-9f30b6b1a764",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5dabf21c-4e44-40a2-82b4-d1ef7de297f8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dabf21c-4e44-40a2-82b4-d1ef7de297f8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5dabf21c-4e44-40a2-82b4-d1ef7de297f8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5dabf21c-4e44-40a2-82b4-d1ef7de297f8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "# we'll deal with the proper noun columns at the end\n",
        "regression_df = df.drop(columns=proper_noun_cols)\n",
        "regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "regression_df.head()"
      ],
      "id": "0484464d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38a13dc1"
      },
      "source": [
        "An interesting thing to check with regression problems is whether any of the individual features correlate very strongly with the label."
      ],
      "id": "38a13dc1"
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad660fd",
        "outputId": "32db5d02-427f-4e16-8dc8-bd58f1fe4c10",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.129459       0.141282      -0.081672   \n",
            "Glucose                      0.129459  1.000000       0.152590       0.057328   \n",
            "BloodPressure                0.141282  0.152590       1.000000       0.207371   \n",
            "SkinThickness               -0.081672  0.057328       0.207371       1.000000   \n",
            "Insulin                     -0.073535  0.331357       0.088933       0.436783   \n",
            "BMI                          0.017683  0.221071       0.281805       0.392573   \n",
            "DiabetesPedigreeFunction    -0.033523  0.137337       0.041265       0.183928   \n",
            "Age                          0.544341  0.263514       0.239528      -0.113970   \n",
            "Outcome                      0.221898  0.466581       0.065068       0.074752   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.073535  0.017683                 -0.033523   \n",
            "Glucose                   0.331357  0.221071                  0.137337   \n",
            "BloodPressure             0.088933  0.281805                  0.041265   \n",
            "SkinThickness             0.436783  0.392573                  0.183928   \n",
            "Insulin                   1.000000  0.197859                  0.185071   \n",
            "BMI                       0.197859  1.000000                  0.140647   \n",
            "DiabetesPedigreeFunction  0.185071  0.140647                  1.000000   \n",
            "Age                      -0.042163  0.036242                  0.033561   \n",
            "Outcome                   0.130548  0.292695                  0.173844   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.544341  0.221898  \n",
            "Glucose                   0.263514  0.466581  \n",
            "BloodPressure             0.239528  0.065068  \n",
            "SkinThickness            -0.113970  0.074752  \n",
            "Insulin                  -0.042163  0.130548  \n",
            "BMI                       0.036242  0.292695  \n",
            "DiabetesPedigreeFunction  0.033561  0.173844  \n",
            "Age                       1.000000  0.238356  \n",
            "Outcome                   0.238356  1.000000   \n",
            "\n",
            "\n",
            "=== SIGNIFICANTLY CORRELATED COLUMNS ===\n",
            "[]\n",
            "OrderedDict()\n"
          ]
        }
      ],
      "source": [
        "print(regression_df.corr(), \"\\n\\n\")\n",
        "\n",
        "# Indexing with numbers on a numpy matrix will probably be faster\n",
        "\n",
        "print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "rows, cols = regression_df.shape\n",
        "corr = regression_df.corr().values\n",
        "fields = list(regression_df.columns)\n",
        "correlated_columns = list()\n",
        "nodes = []\n",
        "correlations = OrderedDict()\n",
        "\n",
        "# data structure that organizes correlations\n",
        "correlations_per_column = list()\n",
        "for i in range(cols):\n",
        "    for j in range(i+1, cols):\n",
        "      corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "      if corr[i,j] > 0.6:\n",
        "          print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "          correlated_columns.append([fields[i], fields[j], corr[i, j]])\n",
        "          correlations[corr[i, j]] = [fields[i], fields[j]]\n",
        "          if fields[i] not in nodes:\n",
        "            nodes.append(fields[i])\n",
        "          if fields[j] not in nodes:\n",
        "            nodes.append(fields[j])\n",
        "\n",
        "print(nodes)\n",
        "print(correlations)"
      ],
      "id": "6ad660fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l74DGBikGfow"
      },
      "source": [
        "We can visualize these attributes and their correlations as a weighted graph."
      ],
      "id": "l74DGBikGfow"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ggXpUuAfWqG2",
        "outputId": "3825319c-4c70-43e5-f857-2a38750fcc4e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "for key,value in correlations.items():\n",
        "  G.add_edge(value[0], value[1], weight=key)\n",
        "\n",
        "\n",
        "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0.5]\n",
        "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.5]\n",
        "\n",
        "pos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n",
        "\n",
        "# nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=700)\n",
        "\n",
        "# edges\n",
        "nx.draw_networkx_edges(G, pos, edgelist=elarge, width=6)\n",
        "nx.draw_networkx_edges(\n",
        "    G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=\"b\", style=\"dashed\"\n",
        ")\n",
        "\n",
        "# node labels\n",
        "nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
        "# edge weight labels\n",
        "edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.margins(0.15)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ggXpUuAfWqG2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBaK36IxauAO"
      },
      "source": [
        "Now, let's organize them by pair and start generating values."
      ],
      "id": "lBaK36IxauAO"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkLcoIVuar-l",
        "outputId": "9034c757-50e3-4c2c-b967-9e7b57b23427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict()\n"
          ]
        }
      ],
      "source": [
        "# weight all the correlations so that they're iterated through correctly\n",
        "weighted_correlations = OrderedDict()\n",
        "for key,value in correlations.items():\n",
        "  mult_factor = 1\n",
        "  if value[0] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  if value[1] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  weighted_correlations[mult_factor * key] = value\n",
        "\n",
        "weighted_correlations = OrderedDict(sorted(weighted_correlations.items(), reverse=True))\n",
        "print(weighted_correlations)\n"
      ],
      "id": "FkLcoIVuar-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numerical Data Generation: Symbolic Regression\n",
        "Now, let's augment the dataset with numerical data.\n",
        "\n",
        "The code is implementing a technique for data augmentation using symbolic regression, kernel density estimation, and K-nearest neighbor regression. The goal is to generate new samples of data that are similar to the original dataset, but not identical, to increase the size and diversity of the dataset for training machine learning models. The code takes in a pandas DataFrame df and the number of new samples to generate n_samples as inputs.\n",
        "\n",
        "The code first scales each column of the input dataset to the range [0, 1], fits a symbolic regressor to the scaled data, and generates new values using the regressor. It then applies kernel density estimation and K-nearest neighbor regression to the original data and generates new values using these methods. Finally, it performs a grid search to find the best kernel and bandwidth for the kernel density estimator, and generates new values using the best estimator.\n",
        "\n",
        "The new values generated by each method are clipped to the range of the original column data, and the new values for all columns are combined to create a new DataFrame new_df. The new_df is then concatenated with the original DataFrame df to create an augmented dataset, which is returned as output.\n",
        "\n"
      ],
      "metadata": {
        "id": "dJCaBCe3meJ0"
      },
      "id": "dJCaBCe3meJ0"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YqEGUD4oDrP",
        "outputId": "99517e93-a13e-480c-d39b-cb92b916dac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}\n",
            "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}\n",
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0    48.81          10071.7        7       0.00305286       0.00301773     14.53m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-212.16250948          -inf          -inf -133.46744026          -inf\n",
            " -281.12881055          -inf          -inf -234.94439958          -inf\n",
            " -335.32241593          -inf          -inf -287.66128371          -inf\n",
            " -365.75219178          -inf          -inf -317.71711134          -inf\n",
            " -375.91694359          -inf          -inf -336.11539337          -inf\n",
            " -378.37136908          -inf          -inf -348.22868624          -inf\n",
            " -379.38704625          -inf          -inf -356.76618794          -inf\n",
            " -380.37205545          -inf          -inf -363.15711045          -inf\n",
            " -381.48333254          -inf          -inf -368.19488025          -inf\n",
            " -382.69898589          -inf          -inf -372.34241894          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          335.372        7       0.00819325         0.008165     18.89m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-2205.95343925           -inf           -inf  -559.25572027\n",
            "           -inf -1052.41953892           -inf           -inf\n",
            "  -629.35077437           -inf  -880.01955654           -inf\n",
            "           -inf  -670.25407298           -inf  -829.48189416\n",
            "           -inf           -inf  -693.18518004           -inf\n",
            "  -800.65028245           -inf           -inf  -706.60941256\n",
            "           -inf  -781.69988178           -inf           -inf\n",
            "  -714.96223286           -inf  -769.74763495           -inf\n",
            "           -inf  -720.45759718           -inf  -762.09498762\n",
            "           -inf           -inf  -724.24488325           -inf\n",
            "  -756.97375804           -inf           -inf  -726.95636458\n",
            "           -inf  -753.39007971           -inf           -inf\n",
            "  -728.95984443           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          282.023        7       0.00763926       0.00761355      8.34m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-2445.03428884           -inf           -inf  -358.32805942\n",
            "           -inf  -946.47998163           -inf           -inf\n",
            "  -420.32378069           -inf  -711.26873186           -inf\n",
            "           -inf  -465.88752342           -inf  -649.1577841\n",
            "           -inf           -inf  -498.9893832            -inf\n",
            "  -631.66409828           -inf           -inf  -523.42788794\n",
            "           -inf  -629.32370997           -inf           -inf\n",
            "  -541.62281829           -inf  -631.0017599            -inf\n",
            "           -inf  -555.3095333            -inf  -631.98932878\n",
            "           -inf           -inf  -565.74609072           -inf\n",
            "  -631.3097818            -inf           -inf  -573.8286537\n",
            "           -inf  -629.57564525           -inf           -inf\n",
            "  -580.18945499           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          987.505        7       0.00278235       0.00292524      8.20m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-13495.19767892            -inf            -inf   -344.50510743\n",
            "            -inf  -3694.16418482            -inf            -inf\n",
            "   -402.35377832            -inf  -1920.79316416            -inf\n",
            "            -inf   -442.07866858            -inf  -1313.77990581\n",
            "            -inf            -inf   -467.64354722            -inf\n",
            "  -1033.1128268             -inf            -inf   -485.03174867\n",
            "            -inf   -880.98162067            -inf            -inf\n",
            "   -497.67865917            -inf   -790.76151053            -inf\n",
            "            -inf   -507.41697048            -inf   -733.6970289\n",
            "            -inf            -inf   -515.25722072            -inf\n",
            "   -695.77605486            -inf            -inf   -521.78785443\n",
            "            -inf   -669.61889804            -inf            -inf\n",
            "   -527.37133899            -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          6071.69        7       0.00125741       0.00139989      6.84m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-309589.17899775             -inf             -inf   -1566.17194355\n",
            "             -inf  -77720.40537206             -inf             -inf\n",
            "   -1016.10823309             -inf  -34824.02519881             -inf\n",
            "             -inf    -857.45327759             -inf  -19830.41047071\n",
            "             -inf             -inf    -788.7057134              -inf\n",
            "  -12900.74227397             -inf             -inf    -753.19806481\n",
            "             -inf   -9143.29642094             -inf             -inf\n",
            "    -733.11756746             -inf   -6882.66247141             -inf\n",
            "             -inf    -721.2209064              -inf   -5419.04564976\n",
            "             -inf             -inf    -714.06064143             -inf\n",
            "   -4418.30252909             -inf             -inf    -709.81166052\n",
            "             -inf   -3704.59787663             -inf             -inf\n",
            "    -707.43120698             -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          520.195        7       0.00641139       0.00657749      8.33m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-1262.16758172           -inf           -inf  -531.60986257\n",
            "           -inf  -701.68271431           -inf           -inf\n",
            "  -524.6201116            -inf  -598.08157738           -inf\n",
            "           -inf  -522.08982663           -inf  -562.26954159\n",
            "           -inf           -inf  -520.80582564           -inf\n",
            "  -545.83285564           -inf           -inf  -520.07313566\n",
            "           -inf  -536.96645631           -inf           -inf\n",
            "  -519.6355193            -inf  -531.70618285           -inf\n",
            "           -inf  -519.37394094           -inf  -528.39056976\n",
            "           -inf           -inf  -519.22595499           -inf\n",
            "  -526.2051062            -inf           -inf  -519.15644156\n",
            "           -inf  -524.71041988           -inf           -inf\n",
            "  -519.14467698           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81            33425        7       0.00224604       0.00245896      8.30m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ -10.97266936          -inf          -inf  -14.47760995          -inf\n",
            "  -28.45302491  -15.9240256   -10.01976593  -33.2008054    -8.55711879\n",
            "  -45.99796192  -29.39019944  -18.86317953  -51.81594996  -15.64128288\n",
            "  -63.69587154  -41.63165588  -27.9746282   -69.66101286  -23.31159305\n",
            "  -81.1537541   -52.94731945  -36.556336    -86.40319724  -30.69022234\n",
            "  -97.94722307  -65.19719414  -45.00726511 -101.9322133   -37.92936599\n",
            " -113.86677799  -78.505134    -53.76679594 -116.28702527  -45.33331512\n",
            " -128.84257618  -92.03010983  -62.82540399 -129.56786715  -52.93043257\n",
            " -142.87546522 -105.13576948  -72.06519569 -141.88902599  -60.66184121\n",
            " -156.00702594 -117.41326858  -81.21217257 -153.3596588   -68.34686653]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          29984.2        7       0.00275687       0.00269347      7.62m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-1612.51579683           -inf           -inf  -329.81974483\n",
            "           -inf  -758.98201829           -inf           -inf\n",
            "  -418.1532406            -inf  -642.26084751           -inf\n",
            "           -inf  -465.94522643           -inf  -612.16400627\n",
            "           -inf           -inf  -492.95834461           -inf\n",
            "  -593.48701589           -inf           -inf  -509.11341421\n",
            "           -inf  -579.95901377           -inf           -inf\n",
            "  -519.42832445           -inf  -571.17871268           -inf\n",
            "           -inf  -526.43809699           -inf  -565.62962012\n",
            "           -inf           -inf  -531.47316777           -inf\n",
            "  -562.06768498           -inf           -inf  -535.26798929\n",
            "           -inf  -559.74470965           -inf           -inf\n",
            "  -538.24897689           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          60.5031        7                0                0      7.46m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KernelDensity, KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from gplearn.genetic import SymbolicRegressor\n",
        "\n",
        "def augment_dataset(df, n_samples):\n",
        "    # Get column names and data types\n",
        "    col_names = df.columns.tolist()\n",
        "    dtypes = df.dtypes.tolist()\n",
        "\n",
        "    # Create a list to store new data samples\n",
        "    new_data = []\n",
        "\n",
        "    # Loop through each column and generate new values\n",
        "    for col_name, dtype in zip(col_names, dtypes):\n",
        "        # Get the data from the column\n",
        "        col_data = df[col_name].values\n",
        "\n",
        "        # Scale the column data to the range [0, 1]\n",
        "        scaler = MinMaxScaler()\n",
        "        col_data_scaled = scaler.fit_transform(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Fit a symbolic regressor to the column data\n",
        "        est_gp = SymbolicRegressor(population_size=5000, tournament_size=50,\n",
        "                                    generations=50, stopping_criteria=0.01,\n",
        "                                    p_crossover=0.7, p_subtree_mutation=0.1,\n",
        "                                    p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
        "                                    max_samples=0.9, verbose=1,\n",
        "                                    parsimony_coefficient=0.001, random_state=0, warm_start=True)\n",
        "        est_gp.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the symbolic regressor\n",
        "        new_col_data = est_gp.predict(np.random.rand(n_samples, 1))\n",
        "\n",
        "        # Reshape the new column data to be a 2D array\n",
        "        new_col_data = new_col_data.reshape(-1, 1)\n",
        "\n",
        "        # Scale the new column data using the scaler fitted to the original column data\n",
        "        new_col_data_scaled = scaler.transform(new_col_data)\n",
        "\n",
        "        # Inverse transform the scaled new column data to get the original scale\n",
        "        new_col_data = scaler.inverse_transform(new_col_data_scaled)\n",
        "\n",
        "        # Fit a kernel density estimator to the original column data\n",
        "        kde = KernelDensity(kernel='gaussian', bandwidth=0.1)\n",
        "        kde.fit(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Generate new values using the kernel density estimator\n",
        "        kde_samples = kde.sample(n_samples)\n",
        "        kde_samples = np.squeeze(kde_samples)\n",
        "\n",
        "        # Fit a KNN regressor to the original column data\n",
        "        knn = KNeighborsRegressor(n_neighbors=5)\n",
        "        knn.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the KNN regressor\n",
        "        knn_samples = knn.predict(np.random.rand(n_samples, 1))\n",
        "        knn_samples = np.squeeze(knn_samples)\n",
        "\n",
        "        # Use a grid search to find the best kernel and bandwidth for the kernel density estimator\n",
        "        params = {'kernel': ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear'],\n",
        "                  'bandwidth': np.linspace(0.1, 1.0, 10)}\n",
        "        grid = GridSearchCV(KernelDensity(), params, cv=5)\n",
        "        print(\"DATA SHAPE HERE: \", print(col_data.reshape(-1, 1).shape))\n",
        "        grid.fit(col_data.reshape(-1, 1))\n",
        "        # grid.fit(col_data)\n",
        "        kde_best = grid.best_estimator_\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        if kde_best.kernel in [\"gaussian\", \"tophat\"]:\n",
        "            kde_best_samples = kde_best.sample(n_samples)\n",
        "        else:\n",
        "            # Evaluate log density for a grid of points\n",
        "            x_grid = np.linspace(col_data.min(), col_data.max(), 1000).reshape(-1, 1)\n",
        "            log_dens = kde_best.score_samples(x_grid)\n",
        "\n",
        "            # Sample from the grid according to the log densities\n",
        "            probs = np.exp(log_dens - log_dens.max())\n",
        "            probs /= probs.sum()\n",
        "            kde_best_samples = np.random.choice(x_grid.flatten(), size=n_samples, p=probs)\n",
        "            \n",
        "        kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        # kde_best_samples = kde_best.sample(n_samples)\n",
        "        # kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Clip the new values to the range of the original column data\n",
        "        kde_best_samples = np.clip(kde_best_samples, np.min(col_data), np.max(col_data))\n",
        "\n",
        "        # Add new values to the list of new data\n",
        "        new_data.append(kde_best_samples.flatten())\n",
        "\n",
        "    # Transpose the list of new data to match the shape of the original dataset\n",
        "    new_data = np.array(new_data).T\n",
        "\n",
        "    # Convert the new data to a dataframe and append it to the original dataset\n",
        "    new_df = pd.DataFrame(new_data, columns=col_names)\n",
        "    augmented_df = pd.concat([df, new_df], ignore_index=True)\n",
        "\n",
        "    # return augmented_df\n",
        "    return new_df\n",
        "\n",
        "print(np.seterr())\n",
        "np.seterr(invalid='warn')\n",
        "np.seterr(under='warn')\n",
        "print(np.seterr())\n",
        "new_df = augment_dataset(regression_df, df.shape[0])"
      ],
      "id": "4YqEGUD4oDrP"
    },
    {
      "cell_type": "code",
      "source": [
        "# def reverse_one_hot_encode(df):\n",
        "#     # Get a list of all columns with '_'\n",
        "#     oh_cols = [col for col in df.columns if '_' in col]\n",
        "\n",
        "#     # Get the original column names and values for each one-hot encoded column\n",
        "#     col_vals = {}\n",
        "#     for col in oh_cols:\n",
        "#         col_name = col.split('_')[0]\n",
        "#         col_val = col.split('_')[1]\n",
        "#         if col_name not in col_vals:\n",
        "#             col_vals[col_name] = {}\n",
        "#         col_vals[col_name][col_val] = col\n",
        "\n",
        "#     # Create a new dataframe to store the reverse one-hot encoded values\n",
        "#     new_df = pd.DataFrame(columns=col_vals.keys())\n",
        "\n",
        "#     # Iterate through each row of the original dataframe\n",
        "#     for index, row in df.iterrows():\n",
        "#         # Initialize a dictionary to store the values for this row\n",
        "#         new_row = {}\n",
        "\n",
        "#         # Iterate through each original column\n",
        "#         for col_name in df.columns:\n",
        "#             # If this is a one-hot encoded column, find the closest value to 1 and use that\n",
        "#             if col_name in oh_cols:\n",
        "#                 col_val = col_name.split('_')[1]\n",
        "#                 col_key = col_vals[col_name.split('_')[0]][col_val]\n",
        "                \n",
        "#                 # Get the numeric values for the one-hot encoded column\n",
        "#                 numeric_values = pd.to_numeric(row[col_key], errors='coerce')\n",
        "#                 # Set values < 0.5 to 0 and values >= 0.5 to the numeric value\n",
        "#                 numeric_values = np.where(numeric_values >= 0.5, numeric_values, 0)\n",
        "#                 # Set the value closest to 1 to 1, and the rest to 0\n",
        "#                 numeric_values = np.where(numeric_values == np.max(numeric_values), 1, 0)\n",
        "#                 # Combine the column name and the value (if it's 1) and add it to the new_row dictionary\n",
        "#                 new_row[col_name.split('_')[0]] = col_val if np.sum(numeric_values) > 0 else np.nan\n",
        "#             # If this is not a one-hot encoded column, use the original value\n",
        "#             else:\n",
        "#                 new_row[col_name] = row[col_name]\n",
        "\n",
        "#         # Append the values for this row to the new dataframe\n",
        "#         new_df = new_df.append(new_row, ignore_index=True)\n",
        "\n",
        "#     # Return the new dataframe\n",
        "#     return new_df"
      ],
      "metadata": {
        "id": "er5lGyrRnbUt"
      },
      "id": "er5lGyrRnbUt",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "x0qeIOSFMs8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c27704-fe77-45b4-8467-7fc28c7d71f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Old:  768\n",
            "New:  768\n",
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  0.000000 -0.824551      -0.635139      -0.857652   \n",
            "Glucose                     -0.824551  0.000000      -0.721007      -0.859591   \n",
            "BloodPressure               -0.635139 -0.721007       0.000000      -0.591221   \n",
            "SkinThickness               -0.857652 -0.859591      -0.591221       0.000000   \n",
            "Insulin                     -0.882690 -0.550096      -0.584159      -0.409961   \n",
            "BMI                         -0.913420 -0.768766      -0.612830      -0.529938   \n",
            "DiabetesPedigreeFunction    -0.941543 -0.787318      -0.712160      -0.708398   \n",
            "Age                         -0.450684 -0.679051      -0.503688      -0.800700   \n",
            "Outcome                     -0.652560 -0.356666      -0.555795      -0.727136   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.882690 -0.913420                 -0.941543   \n",
            "Glucose                  -0.550096 -0.768766                 -0.787318   \n",
            "BloodPressure            -0.584159 -0.612830                 -0.712160   \n",
            "SkinThickness            -0.409961 -0.529938                 -0.708398   \n",
            "Insulin                   0.000000 -0.666502                 -0.802731   \n",
            "BMI                      -0.666502  0.000000                 -0.775112   \n",
            "DiabetesPedigreeFunction -0.802731 -0.775112                  0.000000   \n",
            "Age                      -0.926053 -0.877163                 -0.946705   \n",
            "Outcome                  -0.663926 -0.481190                 -0.635608   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies              -0.450684 -0.652560  \n",
            "Glucose                  -0.679051 -0.356666  \n",
            "BloodPressure            -0.503688 -0.555795  \n",
            "SkinThickness            -0.800700 -0.727136  \n",
            "Insulin                  -0.926053 -0.663926  \n",
            "BMI                      -0.877163 -0.481190  \n",
            "DiabetesPedigreeFunction -0.946705 -0.635608  \n",
            "Age                       0.000000 -0.636698  \n",
            "Outcome                  -0.636698  0.000000  \n",
            "     Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
            "0       0.119119   81.074074      48.116116       0.000000    0.000000   \n",
            "1       4.067067  123.901902      74.006006      27.945946   88.072072   \n",
            "2       1.973974  102.985986      65.945946      16.648649    0.846847   \n",
            "3      10.941942  184.060060      92.080080      45.189189  326.882883   \n",
            "4       1.038038   96.014014      61.915916       0.099099    0.000000   \n",
            "..           ...         ...            ...            ...         ...   \n",
            "763     9.138138  170.912913      88.172172      41.126126  248.972973   \n",
            "764     2.229229  107.965966      68.632633      20.018018    4.234234   \n",
            "765     9.002002  166.132132      87.927928      40.927928  229.495495   \n",
            "766     9.036036  167.128128      88.050050      41.027027  232.036036   \n",
            "767     4.730731  125.893894      74.982983      29.036036   94.846847   \n",
            "\n",
            "           BMI  DiabetesPedigreeFunction        Age   Outcome  \n",
            "0    22.030831                  0.136609  21.720721  0.004004  \n",
            "1    33.650751                  0.474194  32.951952  0.070070  \n",
            "2    28.881882                  0.293680  25.804805  0.030030  \n",
            "3    45.270671                  1.182186  59.138138  0.993994  \n",
            "4    26.329530                  0.230382  23.882883  0.017017  \n",
            "..         ...                       ...        ...       ...  \n",
            "763  42.516817                  0.952440  52.951952  0.986987  \n",
            "764  30.359560                  0.338222  27.126126  0.039039  \n",
            "765  41.777978                  0.907898  51.270270  0.984985  \n",
            "766  41.979479                  0.919620  51.930931  0.985986  \n",
            "767  34.120921                  0.497638  34.033033  0.077077  \n",
            "\n",
            "[768 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# reprocess the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# just brings columns back to normal and replaces values with NaN\n",
        "def reverse_one_hot_encode(df):\n",
        "    oh_cols = [col for col in df.columns if \"_\" in col and col.split(\"_\")[0] in categorical_cols]\n",
        "    print(oh_cols)\n",
        "    new_df = df.drop(columns=oh_cols).copy()\n",
        "    for col in oh_cols:\n",
        "        col_name, val = col.split(\"_\")\n",
        "        mask = df[col] == 1\n",
        "        new_df[col_name] = np.nan\n",
        "        new_df.loc[mask, col_name] = val\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_significantly_correlated_colmns(df):\n",
        "  print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "  rows, cols = df.shape\n",
        "  corr = df.corr().values\n",
        "  fields = list(df.columns)\n",
        "  for i in range(cols):\n",
        "      for j in range(i+1, cols-1):\n",
        "        corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "        if corr[i,j] > 0.75:\n",
        "            print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "\n",
        "new_df = reverse_one_hot_encode(new_df)\n",
        "corr1 = regression_df.corr()\n",
        "corr2 = new_df.corr()\n",
        "# print a matrix to see the difference in correlations\n",
        "diff = np.abs(corr1) - np.abs(corr2)\n",
        "\n",
        "print(\"Old: \", len(regression_df))\n",
        "print(\"New: \", len(new_df))\n",
        "print(diff)\n",
        "\n",
        "# print(print_significantly_correlated_colmns(regression_df))\n",
        "# print(print_significantly_correlated_colmns(new_df))\n",
        "new_df.head()\n",
        "print(new_df)"
      ],
      "id": "x0qeIOSFMs8x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIh2Sn5ohWnz"
      },
      "source": [
        "## Categorical Variable Generation: Hill Climbing Algorithm\n",
        "In this implementation, we use a hill-climbing algorithm to search for the best categorical variable distribution that matches the original distribution and is correlated with the numerical variables in the generated data. The algorithm starts with the original distribution and iteratively perturbs it by swapping two values, then computes the correlation with the numerical variables and updates the distribution if the correlation improves. The algorithm repeats this process for a fixed number of iterations (in this case, 10), then moves on to the next categorical variable.\n",
        "\n",
        "This is an example of an explicit search or optimization technique for program synthesis, as we are searching for the best program (i.e., categorical variable distribution) that satisfies certain constraints (i.e., matching the original distribution and being correlated with the numerical variables)."
      ],
      "id": "cIh2Sn5ohWnz"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "Hp-B6m8qSRXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9b996d9-2469-4533-872e-f9a708d72134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.129459       0.141282      -0.081672   \n",
            "Glucose                      0.129459  1.000000       0.152590       0.057328   \n",
            "BloodPressure                0.141282  0.152590       1.000000       0.207371   \n",
            "SkinThickness               -0.081672  0.057328       0.207371       1.000000   \n",
            "Insulin                     -0.073535  0.331357       0.088933       0.436783   \n",
            "BMI                          0.017683  0.221071       0.281805       0.392573   \n",
            "DiabetesPedigreeFunction    -0.033523  0.137337       0.041265       0.183928   \n",
            "Age                          0.544341  0.263514       0.239528      -0.113970   \n",
            "Outcome                      0.221898  0.466581       0.065068       0.074752   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.073535  0.017683                 -0.033523   \n",
            "Glucose                   0.331357  0.221071                  0.137337   \n",
            "BloodPressure             0.088933  0.281805                  0.041265   \n",
            "SkinThickness             0.436783  0.392573                  0.183928   \n",
            "Insulin                   1.000000  0.197859                  0.185071   \n",
            "BMI                       0.197859  1.000000                  0.140647   \n",
            "DiabetesPedigreeFunction  0.185071  0.140647                  1.000000   \n",
            "Age                      -0.042163  0.036242                  0.033561   \n",
            "Outcome                   0.130548  0.292695                  0.173844   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.544341  0.221898  \n",
            "Glucose                   0.263514  0.466581  \n",
            "BloodPressure             0.239528  0.065068  \n",
            "SkinThickness            -0.113970  0.074752  \n",
            "Insulin                  -0.042163  0.130548  \n",
            "BMI                       0.036242  0.292695  \n",
            "DiabetesPedigreeFunction  0.033561  0.173844  \n",
            "Age                       1.000000  0.238356  \n",
            "Outcome                   0.238356  1.000000  \n",
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.407731       0.354059       0.248327   \n",
            "Glucose                      0.407731  1.000000       0.403984       0.344517   \n",
            "BloodPressure                0.354059  0.403984       1.000000       0.403289   \n",
            "SkinThickness                0.248327  0.344517       0.403289       1.000000   \n",
            "Insulin                      0.271020  0.520677       0.289113       0.572506   \n",
            "BMI                          0.316300  0.482088       0.488181       0.563837   \n",
            "DiabetesPedigreeFunction     0.300471  0.407428       0.283742       0.416063   \n",
            "Age                          0.692784  0.495683       0.410407       0.221713   \n",
            "Outcome                      0.427271  0.582300       0.245082       0.301332   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies               0.271020  0.316300                  0.300471   \n",
            "Glucose                   0.520677  0.482088                  0.407428   \n",
            "BloodPressure             0.289113  0.488181                  0.283742   \n",
            "SkinThickness             0.572506  0.563837                  0.416063   \n",
            "Insulin                   1.000000  0.420642                  0.457183   \n",
            "BMI                       0.420642  1.000000                  0.397927   \n",
            "DiabetesPedigreeFunction  0.457183  0.397927                  1.000000   \n",
            "Age                       0.298877  0.326380                  0.350375   \n",
            "Outcome                   0.344417  0.444561                  0.376873   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.692784  0.427271  \n",
            "Glucose                   0.495683  0.582300  \n",
            "BloodPressure             0.410407  0.245082  \n",
            "SkinThickness             0.221713  0.301332  \n",
            "Insulin                   0.298877  0.344417  \n",
            "BMI                       0.326380  0.444561  \n",
            "DiabetesPedigreeFunction  0.350375  0.376873  \n",
            "Age                       1.000000  0.440934  \n",
            "Outcome                   0.440934  1.000000  \n",
            "1.0     282\n",
            "2.0     220\n",
            "0.0     178\n",
            "3.0     159\n",
            "4.0     134\n",
            "5.0     119\n",
            "6.0     104\n",
            "8.0      85\n",
            "7.0      75\n",
            "9.0      62\n",
            "10.0     49\n",
            "11.0     24\n",
            "13.0     22\n",
            "12.0     16\n",
            "15.0      3\n",
            "14.0      3\n",
            "17.0      1\n",
            "Name: Pregnancies, dtype: int64\n",
            "100.0    40\n",
            "99.0     37\n",
            "106.0    30\n",
            "95.0     30\n",
            "112.0    29\n",
            "         ..\n",
            "190.0     2\n",
            "178.0     1\n",
            "160.0     1\n",
            "67.0      1\n",
            "169.0     1\n",
            "Name: Glucose, Length: 136, dtype: int64\n",
            "70.0     118\n",
            "74.0     103\n",
            "78.0      91\n",
            "64.0      91\n",
            "72.0      89\n",
            "68.0      88\n",
            "80.0      81\n",
            "60.0      75\n",
            "0.0       74\n",
            "62.0      73\n",
            "76.0      73\n",
            "66.0      60\n",
            "88.0      52\n",
            "58.0      46\n",
            "82.0      46\n",
            "86.0      46\n",
            "84.0      45\n",
            "90.0      40\n",
            "50.0      25\n",
            "92.0      22\n",
            "54.0      22\n",
            "52.0      22\n",
            "56.0      19\n",
            "85.0      15\n",
            "75.0      14\n",
            "94.0      12\n",
            "48.0      11\n",
            "65.0      11\n",
            "96.0       8\n",
            "44.0       7\n",
            "110.0      5\n",
            "106.0      5\n",
            "100.0      5\n",
            "104.0      5\n",
            "98.0       5\n",
            "108.0      5\n",
            "30.0       4\n",
            "46.0       3\n",
            "102.0      3\n",
            "114.0      3\n",
            "40.0       2\n",
            "55.0       2\n",
            "24.0       2\n",
            "38.0       2\n",
            "79.0       2\n",
            "122.0      1\n",
            "95.0       1\n",
            "61.0       1\n",
            "69.0       1\n",
            "Name: BloodPressure, dtype: int64\n",
            "0.0     426\n",
            "32.0     64\n",
            "23.0     52\n",
            "30.0     48\n",
            "31.0     47\n",
            "27.0     44\n",
            "33.0     43\n",
            "18.0     41\n",
            "39.0     40\n",
            "28.0     39\n",
            "40.0     36\n",
            "25.0     35\n",
            "19.0     34\n",
            "29.0     34\n",
            "41.0     33\n",
            "37.0     32\n",
            "22.0     32\n",
            "26.0     31\n",
            "20.0     28\n",
            "17.0     27\n",
            "24.0     26\n",
            "13.0     25\n",
            "35.0     24\n",
            "15.0     24\n",
            "36.0     23\n",
            "21.0     22\n",
            "42.0     19\n",
            "45.0     18\n",
            "46.0     17\n",
            "16.0     17\n",
            "12.0     17\n",
            "34.0     16\n",
            "38.0     13\n",
            "14.0     13\n",
            "44.0     11\n",
            "43.0     10\n",
            "10.0     10\n",
            "11.0     10\n",
            "50.0      8\n",
            "7.0       8\n",
            "47.0      7\n",
            "48.0      7\n",
            "54.0      6\n",
            "49.0      5\n",
            "52.0      3\n",
            "63.0      3\n",
            "60.0      2\n",
            "51.0      2\n",
            "8.0       2\n",
            "56.0      1\n",
            "99.0      1\n",
            "Name: SkinThickness, dtype: int64\n",
            "0.0      564\n",
            "1.0       79\n",
            "2.0       33\n",
            "3.0       18\n",
            "105.0     15\n",
            "        ... \n",
            "510.0      1\n",
            "270.0      1\n",
            "178.0      1\n",
            "392.0      1\n",
            "232.0      1\n",
            "Name: Insulin, Length: 277, dtype: int64\n",
            "22.0    140\n",
            "21.0    115\n",
            "25.0    102\n",
            "24.0     98\n",
            "23.0     77\n",
            "28.0     66\n",
            "27.0     66\n",
            "26.0     65\n",
            "29.0     63\n",
            "30.0     47\n",
            "37.0     46\n",
            "41.0     45\n",
            "31.0     43\n",
            "32.0     34\n",
            "33.0     34\n",
            "38.0     32\n",
            "45.0     31\n",
            "42.0     29\n",
            "36.0     28\n",
            "46.0     27\n",
            "40.0     27\n",
            "34.0     24\n",
            "39.0     24\n",
            "43.0     22\n",
            "51.0     20\n",
            "35.0     20\n",
            "58.0     18\n",
            "50.0     17\n",
            "47.0     16\n",
            "52.0     15\n",
            "44.0     13\n",
            "53.0     11\n",
            "49.0     10\n",
            "54.0     10\n",
            "66.0      9\n",
            "62.0      9\n",
            "48.0      9\n",
            "57.0      9\n",
            "60.0      8\n",
            "55.0      7\n",
            "59.0      7\n",
            "63.0      7\n",
            "56.0      6\n",
            "61.0      5\n",
            "65.0      5\n",
            "67.0      5\n",
            "68.0      4\n",
            "69.0      3\n",
            "72.0      3\n",
            "64.0      2\n",
            "70.0      2\n",
            "81.0      1\n",
            "Name: Age, dtype: int64\n",
            "0.0    1002\n",
            "1.0     534\n",
            "Name: Outcome, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0          6.0    148.0           72.0           35.0      0.0  33.6   \n",
              "1          1.0     85.0           66.0           29.0      0.0  26.6   \n",
              "2          8.0    183.0           64.0            0.0      0.0  23.3   \n",
              "3          1.0     89.0           66.0           23.0     94.0  28.1   \n",
              "4          0.0    137.0           40.0           35.0    168.0  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction   Age  Outcome  \n",
              "0                     0.627  50.0      1.0  \n",
              "1                     0.351  31.0      0.0  \n",
              "2                     0.672  32.0      1.0  \n",
              "3                     0.167  21.0      0.0  \n",
              "4                     2.288  33.0      1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-600c53df-c8fa-4162-baf4-1988bec4af67\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-600c53df-c8fa-4162-baf4-1988bec4af67')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-600c53df-c8fa-4162-baf4-1988bec4af67 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-600c53df-c8fa-4162-baf4-1988bec4af67');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "import random\n",
        "import copy\n",
        "\n",
        "def generate_categorical_variables(original_data, generated_data):\n",
        "    # Extract the original categorical variables\n",
        "    original_categorical_data = original_data.select_dtypes(include=['object'])\n",
        "    \n",
        "    # Extract the generated numerical variables\n",
        "    generated_numerical_data = generated_data.select_dtypes(include=['int64', 'float64'])\n",
        "    \n",
        "    # Generate new categorical variables for the generated data using hill-climbing algorithm\n",
        "    new_categorical_data = copy.deepcopy(original_categorical_data)\n",
        "    for col in new_categorical_data.columns:\n",
        "        current_distribution = original_categorical_data[col].value_counts(normalize=True)\n",
        "        for _ in range(10):\n",
        "            # Perturb the distribution by swapping two values\n",
        "            i, j = random.sample(range(len(current_distribution)), 2)\n",
        "            new_distribution = current_distribution.copy()\n",
        "            new_distribution[i], new_distribution[j] = new_distribution[j], new_distribution[i]\n",
        "            new_distribution /= new_distribution.sum()\n",
        "\n",
        "            # Compute the correlation with the numerical variables\n",
        "            new_correlation = abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))\n",
        "            current_correlation=abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))*-1\n",
        "            # Update the categorical variable if the correlation improves\n",
        "            if new_correlation.sum() > current_correlation.sum():\n",
        "                current_correlation = new_correlation\n",
        "                new_categorical_data[col] = np.random.choice(new_distribution.index, size=len(new_categorical_data), p=new_distribution.values)\n",
        "    \n",
        "    # Replace the NaNs in the generated data with the new categorical data\n",
        "    generated_data.update(new_categorical_data)\n",
        "    \n",
        "    return generated_data\n",
        "\n",
        "\n",
        "final_df = generate_categorical_variables(df, new_df)\n",
        "# final_df.head()\n",
        "\n",
        "final_df = pd.concat([df, final_df], axis=0)\n",
        "final_df_normalized = pd.get_dummies(final_df, columns=categorical_cols)\n",
        "complete_encoded = pd.concat([final_df_normalized, regression_df], axis=0)\n",
        "print(regression_df.corr())\n",
        "print(complete_encoded.corr())\n",
        "\n",
        "# make sure that all columns are rounded appropriately\n",
        "for col in final_df.columns:\n",
        "  if col in numerical_cols:\n",
        "    # print(col)\n",
        "    # all(print(x) for x in col)\n",
        "    if all((x*1.0).is_integer() for x in df[col]):\n",
        "      final_df[col] = np.round(final_df[col], 0)\n",
        "      # print(final_df[col])\n",
        "      print(final_df[col].value_counts())\n",
        "\n",
        "# print(final_df)\n",
        "final_df.head()\n"
      ],
      "id": "Hp-B6m8qSRXx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVkqlquMVDX"
      },
      "source": [
        "Let's view both distributions to evaluate the quality of these generated values."
      ],
      "id": "LkVkqlquMVDX"
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "0o2tCRcLOTao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "1c572493-db61-4ae2-be4d-d0909a210fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The distributions are not similar.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def compare_distributions(list1, list2):\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n",
        "\n",
        "    # Plot histograms\n",
        "    ax1.hist(list1, density=True, alpha=0.5)\n",
        "    ax2.hist(list2, density=True, alpha=0.5)\n",
        "\n",
        "    # Fit normal distributions\n",
        "    mu1, std1 = norm.fit(list1)\n",
        "    mu2, std2 = norm.fit(list2)\n",
        "\n",
        "    # Plot normal distribution curves\n",
        "    x1 = np.linspace(min(list1), max(list1), 100)\n",
        "    ax1.plot(x1, norm.pdf(x1, mu1, std1))\n",
        "    ax1.set_title('Original Dataset')\n",
        "    ax2.set_title('Augmented Dataset')\n",
        "    x2 = np.linspace(min(list2), max(list2), 100)\n",
        "    ax2.plot(x2, norm.pdf(x2, mu2, std2))\n",
        "\n",
        "    # Check if distributions are similar\n",
        "    if np.allclose(mu1, mu2) and np.allclose(std1, std2):\n",
        "        print('The distributions are similar.')\n",
        "    else:\n",
        "        print('The distributions are not similar.')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "compare_distributions(list(df[numerical_cols[2]]), list(new_df[numerical_cols[2]]))"
      ],
      "id": "0o2tCRcLOTao"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8av1-IvPy-t"
      },
      "source": [
        "It seems like the criteria for determining if the distributions are similar is too strict, especially given that we are working off a small dataset."
      ],
      "id": "_8av1-IvPy-t"
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "jtmfU3j2P9sA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "8162a134-7ad3-48df-f24d-771e84a6f94b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1AAAAF2CAYAAABgcXkzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABRQElEQVR4nO3dfVhVVf7//xdgHFAEMZQb70C00BGlIMnyrjyJfSxvUkPHFJnS0iwdKo1KzKwwM4fJMZlsNDNLszGnrxlmJJVFWppZmaameRd4F6CoYLB+f/Tj5BGwAyIH8Pm4rn3lWXvttd/rZHv1PnvttV2MMUYAAAAAgD/l6uwAAAAAAKC2IIECAAAAAAeRQAEAAACAg0igAAAAAMBBJFAAAAAA4CASKAAAAABwEAkUAAAAADiIBAoAAAAAHEQCBQAAAAAOIoFCnfLkk0/KxcWlUse++uqrcnFx0d69e6s2qHPs3btXLi4uevXVVy/ZOQAAqE7VMX4CNQkJFGqE77//XnfddZeaNWsmi8WioKAgDR8+XN9//72zQ3OKjIwMubi42DaLxSJ/f3/17NlTzz77rI4cOVLptrdt26Ynn3yyxgx0b7zxhlJSUpwdBoA66qWXXpKLi4uio6OdHYpTnTp1Sk8++aQyMjKcFkPJj5wlW/369dWyZUvdfvvtWrhwoQoKCird9urVq/Xkk09WXbAX6dlnn9XKlSudHQYuERIoON2KFSt07bXXKj09XfHx8XrppZd09913a926dbr22mv1zjvvONzWE088odOnT1cqjhEjRuj06dNq1apVpY6/FB588EEtXrxYL7/8sh555BE1btxYU6dOVbt27fTRRx9Vqs1t27Zp2rRpJFAALgtLlixRcHCwNm7cqF27djk7HKc5deqUpk2b5tQEqsS8efO0ePFizZkzR/fcc4+OHz+uv/3tb+rcubP2799fqTZXr16tadOmVXGklUcCVbfVc3YAuLzt3r1bI0aMUOvWrfXJJ5+oSZMmtn0TJkxQt27dNGLECG3dulWtW7cut538/Hw1aNBA9erVU716lftr7ebmJjc3t0ode6l069ZNgwcPtiv75ptv1Lt3bw0aNEjbtm1TYGCgk6IDgJptz549+vzzz7VixQrde++9WrJkiaZOnerssC57gwcPlp+fn+1zUlKSlixZopEjR2rIkCH64osvnBgd8Oe4AwWnev7553Xq1Cm9/PLLdsmTJPn5+enf//638vPzNXPmTFt5yRSAbdu26a9//at8fX3VtWtXu33nOn36tB588EH5+fmpYcOG6tevnw4ePCgXFxe72/1lzeEODg7WbbfdpvXr16tz587y8PBQ69at9dprr9md4/jx43r44YcVHh4uLy8veXt769Zbb9U333xTRd/UHzp16qSUlBTl5OToX//6l638559/1rhx43T11VfL09NTV155pYYMGWLXn1dffVVDhgyRJN100022aRQlv0j+73//U9++fRUUFCSLxaLQ0FBNnz5dRUVFdjHs3LlTgwYNUkBAgDw8PNS8eXMNHTpUubm5dvVef/11RUZGytPTU40bN9bQoUPtfl3s2bOn3nvvPf3888+2WIKDg6v2CwNw2VqyZIl8fX3Vt29fDR48WEuWLClVp2TK9Pl3Zsp7ZnX58uVq3769PDw81KFDB73zzjsaNWqU3bWr5NhZs2Zp7ty5at26terXr6/evXtr//79MsZo+vTpat68uTw9PdW/f38dP368VGzvv/++unXrpgYNGqhhw4bq27dvqanto0aNkpeXlw4ePKgBAwbIy8tLTZo00cMPP2y7du/du9c2xk6bNs12vT13DNy+fbsGDx6sxo0by8PDQ1FRUXr33XdLxfT999/r5ptvlqenp5o3b66nn35axcXFF/rX4JDhw4frnnvu0YYNG7R27Vpb+aeffqohQ4aoZcuWslgsatGihf7+97/bzTYZNWqU5s6dK0l2UwRLzJo1SzfccIOuvPJKeXp6KjIyUm+//XapGNauXauuXbuqUaNG8vLy0tVXX63HHnvMrk5BQYGmTp2qNm3a2OKZNGmS3fRDFxcX5efna9GiRbZYRo0addHfEWoO7kDBqf7f//t/Cg4OVrdu3crc3717dwUHB+u9994rtW/IkCFq27atnn32WRljyj3HqFGj9NZbb2nEiBG6/vrr9fHHH6tv374Ox7hr1y4NHjxYd999t+Li4rRgwQKNGjVKkZGR+stf/iJJ+umnn7Ry5UoNGTJEISEhys7O1r///W/16NFD27ZtU1BQkMPnc0RJPB988IGeeeYZSdKXX36pzz//XEOHDlXz5s21d+9ezZs3Tz179tS2bdtUv359de/eXQ8++KBefPFFPfbYY2rXrp0k2f756quvysvLSwkJCfLy8tJHH32kpKQk5eXl6fnnn5ckFRYWKiYmRgUFBXrggQcUEBCggwcPatWqVcrJyZGPj48k6ZlnntGUKVN055136p577tGRI0c0Z84cde/eXV9//bUaNWqkxx9/XLm5uTpw4ID+8Y9/SJK8vLyq9LsCcPlasmSJ7rjjDrm7u2vYsGGaN2+evvzyS1133XWVau+9995TbGyswsPDlZycrF9//VV33323mjVrVu75CwsL9cADD+j48eOaOXOm7rzzTt18883KyMjQ5MmTtWvXLs2ZM0cPP/ywFixYYDt28eLFiouLU0xMjJ577jmdOnVK8+bNU9euXfX111/bJWxFRUWKiYlRdHS0Zs2apQ8//FAvvPCCQkNDNXbsWDVp0kTz5s3T2LFjNXDgQN1xxx2SpI4dO0r6PSm68cYb1axZMz366KNq0KCB3nrrLQ0YMED//e9/NXDgQElSVlaWbrrpJv3222+2ei+//LI8PT0r9X2eb8SIEXr55Zf1wQcf6JZbbpH0e8J66tQpjR07VldeeaU2btyoOXPm6MCBA1q+fLkk6d5779WhQ4e0du1aLV68uFS7//znP9WvXz8NHz5chYWFWrp0qYYMGaJVq1bZ/n/g+++/12233aaOHTvqqaeeksVi0a5du/TZZ5/Z2ikuLla/fv20fv16jRkzRu3atdO3336rf/zjH/rxxx9tU/YWL16se+65R507d9aYMWMkSaGhoVXyHaGGMICT5OTkGEmmf//+F6zXr18/I8nk5eUZY4yZOnWqkWSGDRtWqm7JvhKbNm0ykszEiRPt6o0aNcpIMlOnTrWVLVy40Egye/bssZW1atXKSDKffPKJrezw4cPGYrGYhx56yFZ25swZU1RUZHeOPXv2GIvFYp566im7Mklm4cKFF+zzunXrjCSzfPnycut06tTJ+Pr62j6fOnWqVJ3MzEwjybz22mu2suXLlxtJZt26daXql9XGvffea+rXr2/OnDljjDHm66+//tPY9u7da9zc3MwzzzxjV/7tt9+aevXq2ZX37dvXtGrVqty2AKAyvvrqKyPJrF271hhjTHFxsWnevLmZMGGCXb2S6+3518Syrtfh4eGmefPm5sSJE7ayjIwMI8nuOlZybJMmTUxOTo6tPDEx0UgynTp1MmfPnrWVDxs2zLi7u9uusydOnDCNGjUyo0ePtospKyvL+Pj42JXHxcUZSXZjjTHGXHPNNSYyMtL2+ciRI6XGvRK9evUy4eHhtvOXfF833HCDadu2ra1s4sSJRpLZsGGDrezw4cPGx8en1PhZlpIx+siRI2Xu//XXX40kM3DgQFtZWeNScnKycXFxMT///LOt7P777zfl/W/t+W0UFhaaDh06mJtvvtlW9o9//OOCsRljzOLFi42rq6v59NNP7cpTU1ONJPPZZ5/Zyho0aGDi4uLKbQu1G1P44DQnTpyQJDVs2PCC9Ur25+Xl2ZXfd999f3qOtLQ0SdK4cePsyh944AGH42zfvr3dHbImTZro6quv1k8//WQrs1gscnX9/T+noqIiHTt2zHb7f/PmzQ6fqyK8vLxs36Eku18Az549q2PHjqlNmzZq1KiRwzGc28aJEyd09OhRdevWTadOndL27dslyXaHac2aNTp16lSZ7axYsULFxcW68847dfToUdsWEBCgtm3bat26dRXuLwBUxJIlS+Tv76+bbrpJ0u/TqmJjY7V06dJS05IdcejQIX377bcaOXKk3Z3yHj16KDw8vMxjhgwZYrtmSrKtBHjXXXfZPa8bHR2twsJCHTx4UNLvU8lycnI0bNgwu2uom5uboqOjy7yGnj8mduvWzW6cKs/x48f10Ucf6c4777Rd948ePapjx44pJiZGO3futMW1evVqXX/99ercubPt+CZNmmj48OF/eh5HlHyv5Y1t+fn5Onr0qG644QYZY/T111871O65bfz666/Kzc1Vt27d7MbGRo0aSfp9Knt5UxKXL1+udu3aKSwszO7fy8033yxJjG2XERIoOE1JYnTuhbIs5SVaISEhf3qOn3/+Wa6urqXqtmnTxuE4W7ZsWarM19dXv/76q+1zcXGx/vGPf6ht27ayWCzy8/NTkyZNtHXr1lLPBVWVkydP2n0np0+fVlJSklq0aGEXQ05OjsMxfP/99xo4cKB8fHzk7e2tJk2a6K677pIkWxshISFKSEjQK6+8Ij8/P8XExGju3Ll259i5c6eMMWrbtq2aNGlit/3www86fPhwFX4TAGCvqKhIS5cu1U033aQ9e/Zo165d2rVrl6Kjo5Wdna309PQKt/nzzz9LKnv8KG9MOX/8KEmmWrRoUWZ5ybiyc+dOSdLNN99c6hr6wQcflLqGenh4lHqO+Pxxqjy7du2SMUZTpkwpda6SBTdKzvfzzz+rbdu2pdq4+uqr//Q8jjh58qQk+/F+3759GjVqlBo3bmx7vqtHjx6S5PDYtmrVKl1//fXy8PBQ48aNbVMazz0+NjZWN954o+655x75+/tr6NCheuutt+ySqZ07d+r7778v9T1dddVVksTYdhnhGSg4jY+PjwIDA7V169YL1tu6dauaNWsmb29vu/KqmnP9Z8pbmc+c89zVs88+qylTpuhvf/ubpk+frsaNG8vV1VUTJ06skodrz3f27Fn9+OOP6tChg63sgQce0MKFCzVx4kR16dJFPj4+cnFx0dChQx2KIScnRz169JC3t7eeeuophYaGysPDQ5s3b9bkyZPt2njhhRc0atQo/e9//9MHH3ygBx98UMnJyfriiy/UvHlzFRcXy8XFRe+//36Z3x/POQG4lD766CP98ssvWrp0qZYuXVpq/5IlS9S7d29JKvfl65W5S3W+8saPPxtXSq63ixcvVkBAQKl65682ezEryJac6+GHH1ZMTEyZdSryo+PF+O677+zOV1RUpFtuuUXHjx/X5MmTFRYWpgYNGujgwYMaNWqUQ2Pbp59+qn79+ql79+566aWXFBgYqCuuuEILFy7UG2+8Yavn6empTz75ROvWrdN7772ntLQ0LVu2TDfffLM++OADubm5qbi4WOHh4Zo9e3aZ5zo/MUbdRQIFp7rttts0f/58rV+/3raS3rk+/fRT7d27V/fee2+l2m/VqpWKi4u1Z88eu1/NqvpdIG+//bZuuukm/ec//7Erz8nJsVuqtSrPd/r0abvB7u2331ZcXJxeeOEFW9mZM2eUk5Njd2x5/7OQkZGhY8eOacWKFerevbutfM+ePWXWDw8PV3h4uJ544gl9/vnnuvHGG5Wamqqnn35aoaGhMsYoJCTE9stcecqLBwAqa8mSJWratKltZbZzrVixQu+8845SU1Pl6ekpX19fSSp1rSy541Si5B2BZY0fVT2mlCw40LRpU1mt1ipps7xrbckrQq644oo/PVerVq1sd8fOtWPHjosPULItAFEytn377bf68ccftWjRIo0cOdJW79xV+kqU17///ve/8vDw0Jo1a2SxWGzlCxcuLFXX1dVVvXr1Uq9evTR79mw9++yzevzxx7Vu3TpZrVaFhobqm2++Ua9evf507GJsq9uYwgeneuSRR+Tp6al7771Xx44ds9t3/Phx3Xfffapfv74eeeSRSrVfchF+6aWX7MrnzJlTuYDL4ebmVmolwOXLl9vmjVelb775RhMnTpSvr6/uv//+C8YwZ86cUr+iNmjQQFLp/1ko+QXz3DYKCwtLfXd5eXn67bff7MrCw8Pl6upqW8b1jjvukJubm6ZNm1YqJmOM3b/rBg0aXLJpjgAuP6dPn9aKFSt02223afDgwaW28ePH68SJE7Ylulu1aiU3Nzd98skndu2cf+0LCgpShw4d9Nprr9mmmknSxx9/rG+//bZK+xATEyNvb289++yzOnv2bKn9R44cqXCb9evXl1T62t+0aVP17NlT//73v/XLL79c8Fz/93//py+++EIbN26021/W8vAV9cYbb+iVV15Rly5d1KtXL0llj0vGGP3zn/8sdfyFxjYXFxe7sXDv3r2lXnJb1jLyERERkmQb2+68804dPHhQ8+fPL1X39OnTys/Pt4vn/FhQd3AHCk7Vtm1bLVq0SMOHD1d4eLjuvvtuhYSEaO/evfrPf/6jo0eP6s0336z08p+RkZEaNGiQUlJSdOzYMdsy5j/++KOkqvuF6LbbbtNTTz2l+Ph43XDDDfr222+1ZMmSC7781xGffvqpzpw5Y1uY4rPPPtO7774rHx8fvfPOO3ZTO2677TYtXrxYPj4+at++vTIzM/Xhhx/qyiuvtGszIiJCbm5ueu6555SbmyuLxaKbb75ZN9xwg3x9fRUXF6cHH3xQLi4uWrx4cakE6KOPPtL48eM1ZMgQXXXVVfrtt9+0ePFiubm5adCgQZJ+//X06aefVmJiovbu3asBAwaoYcOG2rNnj9555x2NGTNGDz/8sKTf/x0tW7ZMCQkJuu666+Tl5aXbb7/9or43AJevd999VydOnFC/fv3K3H/99derSZMmWrJkiWJjY+Xj46MhQ4Zozpw5cnFxUWhoqFatWlXm8yzPPvus+vfvrxtvvFHx8fH69ddf9a9//UsdOnSwS6oulre3t+bNm6cRI0bo2muv1dChQ9WkSRPt27dP7733nm688Ua79wA6wtPTU+3bt9eyZct01VVXqXHjxurQoYM6dOiguXPnqmvXrgoPD9fo0aPVunVrZWdnKzMzUwcOHLC903DSpElavHix+vTpowkTJtiWMW/VqtWfTsc/19tvvy0vLy/bwhlr1qzRZ599pk6dOtmWJpeksLAwhYaG6uGHH9bBgwfl7e2t//73v2U+2xUZGSlJevDBBxUTEyM3NzcNHTpUffv21ezZs9WnTx/99a9/1eHDhzV37ly1adPGLuannnpKn3zyifr27atWrVrp8OHDeumll9S8eXPbDJkRI0borbfe0n333ad169bpxhtvVFFRkbZv36633npLa9asUVRUlC2eDz/8ULNnz1ZQUJBCQkJsi4igDnDG0n/A+bZu3WqGDRtmAgMDzRVXXGECAgLMsGHDzLfffluq7oWWQT1/GXNjjMnPzzf333+/ady4sfHy8jIDBgwwO3bsMJLMjBkzbPXKW8a8b9++pc7To0cP06NHD9vnM2fOmIceesgEBgYaT09Pc+ONN5rMzMxS9Sq6jHnJdsUVV5gmTZqY7t27m2eeecYcPny41DG//vqriY+PN35+fsbLy8vExMSY7du3m1atWpVaSnX+/PmmdevWxs3NzW753s8++8xcf/31xtPT0wQFBZlJkyaZNWvW2NX56aefzN/+9jcTGhpqPDw8TOPGjc1NN91kPvzww1Ix/fe//zVdu3Y1DRo0MA0aNDBhYWHm/vvvNzt27LDVOXnypPnrX/9qGjVqVGopYACoqNtvv914eHiY/Pz8cuuMGjXKXHHFFebo0aPGmN+X+B40aJCpX7++8fX1Nffee6/57rvvyrxeL1261ISFhRmLxWI6dOhg3n33XTNo0CATFhZmq1NyrX/++eftji3vFRUl48+XX35Zqn5MTIzx8fExHh4eJjQ01IwaNcp89dVXtjpxcXGmQYMGpfpY1nj4+eefm8jISOPu7l5qSfPdu3ebkSNHmoCAAHPFFVeYZs2amdtuu828/fbbdm1s3brV9OjRw3h4eJhmzZqZ6dOnm//85z8VWsa8ZPPw8DDNmzc3t912m1mwYIHdMuoltm3bZqxWq/Hy8jJ+fn5m9OjR5ptvvin17+a3334zDzzwgGnSpIlxcXGx6/t//vMf07ZtW2OxWExYWJhZuHBhqe8nPT3d9O/f3wQFBRl3d3cTFBRkhg0bZn788Ue7eAoLC81zzz1n/vKXvxiLxWJ8fX1NZGSkmTZtmsnNzbXV2759u+nevbvx9PQ0kljSvI5xMeYCbyAF6qgtW7bommuu0euvv15ly68CAC5PERERatKkSZnP5gCoe3gGCnXe6dOnS5WlpKTI1dXVbrEEAAAu5OzZs6WeAc3IyNA333yjnj17OicoANWOZ6BQ582cOVObNm3STTfdpHr16un999/X+++/rzFjxrDkKADAYQcPHpTVatVdd92loKAgbd++XampqQoICHDo5e4A6gam8KHOW7t2raZNm6Zt27bp5MmTatmypUaMGKHHH3+81Ls0AAAoT25ursaMGaPPPvtMR44cUYMGDdSrVy/NmDGj0osdAah9SKAAAAAAwEE8AwUAAAAADiKBAgAAAAAH1YkHQIqLi3Xo0CE1bNiwyl6MCgBwjDFGJ06cUFBQkFxd+V2uBGMTADjHpR6X6kQCdejQIVZTAwAn279/v5o3b+7sMGoMxiYAcK5LNS7ViQSqYcOGkn7/kry9vZ0cDQBcXvLy8tSiRQvbtRi/Y2wCAOe41ONSpRKouXPn6vnnn1dWVpY6deqkOXPmqHPnzmXWXbFihZ599lnt2rVLZ8+eVdu2bfXQQw9pxIgRtjqjRo3SokWL7I6LiYlRWlqaQ/GUTI3w9vZmkAIAJ2Gamj3GJgBwrks1LlU4gVq2bJkSEhKUmpqq6OhopaSkKCYmRjt27FDTpk1L1W/cuLEef/xxhYWFyd3dXatWrVJ8fLyaNm2qmJgYW70+ffpo4cKFts8Wi6WSXQIAAACAS6PCT1XNnj1bo0ePVnx8vNq3b6/U1FTVr19fCxYsKLN+z549NXDgQLVr106hoaGaMGGCOnbsqPXr19vVs1gsCggIsG2+vr6V6xEAAAAAXCIVSqAKCwu1adMmWa3WPxpwdZXValVmZuafHm+MUXp6unbs2KHu3bvb7cvIyFDTpk119dVXa+zYsTp27Fi57RQUFCgvL89uAwAAAIBLrUJT+I4ePaqioiL5+/vblfv7+2v79u3lHpebm6tmzZqpoKBAbm5ueumll3TLLbfY9vfp00d33HGHQkJCtHv3bj322GO69dZblZmZKTc3t1LtJScna9q0aRUJHQAAAAAuWrWswtewYUNt2bJFJ0+eVHp6uhISEtS6dWv17NlTkjR06FBb3fDwcHXs2FGhoaHKyMhQr169SrWXmJiohIQE2+eSlTYAAAAA4FKqUALl5+cnNzc3ZWdn25VnZ2crICCg3ONcXV3Vpk0bSVJERIR++OEHJScn2xKo87Vu3Vp+fn7atWtXmQmUxWJhkQkAAAAA1a5Cz0C5u7srMjJS6enptrLi4mKlp6erS5cuDrdTXFysgoKCcvcfOHBAx44dU2BgYEXCAwAAAIBLqsJT+BISEhQXF6eoqCh17txZKSkpys/PV3x8vCRp5MiRatasmZKTkyX9/rxSVFSUQkNDVVBQoNWrV2vx4sWaN2+eJOnkyZOaNm2aBg0apICAAO3evVuTJk1SmzZt7JY5BwAAAABnq3ACFRsbqyNHjigpKUlZWVmKiIhQWlqabWGJffv2ydX1jxtb+fn5GjdunA4cOCBPT0+FhYXp9ddfV2xsrCTJzc1NW7du1aJFi5STk6OgoCD17t1b06dPZ5oeAAAAgBrFxRhjnB3ExcrLy5OPj49yc3N52zsAVDOuwWXjewEA57jU198Kv0gXAAAAAC5XJFAAAAAA4KBqeQ8UgD8X/Oh7Tj3/3hl9nXp+AADOxbiImoo7UAAAAADgIBIoAAAAAHAQCRQAAAAAOIgECgAAAAAcRAIFAKgT5s6dq+DgYHl4eCg6OlobN24st+6KFSsUFRWlRo0aqUGDBoqIiNDixYvt6owaNUouLi52W58+fS51NwAANRyr8AEAar1ly5YpISFBqampio6OVkpKimJiYrRjxw41bdq0VP3GjRvr8ccfV1hYmNzd3bVq1SrFx8eradOmiomJsdXr06ePFi5caPtssViqpT8AgJqLO1AAgFpv9uzZGj16tOLj49W+fXulpqaqfv36WrBgQZn1e/bsqYEDB6pdu3YKDQ3VhAkT1LFjR61fv96unsViUUBAgG3z9fWtju4AAGowEigAQK1WWFioTZs2yWq12spcXV1ltVqVmZn5p8cbY5Senq4dO3aoe/fudvsyMjLUtGlTXX311Ro7dqyOHTtW5fEDAGoXpvABAGq1o0ePqqioSP7+/nbl/v7+2r59e7nH5ebmqlmzZiooKJCbm5teeukl3XLLLbb9ffr00R133KGQkBDt3r1bjz32mG699VZlZmbKzc2tVHsFBQUqKCiwfc7Ly6uC3gEAahoSKADAZalhw4basmWLTp48qfT0dCUkJKh169bq2bOnJGno0KG2uuHh4erYsaNCQ0OVkZGhXr16lWovOTlZ06ZNq67wAQBOwhQ+AECt5ufnJzc3N2VnZ9uVZ2dnKyAgoNzjXF1d1aZNG0VEROihhx7S4MGDlZycXG791q1by8/PT7t27Spzf2JionJzc23b/v37K9chAECNRgIFAKjV3N3dFRkZqfT0dFtZcXGx0tPT1aVLF4fbKS4utpuCd74DBw7o2LFjCgwMLHO/xWKRt7e33QYAqHuYwgcAqPUSEhIUFxenqKgode7cWSkpKcrPz1d8fLwkaeTIkWrWrJntDlNycrKioqIUGhqqgoICrV69WosXL9a8efMkSSdPntS0adM0aNAgBQQEaPfu3Zo0aZLatGljt8w5AODyQwIFAKj1YmNjdeTIESUlJSkrK0sRERFKS0uzLSyxb98+ubr+MekiPz9f48aN04EDB+Tp6amwsDC9/vrrio2NlSS5ublp69atWrRokXJychQUFKTevXtr+vTpvAsKAC5zLsYY4+wgLlZeXp58fHyUm5vLlAnUWsGPvufU8++d0dep50ftxTW4bHwvwMVhXERlXerrL89AAQAAAICDSKAAAAAAwEEkUAAAAADgIBIoAAAAAHAQq/ABkMTDugAAAI7gDhQAAAAAOIgECgAAAAAcRAIFAAAAAA4igQIAAAAAB5FAAQAAAICDSKAAAAAAwEEkUAAAAADgIBIoAAAAAHAQCRQAAAAAOKhSCdTcuXMVHBwsDw8PRUdHa+PGjeXWXbFihaKiotSoUSM1aNBAERERWrx4sV0dY4ySkpIUGBgoT09PWa1W7dy5szKhAQAAAMAlU+EEatmyZUpISNDUqVO1efNmderUSTExMTp8+HCZ9Rs3bqzHH39cmZmZ2rp1q+Lj4xUfH681a9bY6sycOVMvvviiUlNTtWHDBjVo0EAxMTE6c+ZM5XsGAAAAAFWswgnU7NmzNXr0aMXHx6t9+/ZKTU1V/fr1tWDBgjLr9+zZUwMHDlS7du0UGhqqCRMmqGPHjlq/fr2k3+8+paSk6IknnlD//v3VsWNHvfbaazp06JBWrlx5UZ0DAAAAgKpUoQSqsLBQmzZtktVq/aMBV1dZrVZlZmb+6fHGGKWnp2vHjh3q3r27JGnPnj3Kysqya9PHx0fR0dHltllQUKC8vDy7DQAAAAAutQolUEePHlVRUZH8/f3tyv39/ZWVlVXucbm5ufLy8pK7u7v69u2rOXPm6JZbbpEk23EVaTM5OVk+Pj62rUWLFhXpBgAAAABUSrWswtewYUNt2bJFX375pZ555hklJCQoIyOj0u0lJiYqNzfXtu3fv7/qggUAAACActSrSGU/Pz+5ubkpOzvbrjw7O1sBAQHlHufq6qo2bdpIkiIiIvTDDz8oOTlZPXv2tB2XnZ2twMBAuzYjIiLKbM9ischisVQkdAAAAAC4aBW6A+Xu7q7IyEilp6fbyoqLi5Wenq4uXbo43E5xcbEKCgokSSEhIQoICLBrMy8vTxs2bKhQmwAAAABwqVXoDpQkJSQkKC4uTlFRUercubNSUlKUn5+v+Ph4SdLIkSPVrFkzJScnS/r9eaWoqCiFhoaqoKBAq1ev1uLFizVv3jxJkouLiyZOnKinn35abdu2VUhIiKZMmaKgoCANGDCg6noKAAAAABepwglUbGysjhw5oqSkJGVlZSkiIkJpaWm2RSD27dsnV9c/bmzl5+dr3LhxOnDggDw9PRUWFqbXX39dsbGxtjqTJk1Sfn6+xowZo5ycHHXt2lVpaWny8PCogi4CAAAAQNVwMcYYZwdxsfLy8uTj46Pc3Fx5e3s7OxygUoIffc/ZITjV3hl9nR0CKolrcNn4XoCL4+xxkXGp9rrU199qWYUPAAAAAOoCEigAAAAAcBAJFAAAAAA4iAQKAAAAABxEAgUAAAAADiKBAgDUCXPnzlVwcLA8PDwUHR2tjRs3llt3xYoVioqKUqNGjdSgQQNFRERo8eLFdnWMMUpKSlJgYKA8PT1ltVq1c+fOS90NAEANRwIFAKj1li1bpoSEBE2dOlWbN29Wp06dFBMTo8OHD5dZv3Hjxnr88ceVmZmprVu3Kj4+XvHx8VqzZo2tzsyZM/Xiiy8qNTVVGzZsUIMGDRQTE6MzZ85UV7cAADUQCRQAoNabPXu2Ro8erfj4eLVv316pqamqX7++FixYUGb9nj17auDAgWrXrp1CQ0M1YcIEdezYUevXr5f0+92nlJQUPfHEE+rfv786duyo1157TYcOHdLKlSursWcAgJqGBAoAUKsVFhZq06ZNslqttjJXV1dZrVZlZmb+6fHGGKWnp2vHjh3q3r27JGnPnj3Kysqya9PHx0fR0dHltllQUKC8vDy7DQBQ95BAAQBqtaNHj6qoqEj+/v525f7+/srKyir3uNzcXHl5ecnd3V19+/bVnDlzdMstt0iS7biKtJmcnCwfHx/b1qJFi4vpFgCghiKBAgBclho2bKgtW7boyy+/1DPPPKOEhARlZGRUur3ExETl5ubatv3791ddsACAGqOeswMAAOBi+Pn5yc3NTdnZ2Xbl2dnZCggIKPc4V1dXtWnTRpIUERGhH374QcnJyerZs6ftuOzsbAUGBtq1GRERUWZ7FotFFovlInsDAKjpuAMFAKjV3N3dFRkZqfT0dFtZcXGx0tPT1aVLF4fbKS4uVkFBgSQpJCREAQEBdm3m5eVpw4YNFWoTAFD3cAcKAFDrJSQkKC4uTlFRUercubNSUlKUn5+v+Ph4SdLIkSPVrFkzJScnS/r9eaWoqCiFhoaqoKBAq1ev1uLFizVv3jxJkouLiyZOnKinn35abdu2VUhIiKZMmaKgoCANGDDAWd0EqlXwo+85OwSgRiKBAgDUerGxsTpy5IiSkpKUlZWliIgIpaWl2RaB2Ldvn1xd/5h0kZ+fr3HjxunAgQPy9PRUWFiYXn/9dcXGxtrqTJo0Sfn5+RozZoxycnLUtWtXpaWlycPDo9r7BwCoOVyMMcbZQVysvLw8+fj4KDc3V97e3s4OB6iUy/2Xvr0z+jo7BFQS1+Cy8b2gtmNcYlyqrS719ZdnoAAAAADAQSRQAAAAAOAgEigAAAAAcBAJFAAAAAA4iAQKAAAAABxEAgUAAAAADiKBAgAAAAAHkUABAAAAgINIoAAAAADAQSRQAAAAAOAgEigAAAAAcBAJFAAAAAA4iAQKAAAAABxEAgUAAAAADiKBAgAAAAAHkUABAAAAgIMqlUDNnTtXwcHB8vDwUHR0tDZu3Fhu3fnz56tbt27y9fWVr6+vrFZrqfqjRo2Si4uL3danT5/KhAYAAAAAl0yFE6hly5YpISFBU6dO1ebNm9WpUyfFxMTo8OHDZdbPyMjQsGHDtG7dOmVmZqpFixbq3bu3Dh48aFevT58++uWXX2zbm2++WbkeAQAAAMAlUuEEavbs2Ro9erTi4+PVvn17paamqn79+lqwYEGZ9ZcsWaJx48YpIiJCYWFheuWVV1RcXKz09HS7ehaLRQEBAbbN19e3cj0CAAAAgEukQglUYWGhNm3aJKvV+kcDrq6yWq3KzMx0qI1Tp07p7Nmzaty4sV15RkaGmjZtqquvvlpjx47VsWPHym2joKBAeXl5dhsAAAAAXGoVSqCOHj2qoqIi+fv725X7+/srKyvLoTYmT56soKAguySsT58+eu2115Senq7nnntOH3/8sW699VYVFRWV2UZycrJ8fHxsW4sWLSrSDQAAAAColHrVebIZM2Zo6dKlysjIkIeHh6186NChtj+Hh4erY8eOCg0NVUZGhnr16lWqncTERCUkJNg+5+XlkUQBAAAAuOQqdAfKz89Pbm5uys7OtivPzs5WQEDABY+dNWuWZsyYoQ8++EAdO3a8YN3WrVvLz89Pu3btKnO/xWKRt7e33QYAAAAAl1qFEih3d3dFRkbaLQBRsiBEly5dyj1u5syZmj59utLS0hQVFfWn5zlw4ICOHTumwMDAioQHAAAAAJdUhVfhS0hI0Pz587Vo0SL98MMPGjt2rPLz8xUfHy9JGjlypBITE231n3vuOU2ZMkULFixQcHCwsrKylJWVpZMnT0qSTp48qUceeURffPGF9u7dq/T0dPXv319t2rRRTExMFXUTAAAAAC5ehZ+Bio2N1ZEjR5SUlKSsrCxFREQoLS3NtrDEvn375Or6R142b948FRYWavDgwXbtTJ06VU8++aTc3Ny0detWLVq0SDk5OQoKClLv3r01ffp0WSyWi+weAAAAAFSdSi0iMX78eI0fP77MfRkZGXaf9+7de8G2PD09tWbNmsqEAQAAAADVqsJT+AAAAADgckUCBQAAAAAOIoECANQJc+fOVXBwsDw8PBQdHa2NGzeWW3f+/Pnq1q2bfH195evrK6vVWqr+qFGj5OLiYrf16dPnUncDAFDDkUABAGq9ZcuWKSEhQVOnTtXmzZvVqVMnxcTE6PDhw2XWz8jI0LBhw7Ru3TplZmaqRYsW6t27tw4ePGhXr0+fPvrll19s25tvvlkd3QEA1GAkUACAWm/27NkaPXq04uPj1b59e6Wmpqp+/fpasGBBmfWXLFmicePGKSIiQmFhYXrllVds7zU8l8ViUUBAgG3z9fWtju4AAGowEigAQK1WWFioTZs2yWq12spcXV1ltVqVmZnpUBunTp3S2bNn1bhxY7vyjIwMNW3aVFdffbXGjh2rY8eOVWnsAIDap1LLmAMAUFMcPXpURUVFtvcRlvD399f27dsdamPy5MkKCgqyS8L69OmjO+64QyEhIdq9e7cee+wx3XrrrcrMzJSbm1upNgoKClRQUGD7nJeXV8keAQBqMhIoAMBlbcaMGVq6dKkyMjLk4eFhKx86dKjtz+Hh4erYsaNCQ0OVkZGhXr16lWonOTlZ06ZNq5aYAQDOwxQ+AECt5ufnJzc3N2VnZ9uVZ2dnKyAg4ILHzpo1SzNmzNAHH3ygjh07XrBu69at5efnp127dpW5PzExUbm5ubZt//79FesIAKBWIIECANRq7u7uioyMtFsAomRBiC5dupR73MyZMzV9+nSlpaUpKirqT89z4MABHTt2TIGBgWXut1gs8vb2ttsAAHUPCRQAoNZLSEjQ/PnztWjRIv3www8aO3as8vPzFR8fL0kaOXKkEhMTbfWfe+45TZkyRQsWLFBwcLCysrKUlZWlkydPSpJOnjypRx55RF988YX27t2r9PR09e/fX23atFFMTIxT+ggAqBl4BgoAUOvFxsbqyJEjSkpKUlZWliIiIpSWlmZbWGLfvn1ydf3jN8N58+apsLBQgwcPtmtn6tSpevLJJ+Xm5qatW7dq0aJFysnJUVBQkHr37q3p06fLYrFUa98AADULCRQAoE4YP368xo8fX+a+jIwMu8979+69YFuenp5as2ZNFUUGAKhLmMIHAAAAAA4igQIAAAAAB5FAAQAAAICDSKAAAAAAwEEkUAAAAADgIBIoAAAAAHAQCRQAAAAAOIgECgAAAAAcRAIFAAAAAA4igQIAAAAAB5FAAQAAAICDSKAAAAAAwEEkUAAAAADgIBIoAAAAAHAQCRQAAAAAOIgECgAAAAAcRAIFAAAAAA4igQIAAAAAB5FAAQAAAICDKpVAzZ07V8HBwfLw8FB0dLQ2btxYbt358+erW7du8vX1la+vr6xWa6n6xhglJSUpMDBQnp6eslqt2rlzZ2VCAwAAAIBLpsIJ1LJly5SQkKCpU6dq8+bN6tSpk2JiYnT48OEy62dkZGjYsGFat26dMjMz1aJFC/Xu3VsHDx601Zk5c6ZefPFFpaamasOGDWrQoIFiYmJ05syZyvcMAAAAAKpYhROo2bNna/To0YqPj1f79u2Vmpqq+vXra8GCBWXWX7JkicaNG6eIiAiFhYXplVdeUXFxsdLT0yX9fvcpJSVFTzzxhPr376+OHTvqtdde06FDh7Ry5cqL6hwAAAAAVKUKJVCFhYXatGmTrFbrHw24uspqtSozM9OhNk6dOqWzZ8+qcePGkqQ9e/YoKyvLrk0fHx9FR0c73CYAAAAAVId6Fal89OhRFRUVyd/f367c399f27dvd6iNyZMnKygoyJYwZWVl2do4v82SfecrKChQQUGB7XNeXp7DfQAAAACAyqrWVfhmzJihpUuX6p133pGHh0el20lOTpaPj49ta9GiRRVGCQAAAABlq1AC5efnJzc3N2VnZ9uVZ2dnKyAg4ILHzpo1SzNmzNAHH3ygjh072spLjqtIm4mJicrNzbVt+/fvr0g3AAAAAKBSKpRAubu7KzIy0rYAhCTbghBdunQp97iZM2dq+vTpSktLU1RUlN2+kJAQBQQE2LWZl5enDRs2lNumxWKRt7e33QYAAAAAl1qFnoGSpISEBMXFxSkqKkqdO3dWSkqK8vPzFR8fL0kaOXKkmjVrpuTkZEnSc889p6SkJL3xxhsKDg62Pdfk5eUlLy8vubi4aOLEiXr66afVtm1bhYSEaMqUKQoKCtKAAQOqrqcAAAAAcJEqnEDFxsbqyJEjSkpKUlZWliIiIpSWlmZbBGLfvn1ydf3jxta8efNUWFiowYMH27UzdepUPfnkk5KkSZMmKT8/X2PGjFFOTo66du2qtLS0i3pOCgAAAACqmosxxjg7iIuVl5cnHx8f5ebmMp0PtVbwo+85OwSn2jujr7NDQCVxDS4b3wtqO8YlxqXa6lJff6t1FT4AAC6VuXPnKjg4WB4eHoqOjtbGjRvLrTt//nx169ZNvr6+8vX1ldVqLVXfGKOkpCQFBgbK09NTVqtVO3fuvNTdAADUcCRQAIBab9myZUpISNDUqVO1efNmderUSTExMTp8+HCZ9TMyMjRs2DCtW7dOmZmZatGihXr37q2DBw/a6sycOVMvvviiUlNTtWHDBjVo0EAxMTE6c+ZMdXULAFADkUABAGq92bNna/To0YqPj1f79u2Vmpqq+vXra8GCBWXWX7JkicaNG6eIiAiFhYXplVdesa0qK/1+9yklJUVPPPGE+vfvr44dO+q1117ToUOHtHLlymrsGQCgpiGBAgDUaoWFhdq0aZOsVqutzNXVVVarVZmZmQ61cerUKZ09e1aNGzeWJO3Zs0dZWVl2bfr4+Cg6OrrcNgsKCpSXl2e3AQDqHhIoAECtdvToURUVFdlWgy3h7+9ve3XGn5k8ebKCgoJsCVPJcRVpMzk5WT4+PratRYsWFe0KAKAWIIECAFzWZsyYoaVLl+qdd965qNdnJCYmKjc317bt37+/CqMEANQUFX4PFAAANYmfn5/c3NyUnZ1tV56dna2AgIALHjtr1izNmDFDH374oTp27GgrLzkuOztbgYGBdm1GRESU2ZbFYpHFYqlkLwAAtQV3oAAAtZq7u7siIyNtC0BIsi0I0aVLl3KPmzlzpqZPn660tDRFRUXZ7QsJCVFAQIBdm3l5edqwYcMF2wQA1H3cgQIA1HoJCQmKi4tTVFSUOnfurJSUFOXn5ys+Pl6SNHLkSDVr1kzJycmSpOeee05JSUl64403FBwcbHuuycvLS15eXnJxcdHEiRP19NNPq23btgoJCdGUKVMUFBSkAQMGOKubAIAagAQKAFDrxcbG6siRI0pKSlJWVpYiIiKUlpZmWwRi3759cnX9Y9LFvHnzVFhYqMGDB9u1M3XqVD355JOSpEmTJik/P19jxoxRTk6OunbtqrS0tIt6TgoAUPu5GGOMs4O4WHl5efLx8VFubq68vb2dHQ5QKcGPvufsEJxq74y+zg4BlcQ1uGx8L6jtGJcYl2qrS3395RkoAAAAAHAQCRQAAAAAOIgECgAAAAAcRAIFAAAAAA4igQIAAAAAB5FAAQAAAICDSKAAAAAAwEEkUAAAAADgIBIoAAAAAHAQCRQAAAAAOIgECgAAAAAcRAIFAAAAAA4igQIAAAAAB5FAAQAAAICDSKAAAAAAwEEkUAAAAADgIBIoAAAAAHAQCRQAAAAAOIgECgAAAAAcRAIFAAAAAA4igQIAAAAAB1UqgZo7d66Cg4Pl4eGh6Ohobdy4sdy633//vQYNGqTg4GC5uLgoJSWlVJ0nn3xSLi4udltYWFhlQgMAAACAS6ZeRQ9YtmyZEhISlJqaqujoaKWkpCgmJkY7duxQ06ZNS9U/deqUWrdurSFDhujvf/97ue3+5S9/0YcffvhHYPUqHBoAAABQJYIffc+p5987o69Tz4/yVfgO1OzZszV69GjFx8erffv2Sk1NVf369bVgwYIy61933XV6/vnnNXToUFkslnLbrVevngICAmybn59fRUMDAAAAgEuqQglUYWGhNm3aJKvV+kcDrq6yWq3KzMy8qEB27typoKAgtW7dWsOHD9e+ffsuqj0AAAAAqGoVSqCOHj2qoqIi+fv725X7+/srKyur0kFER0fr1VdfVVpamubNm6c9e/aoW7duOnHiRJn1CwoKlJeXZ7cBAAAAwKVWIx40uvXWW21/7tixo6Kjo9WqVSu99dZbuvvuu0vVT05O1rRp06ozRAAAAACo2B0oPz8/ubm5KTs72648OztbAQEBVRZUo0aNdNVVV2nXrl1l7k9MTFRubq5t279/f5WdGwAAAADKU6EEyt3dXZGRkUpPT7eVFRcXKz09XV26dKmyoE6ePKndu3crMDCwzP0Wi0Xe3t52GwAAAABcahVehS8hIUHz58/XokWL9MMPP2js2LHKz89XfHy8JGnkyJFKTEy01S8sLNSWLVu0ZcsWFRYW6uDBg9qyZYvd3aWHH35YH3/8sfbu3avPP/9cAwcOlJubm4YNG1YFXQQAXA54RyEAoDpU+Bmo2NhYHTlyRElJScrKylJERITS0tJsC0vs27dPrq5/5GWHDh3SNddcY/s8a9YszZo1Sz169FBGRoYk6cCBAxo2bJiOHTumJk2aqGvXrvriiy/UpEmTi+weAOBywDsKAQDVpVIjwfjx4zV+/Pgy95UkRSWCg4NljLlge0uXLq1MGAAASLJ/R6Ekpaam6r333tOCBQv06KOPlqp/3XXX6brrrpOkMveXKHlHIQAAJSo8hQ8AgJqEdxQCAKoTCRQAoFbjHYUAgOrEZG4AAMrAOwoBAGUhgQL+f8GPvufsEABUQk16R2FCQoLtc15enlq0aFFl5wcA1AxM4QMA1Gq8oxAAUJ24AwUAqPUSEhIUFxenqKgode7cWSkpKaXeUdisWTMlJydL+n3hiW3bttn+XPKOQi8vL7Vp00bS7+8ovP3229WqVSsdOnRIU6dO5R2FAAASKABA7cc7CgEA1YUECgBQJ/COQgBAdeAZKAAAAABwEAkUAAAAADiIBAoAAAAAHEQCBQAAAAAOIoECAAAAAAeRQAEAAACAg0igAAAAAMBBJFAAAAAA4CASKAAAAABwEAkUAAAAADiIBAoAAAAAHEQCBQAAAAAOIoECAAAAAAeRQAEAAACAg0igAAAAAMBBJFAAAAAA4CASKAAAAABwEAkUAAAAADionrMDAABJCn70Paeef++Mvk49PwAAqB24AwUAAAAADiKBAgAAAAAHkUABAAAAgINIoAAAAADAQSRQAAAAAOAgEigAAAAAcFClEqi5c+cqODhYHh4eio6O1saNG8ut+/3332vQoEEKDg6Wi4uLUlJSLrpNAAAAAHCGCidQy5YtU0JCgqZOnarNmzerU6dOiomJ0eHDh8usf+rUKbVu3VozZsxQQEBAlbQJAAAAAM5Q4QRq9uzZGj16tOLj49W+fXulpqaqfv36WrBgQZn1r7vuOj3//PMaOnSoLBZLlbQJAAAAAM5QoQSqsLBQmzZtktVq/aMBV1dZrVZlZmZWKoDKtFlQUKC8vDy7DQAAAAAutQolUEePHlVRUZH8/f3tyv39/ZWVlVWpACrTZnJysnx8fGxbixYtKnVuAAAAAKiIWrkKX2JionJzc23b/v37nR0SAMDJWOAIAFAdKpRA+fn5yc3NTdnZ2Xbl2dnZ5S4QcSnatFgs8vb2ttsAAJcvFjgCAFSXCiVQ7u7uioyMVHp6uq2suLhY6enp6tKlS6UCuBRtAgAuLyxwBACoLvUqekBCQoLi4uIUFRWlzp07KyUlRfn5+YqPj5ckjRw5Us2aNVNycrKk3xeJ2LZtm+3PBw8e1JYtW+Tl5aU2bdo41CYAAOUpWYwoMTHRVlZVCxxVpM2CggIVFBTYPrPAEQDUTRVOoGJjY3XkyBElJSUpKytLERERSktLsy0CsW/fPrm6/nFj69ChQ7rmmmtsn2fNmqVZs2apR48eysjIcKhNAADKc6HFiLZv315tbSYnJ2vatGmVOh8AoPaocAIlSePHj9f48ePL3FeSFJUIDg6WMeai2gQAoKZLTExUQkKC7XNeXh6rxAJAHVSpBAoAgJqiJi1wVN7zVACAuqNWLmMOAEAJFjgCAFQn7kABAGo9FjgCAFQXEigAQK3HAkcAgOpCAgUAqBNY4AgAUB14BgoAAAAAHEQCBQAAAAAOIoECAAAAAAeRQAEAAACAg0igAAAAAMBBJFAAAAAA4CASKAAAAABwEAkUAAAAADiIBAoAAAAAHEQCBQAAAAAOIoECAAAAAAeRQAEAAACAg0igAAAAAMBBJFAAAAAA4CASKAAAAABwEAkUAAAAADiIBAoAAAAAHEQCBQAAAAAOIoECAAAAAAeRQAEAAACAg0igAAAAAMBBJFAAAAAA4CASKAAAAABwEAkUAAAAADionrMDAAAAQGnBj77n7BAAlIE7UAAAAADgIBIoAAAAAHBQpRKouXPnKjg4WB4eHoqOjtbGjRsvWH/58uUKCwuTh4eHwsPDtXr1arv9o0aNkouLi93Wp0+fyoQGAAAAAJdMhROoZcuWKSEhQVOnTtXmzZvVqVMnxcTE6PDhw2XW//zzzzVs2DDdfffd+vrrrzVgwAANGDBA3333nV29Pn366JdffrFtb775ZuV6BAAAAACXSIUTqNmzZ2v06NGKj49X+/btlZqaqvr162vBggVl1v/nP/+pPn366JFHHlG7du00ffp0XXvttfrXv/5lV89isSggIMC2+fr6Vq5HAIDLErMjAADVoUIJVGFhoTZt2iSr1fpHA66uslqtyszMLPOYzMxMu/qSFBMTU6p+RkaGmjZtqquvvlpjx47VsWPHyo2joKBAeXl5dhsA4PLF7AgAQHWpUAJ19OhRFRUVyd/f367c399fWVlZZR6TlZX1p/X79Omj1157Tenp6Xruuef08ccf69Zbb1VRUVGZbSYnJ8vHx8e2tWjRoiLdAADUMcyOAABUlxqxCt/QoUPVr18/hYeHa8CAAVq1apW+/PJLZWRklFk/MTFRubm5tm3//v3VGzAAoMaoKbMjAACXhwq9SNfPz09ubm7Kzs62K8/OzlZAQECZxwQEBFSoviS1bt1afn5+2rVrl3r16lVqv8VikcViqUjoAIA66kKzI7Zv317mMY7OjrjjjjsUEhKi3bt367HHHtOtt96qzMxMubm5lWqzoKBABQUFts9VMb3c2S9S3Tujr1PPDwA1UYXuQLm7uysyMlLp6em2suLiYqWnp6tLly5lHtOlSxe7+pK0du3acutL0oEDB3Ts2DEFBgZWJDwAAKpMRWdHML0cAC4PFboDJUkJCQmKi4tTVFSUOnfurJSUFOXn5ys+Pl6SNHLkSDVr1kzJycmSpAkTJqhHjx564YUX1LdvXy1dulRfffWVXn75ZUnSyZMnNW3aNA0aNEgBAQHavXu3Jk2apDZt2igmJqYKu3phzv6VT+KXPgCojJoyOyIxMVEJCQm2z3l5eSRRAFAHVfgZqNjYWM2aNUtJSUmKiIjQli1blJaWZpsKsW/fPv3yyy+2+jfccIPeeOMNvfzyy+rUqZPefvttrVy5Uh06dJAkubm5aevWrerXr5+uuuoq3X333YqMjNSnn37KND0AwJ+qKbMjLBaLvL297TYAQN1T4TtQkjR+/HiNHz++zH1lTW0YMmSIhgwZUmZ9T09PrVmzpjJhAAAgqe7OjgAA1DyVSqAAAKhJYmNjdeTIESUlJSkrK0sRERGlZke4uv4x6aJkdsQTTzyhxx57TG3bti1zdsSiRYuUk5OjoKAg9e7dW9OnT2d2BABc5kigAAB1ArMjAADVoUa8BwoAAAAAagMSKAAAAABwEAkUAAAAADiIZ6AAAACAGsbZ7yjl/aTl4w4UAAAAADiIBAoAAAAAHEQCBQAAAAAOIoECAAAAAAeRQAEAAACAg0igAAAAAMBBJFAAAAAA4CASKAAAAABwEAkUAAAAADiIBAoAAAAAHEQCBQAAAAAOIoECAAAAAAeRQAEAAACAg0igAAAAAMBBJFAAAAAA4KB6zg4AAGqC4Effc+r5987o69TzAwAAx5BAocZw9v/AAgAAAH+GKXwAAAAA4CDuQAEAAACw4+yZQTV5ajt3oAAAAADAQSRQAAAAAOAgEigAAAAAcBAJFAAAAAA4iAQKAAAAABxEAgUAAAAADmIZcwCoAVguFqh5nP3fJYCaqVJ3oObOnavg4GB5eHgoOjpaGzduvGD95cuXKywsTB4eHgoPD9fq1avt9htjlJSUpMDAQHl6espqtWrnzp2VCQ0AcJlibAIAVIcK34FatmyZEhISlJqaqujoaKWkpCgmJkY7duxQ06ZNS9X//PPPNWzYMCUnJ+u2227TG2+8oQEDBmjz5s3q0KGDJGnmzJl68cUXtWjRIoWEhGjKlCmKiYnRtm3b5OHhcfG9BADUaYxNdRN3gADURC7GGFORA6Kjo3XdddfpX//6lySpuLhYLVq00AMPPKBHH320VP3Y2Fjl5+dr1apVtrLrr79eERERSk1NlTFGQUFBeuihh/Twww9LknJzc+Xv769XX31VQ4cO/dOY8vLy5OPjo9zcXHl7e1ekOzY14SLt7Ck0NeE7AOAcF3P9qYpr8MVibLo0GJcAOEtNHpcqdAeqsLBQmzZtUmJioq3M1dVVVqtVmZmZZR6TmZmphIQEu7KYmBitXLlSkrRnzx5lZWXJarXa9vv4+Cg6OlqZmZllDlIFBQUqKCiwfc7NzZX0+5dVWcUFpyp9bFVp+fflzg4BwGXqYq6fJcdW8Pe4KsPYdOlcTOxVwdn9B+A8NXlcqlACdfToURUVFcnf39+u3N/fX9u3by/zmKysrDLrZ2Vl2faXlJVX53zJycmaNm1aqfIWLVo41hEAgB2flItv48SJE/Lx8bn4hiqIsenSqYq/FwBQGVVx/Tl27NglGZdq5Sp8iYmJdr8cFhcX6/jx47ryyivl4uJS4fby8vLUokUL7d+/32nTT6paXetTXeuPVPf6VNf6I9W9Pl2q/hhjdOLECQUFBVVZm7URY1PF0L/ajf7VbnW9f7m5uWrZsqUaN258SdqvUALl5+cnNzc3ZWdn25VnZ2crICCgzGMCAgIuWL/kn9nZ2QoMDLSrExERUWabFotFFovFrqxRo0YV6UqZvL2969xforrWp7rWH6nu9amu9Ueqe326FP1xxp2nEoxNtRv9q93oX+1W1/vn6nppXnlboVbd3d0VGRmp9PR0W1lxcbHS09PVpUuXMo/p0qWLXX1JWrt2ra1+SEiIAgIC7Ork5eVpw4YN5bYJAEAJxiYAQHWq8BS+hIQExcXFKSoqSp07d1ZKSory8/MVHx8vSRo5cqSaNWum5ORkSdKECRPUo0cPvfDCC+rbt6+WLl2qr776Si+//LIkycXFRRMnTtTTTz+ttm3b2paKDQoK0oABA6qupwCAOouxCQBQXSqcQMXGxurIkSNKSkpSVlaWIiIilJaWZnvQdt++fXa3y2644Qa98cYbeuKJJ/TYY4+pbdu2Wrlype09G5I0adIk5efna8yYMcrJyVHXrl2VlpZWbe/ZsFgsmjp1aqmpF7VZXetTXeuPVPf6VNf6I9W9PtW1/pyLsan2oX+1G/2r3ejfxanwe6AAAAAA4HJ1aZ6sAgAAAIA6iAQKAAAAABxEAgUAAAAADiKBAgAAAAAHkUBJmjt3roKDg+Xh4aHo6Ght3LjR2SE5JDk5Wdddd50aNmyopk2basCAAdqxY4ddnTNnzuj+++/XlVdeKS8vLw0aNKjUyyNrqhkzZtiWEi5RG/tz8OBB3XXXXbryyivl6emp8PBwffXVV7b9xhglJSUpMDBQnp6eslqt2rlzpxMjvrCioiJNmTJFISEh8vT0VGhoqKZPn65z16OpyX365JNPdPvttysoKEguLi5auXKl3X5HYj9+/LiGDx8ub29vNWrUSHfffbdOnjxZjb2wd6E+nT17VpMnT1Z4eLgaNGigoKAgjRw5UocOHbJro6b1CbV3bDpXXR+nzldXxq1z1bUx7Hy1fUw7X10c485VY8Y7c5lbunSpcXd3NwsWLDDff/+9GT16tGnUqJHJzs52dmh/KiYmxixcuNB89913ZsuWLeb//u//TMuWLc3Jkydtde677z7TokULk56ebr766itz/fXXmxtuuMGJUTtm48aNJjg42HTs2NFMmDDBVl7b+nP8+HHTqlUrM2rUKLNhwwbz008/mTVr1phdu3bZ6syYMcP4+PiYlStXmm+++cb069fPhISEmNOnTzsx8vI988wz5sorrzSrVq0ye/bsMcuXLzdeXl7mn//8p61OTe7T6tWrzeOPP25WrFhhJJl33nnHbr8jsffp08d06tTJfPHFF+bTTz81bdq0McOGDavmnvzhQn3KyckxVqvVLFu2zGzfvt1kZmaazp07m8jISLs2alqfLne1eWw6V10ep85XV8atc9XFMex8tX1MO19dHOPOVVPGu8s+gercubO5//77bZ+LiopMUFCQSU5OdmJUlXP48GEjyXz88cfGmN//Il1xxRVm+fLltjo//PCDkWQyMzOdFeafOnHihGnbtq1Zu3at6dGjh20gqo39mTx5sunatWu5+4uLi01AQIB5/vnnbWU5OTnGYrGYN998szpCrLC+ffuav/3tb3Zld9xxhxk+fLgxpnb16fyLryOxb9u2zUgyX375pa3O+++/b1xcXMzBgwerLfbylDVgnm/jxo1Gkvn555+NMTW/T5ejujQ2nauujFPnq0vj1rnq4hh2vro0pp2vLo5x53LmeHdZT+ErLCzUpk2bZLVabWWurq6yWq3KzMx0YmSVk5ubK0lq3LixJGnTpk06e/asXf/CwsLUsmXLGt2/+++/X3379rWLW6qd/Xn33XcVFRWlIUOGqGnTprrmmms0f/582/49e/YoKyvLrk8+Pj6Kjo6usX264YYblJ6erh9//FGS9M0332j9+vW69dZbJdXOPpVwJPbMzEw1atRIUVFRtjpWq1Wurq7asGFDtcdcGbm5uXJxcVGjRo0k1Y0+1SV1bWw6V10Zp85Xl8atc9XFMex8dXlMO9/lMsad61KNd/WqOtDa5OjRoyoqKrK9qb6Ev7+/tm/f7qSoKqe4uFgTJ07UjTfeqA4dOkiSsrKy5O7ubvtLU8Lf319ZWVlOiPLPLV26VJs3b9aXX35Zal9t7M9PP/2kefPmKSEhQY899pi+/PJLPfjgg3J3d1dcXJwt7rL+DtbUPj366KPKy8tTWFiY3NzcVFRUpGeeeUbDhw+XpFrZpxKOxJ6VlaWmTZva7a9Xr54aN25c4/sn/f48xuTJkzVs2DB5e3tLqv19qmvq0th0rroyTp2vro1b56qLY9j56vKYdr7LYYw716Uc7y7rBKouuf/++/Xdd99p/fr1zg6l0vbv368JEyZo7dq18vDwcHY4VaK4uFhRUVF69tlnJUnXXHONvvvuO6WmpiouLs7J0VXOW2+9pSVLluiNN97QX/7yF23ZskUTJ05UUFBQre3T5eLs2bO68847ZYzRvHnznB0OLjN1YZw6X10ct85VF8ew8zGm1U2Xery7rKfw+fn5yc3NrdRqONnZ2QoICHBSVBU3fvx4rVq1SuvWrVPz5s1t5QEBASosLFROTo5d/Zrav02bNunw4cO69tprVa9ePdWrV08ff/yxXnzxRdWrV0/+/v61qj+SFBgYqPbt29uVtWvXTvv27ZMkW9y16e/gI488okcffVRDhw5VeHi4RowYob///e9KTk6WVDv7VMKR2AMCAnT48GG7/b/99puOHz9eo/tXMpj8/PPPWrt2re3XOKn29qmuqitj07nqyjh1vro4bp2rLo5h56vLY9r56vIYd67qGO8u6wTK3d1dkZGRSk9Pt5UVFxcrPT1dXbp0cWJkjjHGaPz48XrnnXf00UcfKSQkxG5/ZGSkrrjiCrv+7dixQ/v27auR/evVq5e+/fZbbdmyxbZFRUVp+PDhtj/Xpv5I0o033lhqyd4ff/xRrVq1kiSFhIQoICDArk95eXnasGFDje3TqVOn5Opqf+lwc3NTcXGxpNrZpxKOxN6lSxfl5ORo06ZNtjofffSRiouLFR0dXe0xO6JkMNm5c6c+/PBDXXnllXb7a2Of6rLaPjadq66NU+eri+PWueriGHa+ujymna+ujnHnqrbxrsJLXtQxS5cuNRaLxbz66qtm27ZtZsyYMaZRo0YmKyvL2aH9qbFjxxofHx+TkZFhfvnlF9t26tQpW5377rvPtGzZ0nz00Ufmq6++Ml26dDFdunRxYtQVc+5qRsbUvv5s3LjR1KtXzzzzzDNm586dZsmSJaZ+/frm9ddft9WZMWOGadSokfnf//5ntm7davr3719jl0c1xpi4uDjTrFkz25KvK1asMH5+fmbSpEm2OjW5TydOnDBff/21+frrr40kM3v2bPP111/bVuhxJPY+ffqYa665xmzYsMGsX7/etG3b1qlLvF6oT4WFhaZfv36mefPmZsuWLXbXioKCghrbp8tdbR6bznU5jFPnq+3j1rnq4hh2vto+pp2vLo5x56op491ln0AZY8ycOXNMy5Ytjbu7u+ncubP54osvnB2SQySVuS1cuNBW5/Tp02bcuHHG19fX1K9f3wwcOND88ssvzgu6gs4fiGpjf/7f//t/pkOHDsZisZiwsDDz8ssv2+0vLi42U6ZMMf7+/sZisZhevXqZHTt2OCnaP5eXl2cmTJhgWrZsaTw8PEzr1q3N448/bndxqsl9WrduXZn/3cTFxRljHIv92LFjZtiwYcbLy8t4e3ub+Ph4c+LECSf05ncX6tOePXvKvVasW7euxvYJtXdsOtflME6dry6MW+eqa2PY+Wr7mHa+ujjGnaumjHcuxpzzqmUAAAAAQLku62egAAAAAKAiSKAAAAAAwEEkUAAAAADgIBIoAAAAAHAQCRQAAAAAOIgECgAAAAAcRAIFAAAAAA4igQIAAAAAB5FAAQAAAICDSKAAAAAAwEEkUAAAAADgIBIoAAAAAHDQ/wfSrco7QW5FaAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL DIVERGENCE:  0.06242586714580429\n",
            "The distributions are similar\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_distributions_tolerance(list1, list2, tolerance=0.07):\n",
        "    \"\"\"Compares the distributions of two lists and plots them side by side.\n",
        "\n",
        "    Args:\n",
        "        list1 (list): The first list to compare.\n",
        "        list2 (list): The second list to compare.\n",
        "        tolerance (float): The tolerance for accepting similarity between the distributions.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the distributions are similar, False otherwise.\n",
        "    \"\"\"\n",
        "    # Calculate the histograms for the two lists\n",
        "    hist1, bins1 = np.histogram(list1, density=True)\n",
        "    hist2, bins2 = np.histogram(list2, density=True)\n",
        "\n",
        "    # Normalize the histograms to have unit area\n",
        "    hist1 = hist1 / np.sum(hist1)\n",
        "    hist2 = hist2 / np.sum(hist2)\n",
        "\n",
        "    # Calculate the KL divergence between the two histograms\n",
        "    kl_div = stats.entropy(hist2, hist1)\n",
        "\n",
        "    # Plot the histograms side by side\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    ax[0].bar(bins1[:-1], hist1, width=np.diff(bins1), align='edge')\n",
        "    ax[1].bar(bins2[:-1], hist2, width=np.diff(bins2), align='edge')\n",
        "    ax[0].set_title('Original Dataset')\n",
        "    ax[1].set_title('Augmented Dataset')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Check if the KL divergence is within the tolerance\n",
        "    print(\"KL DIVERGENCE: \", kl_div)\n",
        "    similar = True if kl_div <= tolerance else False\n",
        "\n",
        "    if similar:\n",
        "      print(\"The distributions are similar\")\n",
        "    else:\n",
        "      print(\"The distrubutions are NOT similar\")\n",
        "    return similar\n",
        "\n",
        "\n",
        "compare_distributions_tolerance(list(df[numerical_cols[2]]), list(new_df[numerical_cols[2]]))\n",
        "# new_df.to_csv('augmented_dataset.csv', index=False)"
      ],
      "id": "jtmfU3j2P9sA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PUT IT ALL TOGETHER"
      ],
      "metadata": {
        "id": "W3ptvFuiWa7k"
      },
      "id": "W3ptvFuiWa7k"
    },
    {
      "cell_type": "code",
      "source": [
        "# def prepare_for_regression(df, categorical_cols, proper_noun_cols=None):\n",
        "#   regression_df = df.drop(columns=proper_noun_cols)\n",
        "#   regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "#   return regression_df\n",
        "\n",
        "# def separate_columns(df):\n",
        "#   numerical_cols = list()\n",
        "#   categorical_cols = list()\n",
        "#   proper_noun_cols = list()\n",
        "#   for (index, colname) in enumerate(df):\n",
        "#       if colname in df.select_dtypes(include='object').columns:\n",
        "#         unique_vals = df[colname].unique()\n",
        "#         if len(unique_vals) >= 0.85 * df.shape[0]:\n",
        "#           proper_noun_cols.append(colname)\n",
        "#         else:\n",
        "#           categorical_cols.append(colname)\n",
        "#       else:\n",
        "#         numerical_cols.append(colname)\n",
        "#   return numerical_cols, categorical_cols, proper_noun_cols\n",
        "\n",
        "# def run_data_augmenter(dataset, num_samples=None, percentage_augmentation=None):\n",
        "#     df = pd.read_csv(dataset, header=\"infer\")\n",
        "#     print(df.info())\n",
        "#     print(df)\n",
        "\n",
        "\n",
        "#     # DATA PREPROCESSING\n",
        "#     df = df.dropna(axis=1, how='all')\n",
        "#     # df = df.dropna()\n",
        "#     df = df.fillna(0)\n",
        "#     new_data = 0\n",
        "#     if num_samples != None:\n",
        "#       new_data = num_samples\n",
        "#     else:\n",
        "#       new_data = int(df.shape[0] * percentage_augmentation)\n",
        "    \n",
        "#     numerical_cols, categorical_cols, _ = separate_columns(df)\n",
        "#     regression_df = prepare_for_regression(df, categorical_cols)\n",
        "    \n",
        "#     np.seterr(invalid='warn') \n",
        "#     np.seterr(under='warn')\n",
        "    \n",
        "#     new_df = augment_dataset(regression_df, new_data)\n",
        "#     new_df = reverse_one_hot_encode(new_df, categorical_cols)\n",
        "\n",
        "    \n",
        "#     num_cat_generated = generate_categorical_variables(df, new_df)\n",
        "#     # final_df.head()\n",
        "\n",
        "#     final_df = pd.concat([df, num_cat_generated], axis=0)\n",
        "\n",
        "#     # make sure that all columns are rounded appropriately\n",
        "#     for col in final_df.columns:\n",
        "#       if col in numerical_cols:\n",
        "#         # print(col)\n",
        "#         # all(print(x) for x in col)\n",
        "#         if all((x*1.0).is_integer() for x in df[col]):\n",
        "#           final_df[col] = np.round(final_df[col], 0)\n",
        "#           # print(final_df[col])\n",
        "#           print(final_df[col].value_counts())\n",
        "    \n",
        "#     return final_df"
      ],
      "metadata": {
        "id": "3wxGi_QLUdV6"
      },
      "id": "3wxGi_QLUdV6",
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkDfuWoz7FJ"
      },
      "source": [
        "##Downstream ML task\n",
        "\n",
        "To demonstrate the usefulness of this dataset augmentation, we're going to be predicting the diabetes diagnoses of patients."
      ],
      "id": "okkDfuWoz7FJ"
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "8j2Gng6G0RcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ea19fa-8a3d-4b91-dc42-85e8f882a64a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "609            1      111             62             13      182  24.0   \n",
            "610            3      106             54             21      158  30.9   \n",
            "611            3      174             58             22      194  32.9   \n",
            "612            7      168             88             42      321  38.2   \n",
            "613            6      105             80             28        0  32.5   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "609                     0.138   23        0  \n",
            "610                     0.292   24        0  \n",
            "611                     0.593   36        1  \n",
            "612                     0.787   40        1  \n",
            "613                     0.878   26        0  \n",
            "\n",
            "[614 rows x 9 columns]\n",
            "Epoch 1/150\n",
            "62/62 [==============================] - 2s 3ms/step - loss: 19.3221 - accuracy: 0.6384 - precision_15: 0.2353 - recall_15: 0.0188\n",
            "Epoch 2/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 2.1776 - accuracy: 0.4104 - precision_15: 0.2508 - recall_15: 0.3521\n",
            "Epoch 3/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 1.2578 - accuracy: 0.4935 - precision_15: 0.2870 - recall_15: 0.3099\n",
            "Epoch 4/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 1.0377 - accuracy: 0.5570 - precision_15: 0.3532 - recall_15: 0.3333\n",
            "Epoch 5/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.9034 - accuracy: 0.5928 - precision_15: 0.4098 - recall_15: 0.3944\n",
            "Epoch 6/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.8034 - accuracy: 0.6107 - precision_15: 0.4261 - recall_15: 0.3521\n",
            "Epoch 7/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.8065 - accuracy: 0.6238 - precision_15: 0.4531 - recall_15: 0.4085\n",
            "Epoch 8/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.7395 - accuracy: 0.6173 - precision_15: 0.4450 - recall_15: 0.4178\n",
            "Epoch 9/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.7366 - accuracy: 0.6254 - precision_15: 0.4479 - recall_15: 0.3427\n",
            "Epoch 10/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6990 - accuracy: 0.6743 - precision_15: 0.5340 - recall_15: 0.4789\n",
            "Epoch 11/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.7099 - accuracy: 0.6433 - precision_15: 0.4842 - recall_15: 0.4319\n",
            "Epoch 12/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.7074 - accuracy: 0.6580 - precision_15: 0.5087 - recall_15: 0.4131\n",
            "Epoch 13/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.6384 - precision_15: 0.4772 - recall_15: 0.4413\n",
            "Epoch 14/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6547 - accuracy: 0.6612 - precision_15: 0.5153 - recall_15: 0.3944\n",
            "Epoch 15/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6768 - accuracy: 0.6645 - precision_15: 0.5198 - recall_15: 0.4319\n",
            "Epoch 16/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7681 - accuracy: 0.6433 - precision_15: 0.4847 - recall_15: 0.4460\n",
            "Epoch 17/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6430 - accuracy: 0.6678 - precision_15: 0.5236 - recall_15: 0.4695\n",
            "Epoch 18/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.6971 - precision_15: 0.5746 - recall_15: 0.4883\n",
            "Epoch 19/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.6547 - precision_15: 0.5024 - recall_15: 0.4883\n",
            "Epoch 20/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6733 - accuracy: 0.6743 - precision_15: 0.5363 - recall_15: 0.4507\n",
            "Epoch 21/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6600 - accuracy: 0.6694 - precision_15: 0.5260 - recall_15: 0.4742\n",
            "Epoch 22/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.6808 - precision_15: 0.5455 - recall_15: 0.4789\n",
            "Epoch 23/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.6906 - precision_15: 0.5628 - recall_15: 0.4836\n",
            "Epoch 24/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5960 - accuracy: 0.6873 - precision_15: 0.5556 - recall_15: 0.4930\n",
            "Epoch 25/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6532 - accuracy: 0.6580 - precision_15: 0.5086 - recall_15: 0.4178\n",
            "Epoch 26/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6119 - accuracy: 0.7003 - precision_15: 0.5838 - recall_15: 0.4742\n",
            "Epoch 27/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5896 - accuracy: 0.7182 - precision_15: 0.6099 - recall_15: 0.5211\n",
            "Epoch 28/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6111 - accuracy: 0.7117 - precision_15: 0.5947 - recall_15: 0.5305\n",
            "Epoch 29/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6416 - accuracy: 0.6743 - precision_15: 0.5363 - recall_15: 0.4507\n",
            "Epoch 30/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6116 - accuracy: 0.6889 - precision_15: 0.5611 - recall_15: 0.4742\n",
            "Epoch 31/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6181 - accuracy: 0.7052 - precision_15: 0.5889 - recall_15: 0.4977\n",
            "Epoch 32/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.6840 - precision_15: 0.5531 - recall_15: 0.4648\n",
            "Epoch 33/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.7134 - precision_15: 0.6069 - recall_15: 0.4930\n",
            "Epoch 34/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.6922 - precision_15: 0.5723 - recall_15: 0.4460\n",
            "Epoch 35/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.6873 - precision_15: 0.5522 - recall_15: 0.5211\n",
            "Epoch 36/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.6938 - precision_15: 0.5723 - recall_15: 0.4648\n",
            "Epoch 37/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6277 - accuracy: 0.6889 - precision_15: 0.5585 - recall_15: 0.4930\n",
            "Epoch 38/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6322 - accuracy: 0.6743 - precision_15: 0.5344 - recall_15: 0.4742\n",
            "Epoch 39/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5858 - accuracy: 0.7101 - precision_15: 0.6036 - recall_15: 0.4789\n",
            "Epoch 40/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.6889 - precision_15: 0.5539 - recall_15: 0.5305\n",
            "Epoch 41/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.7068 - precision_15: 0.6051 - recall_15: 0.4460\n",
            "Epoch 42/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.7036 - precision_15: 0.5803 - recall_15: 0.5258\n",
            "Epoch 43/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5869 - accuracy: 0.7020 - precision_15: 0.5843 - recall_15: 0.4883\n",
            "Epoch 44/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5925 - accuracy: 0.6954 - precision_15: 0.5756 - recall_15: 0.4648\n",
            "Epoch 45/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5968 - accuracy: 0.6987 - precision_15: 0.5769 - recall_15: 0.4930\n",
            "Epoch 46/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5741 - accuracy: 0.7052 - precision_15: 0.5851 - recall_15: 0.5164\n",
            "Epoch 47/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.7052 - precision_15: 0.5899 - recall_15: 0.4930\n",
            "Epoch 48/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7231 - precision_15: 0.6201 - recall_15: 0.5211\n",
            "Epoch 49/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5937 - accuracy: 0.7134 - precision_15: 0.6045 - recall_15: 0.5023\n",
            "Epoch 50/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5963 - accuracy: 0.7020 - precision_15: 0.5862 - recall_15: 0.4789\n",
            "Epoch 51/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5902 - accuracy: 0.7117 - precision_15: 0.6034 - recall_15: 0.4930\n",
            "Epoch 52/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5763 - accuracy: 0.7134 - precision_15: 0.6082 - recall_15: 0.4883\n",
            "Epoch 53/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.7215 - precision_15: 0.6193 - recall_15: 0.5117\n",
            "Epoch 54/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5699 - accuracy: 0.7150 - precision_15: 0.6145 - recall_15: 0.4789\n",
            "Epoch 55/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5666 - accuracy: 0.7280 - precision_15: 0.6369 - recall_15: 0.5023\n",
            "Epoch 56/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5736 - accuracy: 0.7085 - precision_15: 0.5934 - recall_15: 0.5070\n",
            "Epoch 57/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5763 - accuracy: 0.7182 - precision_15: 0.6205 - recall_15: 0.4836\n",
            "Epoch 58/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5877 - accuracy: 0.7020 - precision_15: 0.5824 - recall_15: 0.4977\n",
            "Epoch 59/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.7248 - precision_15: 0.6183 - recall_15: 0.5399\n",
            "Epoch 60/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.7020 - precision_15: 0.5904 - recall_15: 0.4601\n",
            "Epoch 61/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.7296 - precision_15: 0.6328 - recall_15: 0.5258\n",
            "Epoch 62/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6250 - accuracy: 0.6840 - precision_15: 0.5543 - recall_15: 0.4554\n",
            "Epoch 63/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5455 - accuracy: 0.7231 - precision_15: 0.6243 - recall_15: 0.5070\n",
            "Epoch 64/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5543 - accuracy: 0.7345 - precision_15: 0.6374 - recall_15: 0.5446\n",
            "Epoch 65/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6096 - accuracy: 0.7085 - precision_15: 0.5966 - recall_15: 0.4930\n",
            "Epoch 66/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5652 - accuracy: 0.7215 - precision_15: 0.6167 - recall_15: 0.5211\n",
            "Epoch 67/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5896 - accuracy: 0.7150 - precision_15: 0.6118 - recall_15: 0.4883\n",
            "Epoch 68/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7199 - precision_15: 0.6158 - recall_15: 0.5117\n",
            "Epoch 69/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.7231 - precision_15: 0.6272 - recall_15: 0.4977\n",
            "Epoch 70/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7427 - precision_15: 0.6647 - recall_15: 0.5211\n",
            "Epoch 71/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5475 - accuracy: 0.7296 - precision_15: 0.6442 - recall_15: 0.4930\n",
            "Epoch 72/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7052 - precision_15: 0.5909 - recall_15: 0.4883\n",
            "Epoch 73/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5988 - accuracy: 0.7166 - precision_15: 0.5951 - recall_15: 0.5728\n",
            "Epoch 74/150\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5888 - accuracy: 0.6971 - precision_15: 0.5771 - recall_15: 0.4742\n",
            "Epoch 75/150\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5625 - accuracy: 0.7199 - precision_15: 0.6171 - recall_15: 0.5070\n",
            "Epoch 76/150\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 0.5568 - accuracy: 0.7394 - precision_15: 0.6667 - recall_15: 0.4977\n",
            "Epoch 77/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5538 - accuracy: 0.7459 - precision_15: 0.6647 - recall_15: 0.5399\n",
            "Epoch 78/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5452 - accuracy: 0.7394 - precision_15: 0.6550 - recall_15: 0.5258\n",
            "Epoch 79/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5525 - accuracy: 0.7362 - precision_15: 0.6564 - recall_15: 0.5023\n",
            "Epoch 80/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5650 - accuracy: 0.7101 - precision_15: 0.5936 - recall_15: 0.5211\n",
            "Epoch 81/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5518 - accuracy: 0.7329 - precision_15: 0.6400 - recall_15: 0.5258\n",
            "Epoch 82/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7427 - precision_15: 0.6503 - recall_15: 0.5587\n",
            "Epoch 83/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5499 - accuracy: 0.7117 - precision_15: 0.6034 - recall_15: 0.4930\n",
            "Epoch 84/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5804 - accuracy: 0.7248 - precision_15: 0.6236 - recall_15: 0.5211\n",
            "Epoch 85/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6313 - accuracy: 0.7020 - precision_15: 0.5765 - recall_15: 0.5305\n",
            "Epoch 86/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5514 - accuracy: 0.7150 - precision_15: 0.6033 - recall_15: 0.5211\n",
            "Epoch 87/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5363 - accuracy: 0.7459 - precision_15: 0.6727 - recall_15: 0.5211\n",
            "Epoch 88/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5479 - accuracy: 0.7280 - precision_15: 0.6337 - recall_15: 0.5117\n",
            "Epoch 89/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5719 - accuracy: 0.7280 - precision_15: 0.6292 - recall_15: 0.5258\n",
            "Epoch 90/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5273 - accuracy: 0.7443 - precision_15: 0.6573 - recall_15: 0.5493\n",
            "Epoch 91/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5460 - accuracy: 0.7459 - precision_15: 0.6647 - recall_15: 0.5399\n",
            "Epoch 92/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5788 - accuracy: 0.7264 - precision_15: 0.6316 - recall_15: 0.5070\n",
            "Epoch 93/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5282 - accuracy: 0.7655 - precision_15: 0.6949 - recall_15: 0.5775\n",
            "Epoch 94/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5593 - accuracy: 0.7280 - precision_15: 0.6337 - recall_15: 0.5117\n",
            "Epoch 95/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5479 - accuracy: 0.7362 - precision_15: 0.6564 - recall_15: 0.5023\n",
            "Epoch 96/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5223 - accuracy: 0.7524 - precision_15: 0.6918 - recall_15: 0.5164\n",
            "Epoch 97/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5532 - accuracy: 0.7410 - precision_15: 0.6517 - recall_15: 0.5446\n",
            "Epoch 98/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.7150 - precision_15: 0.6067 - recall_15: 0.5070\n",
            "Epoch 99/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7573 - precision_15: 0.6860 - recall_15: 0.5540\n",
            "Epoch 100/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5402 - accuracy: 0.7394 - precision_15: 0.6732 - recall_15: 0.4836\n",
            "Epoch 101/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.7166 - precision_15: 0.6089 - recall_15: 0.5117\n",
            "Epoch 102/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.7508 - precision_15: 0.6596 - recall_15: 0.5822\n",
            "Epoch 103/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7508 - precision_15: 0.6875 - recall_15: 0.5164\n",
            "Epoch 104/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7329 - precision_15: 0.6384 - recall_15: 0.5305\n",
            "Epoch 105/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7443 - precision_15: 0.6538 - recall_15: 0.5587\n",
            "Epoch 106/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5696 - accuracy: 0.7264 - precision_15: 0.6398 - recall_15: 0.4836\n",
            "Epoch 107/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7443 - precision_15: 0.6538 - recall_15: 0.5587\n",
            "Epoch 108/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5253 - accuracy: 0.7378 - precision_15: 0.6548 - recall_15: 0.5164\n",
            "Epoch 109/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.7427 - precision_15: 0.6608 - recall_15: 0.5305\n",
            "Epoch 110/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7459 - precision_15: 0.6667 - recall_15: 0.5352\n",
            "Epoch 111/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7459 - precision_15: 0.6557 - recall_15: 0.5634\n",
            "Epoch 112/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5225 - accuracy: 0.7622 - precision_15: 0.7006 - recall_15: 0.5493\n",
            "Epoch 113/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.7410 - precision_15: 0.6607 - recall_15: 0.5211\n",
            "Epoch 114/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7524 - precision_15: 0.6564 - recall_15: 0.6009\n",
            "Epoch 115/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.7410 - precision_15: 0.6552 - recall_15: 0.5352\n",
            "Epoch 116/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7492 - precision_15: 0.6980 - recall_15: 0.4883\n",
            "Epoch 117/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7508 - precision_15: 0.6923 - recall_15: 0.5070\n",
            "Epoch 118/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5423 - accuracy: 0.7345 - precision_15: 0.6404 - recall_15: 0.5352\n",
            "Epoch 119/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7606 - precision_15: 0.7012 - recall_15: 0.5399\n",
            "Epoch 120/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7557 - precision_15: 0.6821 - recall_15: 0.5540\n",
            "Epoch 121/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7378 - precision_15: 0.6383 - recall_15: 0.5634\n",
            "Epoch 122/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7329 - precision_15: 0.6541 - recall_15: 0.4883\n",
            "Epoch 123/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7590 - precision_15: 0.6994 - recall_15: 0.5352\n",
            "Epoch 124/150\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5150 - accuracy: 0.7394 - precision_15: 0.6626 - recall_15: 0.5070\n",
            "Epoch 125/150\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5478 - accuracy: 0.7459 - precision_15: 0.6508 - recall_15: 0.5775\n",
            "Epoch 126/150\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.5103 - accuracy: 0.7541 - precision_15: 0.6703 - recall_15: 0.5728\n",
            "Epoch 127/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7622 - precision_15: 0.7055 - recall_15: 0.5399\n",
            "Epoch 128/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7541 - precision_15: 0.6824 - recall_15: 0.5446\n",
            "Epoch 129/150\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5083 - accuracy: 0.7443 - precision_15: 0.6591 - recall_15: 0.5446\n",
            "Epoch 130/150\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5264 - accuracy: 0.7476 - precision_15: 0.6835 - recall_15: 0.5070\n",
            "Epoch 131/150\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.5370 - accuracy: 0.7524 - precision_15: 0.6743 - recall_15: 0.5540\n",
            "Epoch 132/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6633 - accuracy: 0.6954 - precision_15: 0.5691 - recall_15: 0.5023\n",
            "Epoch 133/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7427 - precision_15: 0.6627 - recall_15: 0.5258\n",
            "Epoch 134/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7655 - precision_15: 0.7143 - recall_15: 0.5399\n",
            "Epoch 135/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7524 - precision_15: 0.6564 - recall_15: 0.6009\n",
            "Epoch 136/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4989 - accuracy: 0.7638 - precision_15: 0.7073 - recall_15: 0.5446\n",
            "Epoch 137/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7524 - precision_15: 0.6943 - recall_15: 0.5117\n",
            "Epoch 138/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7622 - precision_15: 0.7006 - recall_15: 0.5493\n",
            "Epoch 139/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5518 - accuracy: 0.7427 - precision_15: 0.6554 - recall_15: 0.5446\n",
            "Epoch 140/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.7476 - precision_15: 0.6726 - recall_15: 0.5305\n",
            "Epoch 141/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7427 - precision_15: 0.6730 - recall_15: 0.5023\n",
            "Epoch 142/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7362 - precision_15: 0.6364 - recall_15: 0.5587\n",
            "Epoch 143/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7476 - precision_15: 0.6726 - recall_15: 0.5305\n",
            "Epoch 144/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.7524 - precision_15: 0.6826 - recall_15: 0.5352\n",
            "Epoch 145/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.7785 - precision_15: 0.7516 - recall_15: 0.5399\n",
            "Epoch 146/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7590 - precision_15: 0.6816 - recall_15: 0.5728\n",
            "Epoch 147/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6314 - accuracy: 0.7036 - precision_15: 0.5963 - recall_15: 0.4507\n",
            "Epoch 148/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7557 - precision_15: 0.6740 - recall_15: 0.5728\n",
            "Epoch 149/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7492 - precision_15: 0.6595 - recall_15: 0.5728\n",
            "Epoch 150/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7655 - precision_15: 0.7315 - recall_15: 0.5117\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5582 - accuracy: 0.7273 - precision_15: 0.6182 - recall_15: 0.6182\n",
            "Accuracy: 72.73%\n",
            "Precision: 61.82%\n",
            "Recall: 61.82%\n"
          ]
        }
      ],
      "source": [
        "eval_original_df = df\n",
        "eval_augmented_df = final_df\n",
        "\n",
        "if 'id' in eval_original_df.columns:\n",
        "  eval_original_df = df.drop(columns=['id'])\n",
        "  eval_augmented_df = final_df.drop(columns=['id'])\n",
        "\n",
        "\n",
        "experiments = []\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "train_percentages=[]\n",
        "test_percentages=[]\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras.metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "# data = pd.read_csv('path/to/dataset.csv')\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "test_size = 0.2\n",
        "train_data, test_data = train_test_split(eval_original_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_data)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "experiments.append(\"Original dataset\")\n",
        "test_percentages.append(test_size)\n",
        "train_percentages.append(1-test_size)\n",
        "accuracies.append(accuracy)\n",
        "precisions.append(precision)\n",
        "recalls.append(recall)\n",
        "\n"
      ],
      "id": "8j2Gng6G0RcV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# data = pd.read_csv('path/to/dataset.csv')\n",
        "\n",
        "# Double the size of the original df\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "test_size = 0.2\n",
        "train_data, test_data = train_test_split(eval_original_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "\n",
        "print(\"Before: \", len(train_data))\n",
        "# train_data = np.concatenate((train_data, train_data))\n",
        "train_data = pd.concat([train_data, train_data])\n",
        "print(\"After: \", train_data.shape[0])\n",
        "\n",
        "\n",
        "print(\"Before: \", test_data.shape[0])\n",
        "# test_data = np.concatenate((test_data, test_data))\n",
        "test_data = pd.concat([test_data, test_data])\n",
        "print(\"After: \", test_data.shape[0])\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_data)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "\n",
        "\n",
        "experiments.append(\"Original dataset x2\")\n",
        "test_percentages.append(test_size)\n",
        "train_percentages.append(1-test_size)\n",
        "accuracies.append(accuracy)\n",
        "precisions.append(precision)\n",
        "recalls.append(recall)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfL_-ecgmQ1O",
        "outputId": "3a7596d4-e0e4-4d00-b585-cff11c9666fa"
      },
      "id": "IfL_-ecgmQ1O",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:  614\n",
            "After:  1228\n",
            "Before:  154\n",
            "After:  308\n",
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "609            1      111             62             13      182  24.0   \n",
            "610            3      106             54             21      158  30.9   \n",
            "611            3      174             58             22      194  32.9   \n",
            "612            7      168             88             42      321  38.2   \n",
            "613            6      105             80             28        0  32.5   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "609                     0.138   23        0  \n",
            "610                     0.292   24        0  \n",
            "611                     0.593   36        1  \n",
            "612                     0.787   40        1  \n",
            "613                     0.878   26        0  \n",
            "\n",
            "[1228 rows x 9 columns]\n",
            "Epoch 1/150\n",
            "123/123 [==============================] - 2s 3ms/step - loss: 1.9112 - accuracy: 0.5725 - precision_16: 0.3813 - recall_16: 0.3732\n",
            "Epoch 2/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.9954 - accuracy: 0.6221 - precision_16: 0.4552 - recall_16: 0.4531\n",
            "Epoch 3/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.8631 - accuracy: 0.6254 - precision_16: 0.4591 - recall_16: 0.4484\n",
            "Epoch 4/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.7848 - accuracy: 0.6336 - precision_16: 0.4710 - recall_16: 0.4577\n",
            "Epoch 5/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.6992 - accuracy: 0.6669 - precision_16: 0.5207 - recall_16: 0.5023\n",
            "Epoch 6/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.6244 - accuracy: 0.6954 - precision_16: 0.5707 - recall_16: 0.4930\n",
            "Epoch 7/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.6067 - accuracy: 0.7068 - precision_16: 0.5887 - recall_16: 0.5141\n",
            "Epoch 8/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.7239 - precision_16: 0.6074 - recall_16: 0.5775\n",
            "Epoch 9/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5944 - accuracy: 0.6987 - precision_16: 0.5745 - recall_16: 0.5070\n",
            "Epoch 10/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5874 - accuracy: 0.7036 - precision_16: 0.5847 - recall_16: 0.5023\n",
            "Epoch 11/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.7077 - precision_16: 0.5918 - recall_16: 0.5070\n",
            "Epoch 12/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.7077 - precision_16: 0.5884 - recall_16: 0.5235\n",
            "Epoch 13/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5804 - accuracy: 0.7199 - precision_16: 0.6079 - recall_16: 0.5423\n",
            "Epoch 14/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5730 - accuracy: 0.7101 - precision_16: 0.5978 - recall_16: 0.5023\n",
            "Epoch 15/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7142 - precision_16: 0.5969 - recall_16: 0.5423\n",
            "Epoch 16/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7109 - precision_16: 0.5994 - recall_16: 0.5023\n",
            "Epoch 17/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7264 - precision_16: 0.6223 - recall_16: 0.5376\n",
            "Epoch 18/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.7231 - precision_16: 0.6250 - recall_16: 0.5047\n",
            "Epoch 19/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5519 - accuracy: 0.7231 - precision_16: 0.6144 - recall_16: 0.5423\n",
            "Epoch 20/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.7296 - precision_16: 0.6263 - recall_16: 0.5469\n",
            "Epoch 21/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.7158 - precision_16: 0.6066 - recall_16: 0.5141\n",
            "Epoch 22/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5453 - accuracy: 0.7313 - precision_16: 0.6283 - recall_16: 0.5516\n",
            "Epoch 23/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5395 - accuracy: 0.7272 - precision_16: 0.6275 - recall_16: 0.5258\n",
            "Epoch 24/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7305 - precision_16: 0.6267 - recall_16: 0.5516\n",
            "Epoch 25/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7280 - precision_16: 0.6271 - recall_16: 0.5329\n",
            "Epoch 26/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5405 - accuracy: 0.7402 - precision_16: 0.6499 - recall_16: 0.5446\n",
            "Epoch 27/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7223 - precision_16: 0.6133 - recall_16: 0.5399\n",
            "Epoch 28/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7280 - precision_16: 0.6204 - recall_16: 0.5563\n",
            "Epoch 29/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7288 - precision_16: 0.6310 - recall_16: 0.5258\n",
            "Epoch 30/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7166 - precision_16: 0.6066 - recall_16: 0.5211\n",
            "Epoch 31/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7329 - precision_16: 0.6332 - recall_16: 0.5469\n",
            "Epoch 32/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7296 - precision_16: 0.6335 - recall_16: 0.5235\n",
            "Epoch 33/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7378 - precision_16: 0.6413 - recall_16: 0.5540\n",
            "Epoch 34/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7402 - precision_16: 0.6434 - recall_16: 0.5634\n",
            "Epoch 35/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7362 - precision_16: 0.6409 - recall_16: 0.5446\n",
            "Epoch 36/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5279 - accuracy: 0.7370 - precision_16: 0.6403 - recall_16: 0.5516\n",
            "Epoch 37/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7370 - precision_16: 0.6419 - recall_16: 0.5469\n",
            "Epoch 38/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7239 - precision_16: 0.6232 - recall_16: 0.5164\n",
            "Epoch 39/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.7353 - precision_16: 0.6399 - recall_16: 0.5423\n",
            "Epoch 40/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7280 - precision_16: 0.6223 - recall_16: 0.5493\n",
            "Epoch 41/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7337 - precision_16: 0.6272 - recall_16: 0.5728\n",
            "Epoch 42/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7541 - precision_16: 0.6703 - recall_16: 0.5728\n",
            "Epoch 43/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7476 - precision_16: 0.6568 - recall_16: 0.5704\n",
            "Epoch 44/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.5096 - accuracy: 0.7484 - precision_16: 0.6657 - recall_16: 0.5516\n",
            "Epoch 45/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.5091 - accuracy: 0.7451 - precision_16: 0.6628 - recall_16: 0.5399\n",
            "Epoch 46/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7459 - precision_16: 0.6667 - recall_16: 0.5352\n",
            "Epoch 47/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.5114 - accuracy: 0.7435 - precision_16: 0.6546 - recall_16: 0.5516\n",
            "Epoch 48/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.5144 - accuracy: 0.7402 - precision_16: 0.6490 - recall_16: 0.5469\n",
            "Epoch 49/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.7427 - precision_16: 0.6528 - recall_16: 0.5516\n",
            "Epoch 50/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7557 - precision_16: 0.6760 - recall_16: 0.5681\n",
            "Epoch 51/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4992 - accuracy: 0.7500 - precision_16: 0.6648 - recall_16: 0.5634\n",
            "Epoch 52/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4982 - accuracy: 0.7573 - precision_16: 0.6829 - recall_16: 0.5610\n",
            "Epoch 53/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.5001 - accuracy: 0.7728 - precision_16: 0.7014 - recall_16: 0.6009\n",
            "Epoch 54/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.5055 - accuracy: 0.7573 - precision_16: 0.6798 - recall_16: 0.5681\n",
            "Epoch 55/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4974 - accuracy: 0.7549 - precision_16: 0.6791 - recall_16: 0.5563\n",
            "Epoch 56/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4971 - accuracy: 0.7524 - precision_16: 0.6667 - recall_16: 0.5728\n",
            "Epoch 57/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4978 - accuracy: 0.7508 - precision_16: 0.6685 - recall_16: 0.5587\n",
            "Epoch 58/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7500 - precision_16: 0.6545 - recall_16: 0.5915\n",
            "Epoch 59/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7598 - precision_16: 0.6944 - recall_16: 0.5493\n",
            "Epoch 60/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7459 - precision_16: 0.6575 - recall_16: 0.5587\n",
            "Epoch 61/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7492 - precision_16: 0.6648 - recall_16: 0.5587\n",
            "Epoch 62/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7647 - precision_16: 0.6952 - recall_16: 0.5728\n",
            "Epoch 63/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7687 - precision_16: 0.6929 - recall_16: 0.5986\n",
            "Epoch 64/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7549 - precision_16: 0.6761 - recall_16: 0.5634\n",
            "Epoch 65/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7557 - precision_16: 0.6694 - recall_16: 0.5845\n",
            "Epoch 66/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.7614 - precision_16: 0.6873 - recall_16: 0.5728\n",
            "Epoch 67/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7565 - precision_16: 0.6740 - recall_16: 0.5775\n",
            "Epoch 68/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.7614 - precision_16: 0.6842 - recall_16: 0.5798\n",
            "Epoch 69/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7712 - precision_16: 0.6965 - recall_16: 0.6033\n",
            "Epoch 70/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.7687 - precision_16: 0.7139 - recall_16: 0.5563\n",
            "Epoch 71/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7647 - precision_16: 0.6908 - recall_16: 0.5822\n",
            "Epoch 72/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.7598 - precision_16: 0.6785 - recall_16: 0.5845\n",
            "Epoch 73/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7565 - precision_16: 0.6667 - recall_16: 0.5962\n",
            "Epoch 74/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7687 - precision_16: 0.7006 - recall_16: 0.5822\n",
            "Epoch 75/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.7614 - precision_16: 0.6822 - recall_16: 0.5845\n",
            "Epoch 76/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7638 - precision_16: 0.6878 - recall_16: 0.5845\n",
            "Epoch 77/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7687 - precision_16: 0.7064 - recall_16: 0.5704\n",
            "Epoch 78/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7687 - precision_16: 0.6919 - recall_16: 0.6009\n",
            "Epoch 79/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.7679 - precision_16: 0.6942 - recall_16: 0.5915\n",
            "Epoch 80/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.7630 - precision_16: 0.6880 - recall_16: 0.5798\n",
            "Epoch 81/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.7590 - precision_16: 0.6826 - recall_16: 0.5704\n",
            "Epoch 82/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.7671 - precision_16: 0.6977 - recall_16: 0.5798\n",
            "Epoch 83/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7671 - precision_16: 0.6966 - recall_16: 0.5822\n",
            "Epoch 84/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7467 - precision_16: 0.6533 - recall_16: 0.5751\n",
            "Epoch 85/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4763 - accuracy: 0.7630 - precision_16: 0.6860 - recall_16: 0.5845\n",
            "Epoch 86/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7573 - precision_16: 0.6788 - recall_16: 0.5704\n",
            "Epoch 87/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7638 - precision_16: 0.6858 - recall_16: 0.5892\n",
            "Epoch 88/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.7728 - precision_16: 0.7094 - recall_16: 0.5845\n",
            "Epoch 89/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.7655 - precision_16: 0.6927 - recall_16: 0.5822\n",
            "Epoch 90/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.7736 - precision_16: 0.6958 - recall_16: 0.6174\n",
            "Epoch 91/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.7671 - precision_16: 0.6989 - recall_16: 0.5775\n",
            "Epoch 92/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.7590 - precision_16: 0.6747 - recall_16: 0.5892\n",
            "Epoch 93/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.7704 - precision_16: 0.7057 - recall_16: 0.5798\n",
            "Epoch 94/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.7704 - precision_16: 0.7034 - recall_16: 0.5845\n",
            "Epoch 95/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7704 - precision_16: 0.6978 - recall_16: 0.5962\n",
            "Epoch 96/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4765 - accuracy: 0.7712 - precision_16: 0.6986 - recall_16: 0.5986\n",
            "Epoch 97/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4776 - accuracy: 0.7638 - precision_16: 0.6878 - recall_16: 0.5845\n",
            "Epoch 98/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4662 - accuracy: 0.7752 - precision_16: 0.7060 - recall_16: 0.6033\n",
            "Epoch 99/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4777 - accuracy: 0.7630 - precision_16: 0.6810 - recall_16: 0.5962\n",
            "Epoch 100/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4777 - accuracy: 0.7581 - precision_16: 0.6817 - recall_16: 0.5681\n",
            "Epoch 101/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4710 - accuracy: 0.7655 - precision_16: 0.6788 - recall_16: 0.6150\n",
            "Epoch 102/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4725 - accuracy: 0.7655 - precision_16: 0.6971 - recall_16: 0.5728\n",
            "Epoch 103/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4700 - accuracy: 0.7687 - precision_16: 0.6940 - recall_16: 0.5962\n",
            "Epoch 104/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.7712 - precision_16: 0.7054 - recall_16: 0.5845\n",
            "Epoch 105/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4643 - accuracy: 0.7818 - precision_16: 0.7182 - recall_16: 0.6103\n",
            "Epoch 106/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.7704 - precision_16: 0.7045 - recall_16: 0.5822\n",
            "Epoch 107/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4754 - accuracy: 0.7614 - precision_16: 0.6852 - recall_16: 0.5775\n",
            "Epoch 108/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4630 - accuracy: 0.7736 - precision_16: 0.7022 - recall_16: 0.6033\n",
            "Epoch 109/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4637 - accuracy: 0.7752 - precision_16: 0.7083 - recall_16: 0.5986\n",
            "Epoch 110/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7793 - precision_16: 0.7171 - recall_16: 0.6009\n",
            "Epoch 111/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.7736 - precision_16: 0.7033 - recall_16: 0.6009\n",
            "Epoch 112/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.7850 - precision_16: 0.7213 - recall_16: 0.6197\n",
            "Epoch 113/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.7809 - precision_16: 0.7175 - recall_16: 0.6080\n",
            "Epoch 114/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4802 - accuracy: 0.7622 - precision_16: 0.6811 - recall_16: 0.5915\n",
            "Epoch 115/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4559 - accuracy: 0.7728 - precision_16: 0.7130 - recall_16: 0.5775\n",
            "Epoch 116/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.7777 - precision_16: 0.7008 - recall_16: 0.6268\n",
            "Epoch 117/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7858 - precision_16: 0.7209 - recall_16: 0.6244\n",
            "Epoch 118/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7801 - precision_16: 0.7241 - recall_16: 0.5915\n",
            "Epoch 119/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4514 - accuracy: 0.7826 - precision_16: 0.7214 - recall_16: 0.6080\n",
            "Epoch 120/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4507 - accuracy: 0.7850 - precision_16: 0.7213 - recall_16: 0.6197\n",
            "Epoch 121/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7769 - precision_16: 0.7123 - recall_16: 0.5986\n",
            "Epoch 122/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7818 - precision_16: 0.7194 - recall_16: 0.6080\n",
            "Epoch 123/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4507 - accuracy: 0.7980 - precision_16: 0.7405 - recall_16: 0.6432\n",
            "Epoch 124/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.7850 - precision_16: 0.7213 - recall_16: 0.6197\n",
            "Epoch 125/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.7818 - precision_16: 0.7158 - recall_16: 0.6150\n",
            "Epoch 126/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7761 - precision_16: 0.7127 - recall_16: 0.5939\n",
            "Epoch 127/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7679 - precision_16: 0.6921 - recall_16: 0.5962\n",
            "Epoch 128/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7793 - precision_16: 0.7112 - recall_16: 0.6127\n",
            "Epoch 129/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.7818 - precision_16: 0.7194 - recall_16: 0.6080\n",
            "Epoch 130/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.7842 - precision_16: 0.7242 - recall_16: 0.6103\n",
            "Epoch 131/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.7818 - precision_16: 0.7170 - recall_16: 0.6127\n",
            "Epoch 132/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4480 - accuracy: 0.7899 - precision_16: 0.7333 - recall_16: 0.6197\n",
            "Epoch 133/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.7720 - precision_16: 0.7005 - recall_16: 0.5986\n",
            "Epoch 134/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4484 - accuracy: 0.7858 - precision_16: 0.7270 - recall_16: 0.6127\n",
            "Epoch 135/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7728 - precision_16: 0.6899 - recall_16: 0.6268\n",
            "Epoch 136/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.7761 - precision_16: 0.7057 - recall_16: 0.6080\n",
            "Epoch 137/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.7801 - precision_16: 0.7131 - recall_16: 0.6127\n",
            "Epoch 138/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7801 - precision_16: 0.7167 - recall_16: 0.6056\n",
            "Epoch 139/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4457 - accuracy: 0.7761 - precision_16: 0.7035 - recall_16: 0.6127\n",
            "Epoch 140/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4492 - accuracy: 0.7842 - precision_16: 0.7280 - recall_16: 0.6033\n",
            "Epoch 141/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.7923 - precision_16: 0.7209 - recall_16: 0.6549\n",
            "Epoch 142/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7875 - precision_16: 0.7337 - recall_16: 0.6080\n",
            "Epoch 143/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7736 - precision_16: 0.6927 - recall_16: 0.6244\n",
            "Epoch 144/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7801 - precision_16: 0.7063 - recall_16: 0.6268\n",
            "Epoch 145/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.7883 - precision_16: 0.7280 - recall_16: 0.6221\n",
            "Epoch 146/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.7720 - precision_16: 0.7028 - recall_16: 0.5939\n",
            "Epoch 147/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4428 - accuracy: 0.7875 - precision_16: 0.7248 - recall_16: 0.6244\n",
            "Epoch 148/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.7866 - precision_16: 0.7216 - recall_16: 0.6268\n",
            "Epoch 149/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4436 - accuracy: 0.7712 - precision_16: 0.6997 - recall_16: 0.5962\n",
            "Epoch 150/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4503 - accuracy: 0.7842 - precision_16: 0.7218 - recall_16: 0.6150\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6246 - accuracy: 0.7338 - precision_16: 0.6296 - recall_16: 0.6182\n",
            "Accuracy: 73.38%\n",
            "Precision: 62.96%\n",
            "Recall: 61.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "woXDJmn21s5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13134228-dda7-42aa-fb8c-c2f96c841460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin        BMI  \\\n",
            "0            6.0    148.0           72.0           35.0      0.0  33.600000   \n",
            "1            1.0     85.0           66.0           29.0      0.0  26.600000   \n",
            "2            8.0    183.0           64.0            0.0      0.0  23.300000   \n",
            "3            1.0     89.0           66.0           23.0     94.0  28.100000   \n",
            "4            0.0    137.0           40.0           35.0    168.0  43.100000   \n",
            "..           ...      ...            ...            ...      ...        ...   \n",
            "455          2.0    107.0           68.0           19.0      3.0  30.090891   \n",
            "456         10.0    180.0           90.0           44.0    294.0  44.128829   \n",
            "457          2.0    100.0           64.0           13.0      1.0  27.874374   \n",
            "458          3.0    116.0           72.0           25.0     64.0  32.307407   \n",
            "459          8.0    161.0           86.0           40.0    207.0  40.568969   \n",
            "\n",
            "     DiabetesPedigreeFunction   Age  Outcome  \n",
            "0                    0.627000  50.0      1.0  \n",
            "1                    0.351000  31.0      0.0  \n",
            "2                    0.672000  32.0      1.0  \n",
            "3                    0.167000  21.0      0.0  \n",
            "4                    2.288000  33.0      1.0  \n",
            "..                        ...   ...      ...  \n",
            "455                  0.328845  27.0      0.0  \n",
            "456                  1.088412  57.0      1.0  \n",
            "457                  0.267892  25.0      0.0  \n",
            "458                  0.406208  30.0      0.0  \n",
            "459                  0.846945  49.0      1.0  \n",
            "\n",
            "[1228 rows x 9 columns]\n",
            "Epoch 1/150\n",
            "123/123 [==============================] - 2s 3ms/step - loss: 1.4722 - accuracy: 0.6800 - precision_17: 0.5367 - recall_17: 0.6131\n",
            "Epoch 2/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.6676 - accuracy: 0.7223 - precision_17: 0.5821 - recall_17: 0.7273\n",
            "Epoch 3/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.6120 - accuracy: 0.7223 - precision_17: 0.5880 - recall_17: 0.6853\n",
            "Epoch 4/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.7370 - precision_17: 0.6052 - recall_17: 0.7110\n",
            "Epoch 5/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5655 - accuracy: 0.7598 - precision_17: 0.6450 - recall_17: 0.6946\n",
            "Epoch 6/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7622 - precision_17: 0.6519 - recall_17: 0.6853\n",
            "Epoch 7/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5452 - accuracy: 0.7663 - precision_17: 0.6628 - recall_17: 0.6737\n",
            "Epoch 8/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7801 - precision_17: 0.6819 - recall_17: 0.6946\n",
            "Epoch 9/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7679 - precision_17: 0.6682 - recall_17: 0.6667\n",
            "Epoch 10/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7622 - precision_17: 0.6619 - recall_17: 0.6527\n",
            "Epoch 11/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7736 - precision_17: 0.6793 - recall_17: 0.6667\n",
            "Epoch 12/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7638 - precision_17: 0.6576 - recall_17: 0.6760\n",
            "Epoch 13/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7728 - precision_17: 0.6728 - recall_17: 0.6807\n",
            "Epoch 14/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5220 - accuracy: 0.7598 - precision_17: 0.6509 - recall_17: 0.6737\n",
            "Epoch 15/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7720 - precision_17: 0.6745 - recall_17: 0.6713\n",
            "Epoch 16/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7720 - precision_17: 0.6705 - recall_17: 0.6830\n",
            "Epoch 17/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7769 - precision_17: 0.6765 - recall_17: 0.6923\n",
            "Epoch 18/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.7785 - precision_17: 0.6865 - recall_17: 0.6737\n",
            "Epoch 19/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.7704 - precision_17: 0.6780 - recall_17: 0.6527\n",
            "Epoch 20/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7809 - precision_17: 0.6835 - recall_17: 0.6946\n",
            "Epoch 21/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.7940 - precision_17: 0.7189 - recall_17: 0.6737\n",
            "Epoch 22/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7858 - precision_17: 0.6976 - recall_17: 0.6830\n",
            "Epoch 23/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.7769 - precision_17: 0.6867 - recall_17: 0.6643\n",
            "Epoch 24/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.7915 - precision_17: 0.7055 - recall_17: 0.6923\n",
            "Epoch 25/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.7932 - precision_17: 0.7059 - recall_17: 0.6993\n",
            "Epoch 26/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.7932 - precision_17: 0.7108 - recall_17: 0.6876\n",
            "Epoch 27/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4541 - accuracy: 0.7956 - precision_17: 0.7119 - recall_17: 0.6970\n",
            "Epoch 28/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4524 - accuracy: 0.8005 - precision_17: 0.7201 - recall_17: 0.7016\n",
            "Epoch 29/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.7997 - precision_17: 0.7328 - recall_17: 0.6713\n",
            "Epoch 30/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8054 - precision_17: 0.7251 - recall_17: 0.7133\n",
            "Epoch 31/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7948 - precision_17: 0.7143 - recall_17: 0.6876\n",
            "Epoch 32/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4442 - accuracy: 0.8046 - precision_17: 0.7255 - recall_17: 0.7086\n",
            "Epoch 33/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4370 - accuracy: 0.8078 - precision_17: 0.7383 - recall_17: 0.6970\n",
            "Epoch 34/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.8086 - precision_17: 0.7354 - recall_17: 0.7063\n",
            "Epoch 35/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8127 - precision_17: 0.7398 - recall_17: 0.7156\n",
            "Epoch 36/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.8070 - precision_17: 0.7319 - recall_17: 0.7063\n",
            "Epoch 37/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.8094 - precision_17: 0.7327 - recall_17: 0.7156\n",
            "Epoch 38/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8013 - precision_17: 0.7262 - recall_17: 0.6923\n",
            "Epoch 39/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.8111 - precision_17: 0.7362 - recall_17: 0.7156\n",
            "Epoch 40/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8111 - precision_17: 0.7408 - recall_17: 0.7063\n",
            "Epoch 41/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4249 - accuracy: 0.8143 - precision_17: 0.7481 - recall_17: 0.7063\n",
            "Epoch 42/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8160 - precision_17: 0.7494 - recall_17: 0.7110\n",
            "Epoch 43/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8192 - precision_17: 0.7518 - recall_17: 0.7203\n",
            "Epoch 44/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4207 - accuracy: 0.8078 - precision_17: 0.7406 - recall_17: 0.6923\n",
            "Epoch 45/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4132 - accuracy: 0.8314 - precision_17: 0.7734 - recall_17: 0.7319\n",
            "Epoch 46/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8103 - precision_17: 0.7450 - recall_17: 0.6946\n",
            "Epoch 47/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4136 - accuracy: 0.8111 - precision_17: 0.7385 - recall_17: 0.7110\n",
            "Epoch 48/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4228 - accuracy: 0.8208 - precision_17: 0.7543 - recall_17: 0.7226\n",
            "Epoch 49/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4111 - accuracy: 0.8184 - precision_17: 0.7537 - recall_17: 0.7133\n",
            "Epoch 50/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4242 - accuracy: 0.8078 - precision_17: 0.7348 - recall_17: 0.7040\n",
            "Epoch 51/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4231 - accuracy: 0.8233 - precision_17: 0.7650 - recall_17: 0.7133\n",
            "Epoch 52/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.8143 - precision_17: 0.7422 - recall_17: 0.7179\n",
            "Epoch 53/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4110 - accuracy: 0.8192 - precision_17: 0.7543 - recall_17: 0.7156\n",
            "Epoch 54/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4094 - accuracy: 0.8233 - precision_17: 0.7598 - recall_17: 0.7226\n",
            "Epoch 55/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4095 - accuracy: 0.8160 - precision_17: 0.7544 - recall_17: 0.7016\n",
            "Epoch 56/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8274 - precision_17: 0.7666 - recall_17: 0.7273\n",
            "Epoch 57/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8233 - precision_17: 0.7512 - recall_17: 0.7389\n",
            "Epoch 58/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8298 - precision_17: 0.7750 - recall_17: 0.7226\n",
            "Epoch 59/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.8168 - precision_17: 0.7512 - recall_17: 0.7110\n",
            "Epoch 60/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8339 - precision_17: 0.7834 - recall_17: 0.7249\n",
            "Epoch 61/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8265 - precision_17: 0.7673 - recall_17: 0.7226\n",
            "Epoch 62/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.8282 - precision_17: 0.7672 - recall_17: 0.7296\n",
            "Epoch 63/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8241 - precision_17: 0.7617 - recall_17: 0.7226\n",
            "Epoch 64/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8249 - precision_17: 0.7675 - recall_17: 0.7156\n",
            "Epoch 65/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.8339 - precision_17: 0.7751 - recall_17: 0.7389\n",
            "Epoch 66/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8282 - precision_17: 0.7753 - recall_17: 0.7156\n",
            "Epoch 67/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8233 - precision_17: 0.7573 - recall_17: 0.7273\n",
            "Epoch 68/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8265 - precision_17: 0.7673 - recall_17: 0.7226\n",
            "Epoch 69/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8265 - precision_17: 0.7621 - recall_17: 0.7319\n",
            "Epoch 70/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3998 - accuracy: 0.8249 - precision_17: 0.7649 - recall_17: 0.7203\n",
            "Epoch 71/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8282 - precision_17: 0.7795 - recall_17: 0.7086\n",
            "Epoch 72/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8233 - precision_17: 0.7663 - recall_17: 0.7110\n",
            "Epoch 73/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8225 - precision_17: 0.7605 - recall_17: 0.7179\n",
            "Epoch 74/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8331 - precision_17: 0.7814 - recall_17: 0.7249\n",
            "Epoch 75/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8412 - precision_17: 0.7910 - recall_17: 0.7413\n",
            "Epoch 76/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8249 - precision_17: 0.7662 - recall_17: 0.7179\n",
            "Epoch 77/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8396 - precision_17: 0.7857 - recall_17: 0.7436\n",
            "Epoch 78/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.8306 - precision_17: 0.7826 - recall_17: 0.7133\n",
            "Epoch 79/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8379 - precision_17: 0.7904 - recall_17: 0.7296\n",
            "Epoch 80/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3790 - accuracy: 0.8396 - precision_17: 0.7829 - recall_17: 0.7483\n",
            "Epoch 81/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8282 - precision_17: 0.7711 - recall_17: 0.7226\n",
            "Epoch 82/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8453 - precision_17: 0.7980 - recall_17: 0.7459\n",
            "Epoch 83/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3821 - accuracy: 0.8298 - precision_17: 0.7764 - recall_17: 0.7203\n",
            "Epoch 84/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8290 - precision_17: 0.7704 - recall_17: 0.7273\n",
            "Epoch 85/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8404 - precision_17: 0.7905 - recall_17: 0.7389\n",
            "Epoch 86/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.8322 - precision_17: 0.7753 - recall_17: 0.7319\n",
            "Epoch 87/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8200 - precision_17: 0.7562 - recall_17: 0.7156\n",
            "Epoch 88/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8412 - precision_17: 0.8015 - recall_17: 0.7249\n",
            "Epoch 89/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.8363 - precision_17: 0.7754 - recall_17: 0.7483\n",
            "Epoch 90/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3777 - accuracy: 0.8396 - precision_17: 0.7915 - recall_17: 0.7343\n",
            "Epoch 91/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8379 - precision_17: 0.7778 - recall_17: 0.7506\n",
            "Epoch 92/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3777 - accuracy: 0.8453 - precision_17: 0.8072 - recall_17: 0.7319\n",
            "Epoch 93/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8257 - precision_17: 0.7641 - recall_17: 0.7249\n",
            "Epoch 94/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3809 - accuracy: 0.8428 - precision_17: 0.7850 - recall_17: 0.7576\n",
            "Epoch 95/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8322 - precision_17: 0.7753 - recall_17: 0.7319\n",
            "Epoch 96/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8477 - precision_17: 0.8040 - recall_17: 0.7459\n",
            "Epoch 97/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3688 - accuracy: 0.8477 - precision_17: 0.8071 - recall_17: 0.7413\n",
            "Epoch 98/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3774 - accuracy: 0.8347 - precision_17: 0.7825 - recall_17: 0.7296\n",
            "Epoch 99/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.8314 - precision_17: 0.7748 - recall_17: 0.7296\n",
            "Epoch 100/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3814 - accuracy: 0.8396 - precision_17: 0.7788 - recall_17: 0.7552\n",
            "Epoch 101/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8331 - precision_17: 0.7843 - recall_17: 0.7203\n",
            "Epoch 102/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8445 - precision_17: 0.8020 - recall_17: 0.7366\n",
            "Epoch 103/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8363 - precision_17: 0.7879 - recall_17: 0.7273\n",
            "Epoch 104/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3731 - accuracy: 0.8379 - precision_17: 0.7889 - recall_17: 0.7319\n",
            "Epoch 105/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3708 - accuracy: 0.8404 - precision_17: 0.7848 - recall_17: 0.7483\n",
            "Epoch 106/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8485 - precision_17: 0.7985 - recall_17: 0.7576\n",
            "Epoch 107/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.8461 - precision_17: 0.7970 - recall_17: 0.7506\n",
            "Epoch 108/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8477 - precision_17: 0.8010 - recall_17: 0.7506\n",
            "Epoch 109/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8355 - precision_17: 0.7918 - recall_17: 0.7179\n",
            "Epoch 110/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3668 - accuracy: 0.8396 - precision_17: 0.7871 - recall_17: 0.7413\n",
            "Epoch 111/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8355 - precision_17: 0.7873 - recall_17: 0.7249\n",
            "Epoch 112/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3590 - accuracy: 0.8436 - precision_17: 0.7985 - recall_17: 0.7389\n",
            "Epoch 113/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8379 - precision_17: 0.7904 - recall_17: 0.7296\n",
            "Epoch 114/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8388 - precision_17: 0.7924 - recall_17: 0.7296\n",
            "Epoch 115/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3739 - accuracy: 0.8339 - precision_17: 0.7820 - recall_17: 0.7273\n",
            "Epoch 116/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.8379 - precision_17: 0.7861 - recall_17: 0.7366\n",
            "Epoch 117/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8428 - precision_17: 0.7921 - recall_17: 0.7459\n",
            "Epoch 118/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8404 - precision_17: 0.7935 - recall_17: 0.7343\n",
            "Epoch 119/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.8412 - precision_17: 0.7840 - recall_17: 0.7529\n",
            "Epoch 120/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3594 - accuracy: 0.8428 - precision_17: 0.7980 - recall_17: 0.7366\n",
            "Epoch 121/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8396 - precision_17: 0.7816 - recall_17: 0.7506\n",
            "Epoch 122/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8436 - precision_17: 0.7985 - recall_17: 0.7389\n",
            "Epoch 123/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.8420 - precision_17: 0.8036 - recall_17: 0.7249\n",
            "Epoch 124/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.8322 - precision_17: 0.7700 - recall_17: 0.7413\n",
            "Epoch 125/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.8493 - precision_17: 0.8050 - recall_17: 0.7506\n",
            "Epoch 126/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8436 - precision_17: 0.7897 - recall_17: 0.7529\n",
            "Epoch 127/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.8428 - precision_17: 0.8010 - recall_17: 0.7319\n",
            "Epoch 128/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8518 - precision_17: 0.8175 - recall_17: 0.7413\n",
            "Epoch 129/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3623 - accuracy: 0.8420 - precision_17: 0.7960 - recall_17: 0.7366\n",
            "Epoch 130/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.8493 - precision_17: 0.8020 - recall_17: 0.7552\n",
            "Epoch 131/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3559 - accuracy: 0.8485 - precision_17: 0.8045 - recall_17: 0.7483\n",
            "Epoch 132/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3590 - accuracy: 0.8404 - precision_17: 0.7877 - recall_17: 0.7436\n",
            "Epoch 133/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3514 - accuracy: 0.8379 - precision_17: 0.7861 - recall_17: 0.7366\n",
            "Epoch 134/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3551 - accuracy: 0.8404 - precision_17: 0.7877 - recall_17: 0.7436\n",
            "Epoch 135/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.8493 - precision_17: 0.8005 - recall_17: 0.7576\n",
            "Epoch 136/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8485 - precision_17: 0.8045 - recall_17: 0.7483\n",
            "Epoch 137/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8469 - precision_17: 0.8035 - recall_17: 0.7436\n",
            "Epoch 138/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8420 - precision_17: 0.7945 - recall_17: 0.7389\n",
            "Epoch 139/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8404 - precision_17: 0.7862 - recall_17: 0.7459\n",
            "Epoch 140/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3473 - accuracy: 0.8502 - precision_17: 0.8040 - recall_17: 0.7552\n",
            "Epoch 141/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.8453 - precision_17: 0.7965 - recall_17: 0.7483\n",
            "Epoch 142/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8493 - precision_17: 0.7990 - recall_17: 0.7599\n",
            "Epoch 143/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8461 - precision_17: 0.7985 - recall_17: 0.7483\n",
            "Epoch 144/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8485 - precision_17: 0.8045 - recall_17: 0.7483\n",
            "Epoch 145/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8396 - precision_17: 0.7871 - recall_17: 0.7413\n",
            "Epoch 146/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3581 - accuracy: 0.8469 - precision_17: 0.8020 - recall_17: 0.7459\n",
            "Epoch 147/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3457 - accuracy: 0.8510 - precision_17: 0.8060 - recall_17: 0.7552\n",
            "Epoch 148/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8298 - precision_17: 0.7644 - recall_17: 0.7413\n",
            "Epoch 149/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3493 - accuracy: 0.8436 - precision_17: 0.7926 - recall_17: 0.7483\n",
            "Epoch 150/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 0.8412 - precision_17: 0.7925 - recall_17: 0.7389\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0997 - accuracy: 0.9968 - precision_17: 0.9906 - recall_17: 1.0000\n",
            "Accuracy: 99.68%\n",
            "Precision: 99.06%\n",
            "Recall: 100.00%\n"
          ]
        }
      ],
      "source": [
        "test_size = 0.2\n",
        "train_data, test_data = train_test_split(eval_augmented_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "# last column\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_data)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "\n",
        "\n",
        "experiments.append(\"Augmented dataset\")\n",
        "test_percentages.append(test_size)\n",
        "train_percentages.append(1-test_size)\n",
        "accuracies.append(accuracy)\n",
        "precisions.append(precision)\n",
        "recalls.append(recall)\n"
      ],
      "id": "woXDJmn21s5Y"
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "XQiPojfz22mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a90abbc-9a66-47e3-87ff-3d245f22009a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "77/77 [==============================] - 2s 3ms/step - loss: 13.8751 - accuracy: 0.3490 - precision_18: 0.3490 - recall_18: 1.0000\n",
            "Epoch 2/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 3.8740 - accuracy: 0.4479 - precision_18: 0.3528 - recall_18: 0.6978\n",
            "Epoch 3/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 1.4502 - accuracy: 0.6042 - precision_18: 0.4366 - recall_18: 0.4627\n",
            "Epoch 4/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 1.1699 - accuracy: 0.6133 - precision_18: 0.4477 - recall_18: 0.4627\n",
            "Epoch 5/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.9976 - accuracy: 0.6016 - precision_18: 0.4358 - recall_18: 0.4813\n",
            "Epoch 6/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.8783 - accuracy: 0.6107 - precision_18: 0.4428 - recall_18: 0.4478\n",
            "Epoch 7/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.8164 - accuracy: 0.6107 - precision_18: 0.4428 - recall_18: 0.4478\n",
            "Epoch 8/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7979 - accuracy: 0.6003 - precision_18: 0.4264 - recall_18: 0.4216\n",
            "Epoch 9/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7618 - accuracy: 0.6172 - precision_18: 0.4529 - recall_18: 0.4664\n",
            "Epoch 10/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7392 - accuracy: 0.6107 - precision_18: 0.4411 - recall_18: 0.4328\n",
            "Epoch 11/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7267 - accuracy: 0.6081 - precision_18: 0.4387 - recall_18: 0.4403\n",
            "Epoch 12/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6979 - accuracy: 0.6081 - precision_18: 0.4377 - recall_18: 0.4328\n",
            "Epoch 13/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.6393 - precision_18: 0.4815 - recall_18: 0.4366\n",
            "Epoch 14/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.6289 - precision_18: 0.4656 - recall_18: 0.4291\n",
            "Epoch 15/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6552 - accuracy: 0.6536 - precision_18: 0.5043 - recall_18: 0.4328\n",
            "Epoch 16/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6506 - accuracy: 0.6445 - precision_18: 0.4897 - recall_18: 0.4440\n",
            "Epoch 17/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6369 - accuracy: 0.6484 - precision_18: 0.4958 - recall_18: 0.4403\n",
            "Epoch 18/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6327 - accuracy: 0.6523 - precision_18: 0.5024 - recall_18: 0.3918\n",
            "Epoch 19/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.6602 - precision_18: 0.5163 - recall_18: 0.4142\n",
            "Epoch 20/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6204 - accuracy: 0.6706 - precision_18: 0.5352 - recall_18: 0.4254\n",
            "Epoch 21/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6232 - accuracy: 0.6615 - precision_18: 0.5196 - recall_18: 0.3955\n",
            "Epoch 22/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6081 - accuracy: 0.6758 - precision_18: 0.5514 - recall_18: 0.3806\n",
            "Epoch 23/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6054 - accuracy: 0.6589 - precision_18: 0.5143 - recall_18: 0.4030\n",
            "Epoch 24/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5999 - accuracy: 0.6953 - precision_18: 0.5895 - recall_18: 0.4179\n",
            "Epoch 25/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6041 - accuracy: 0.6719 - precision_18: 0.5430 - recall_18: 0.3769\n",
            "Epoch 26/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5969 - accuracy: 0.6771 - precision_18: 0.5538 - recall_18: 0.3843\n",
            "Epoch 27/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5967 - accuracy: 0.6940 - precision_18: 0.5932 - recall_18: 0.3918\n",
            "Epoch 28/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5910 - accuracy: 0.6823 - precision_18: 0.5612 - recall_18: 0.4104\n",
            "Epoch 29/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5831 - accuracy: 0.6888 - precision_18: 0.5792 - recall_18: 0.3955\n",
            "Epoch 30/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5827 - accuracy: 0.6836 - precision_18: 0.5661 - recall_18: 0.3993\n",
            "Epoch 31/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5817 - accuracy: 0.6875 - precision_18: 0.5814 - recall_18: 0.3731\n",
            "Epoch 32/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.6953 - precision_18: 0.5914 - recall_18: 0.4104\n",
            "Epoch 33/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.7031 - precision_18: 0.6099 - recall_18: 0.4142\n",
            "Epoch 34/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.7057 - precision_18: 0.6117 - recall_18: 0.4291\n",
            "Epoch 35/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5827 - accuracy: 0.6888 - precision_18: 0.5751 - recall_18: 0.4142\n",
            "Epoch 36/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5705 - accuracy: 0.7031 - precision_18: 0.6124 - recall_18: 0.4067\n",
            "Epoch 37/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7161 - precision_18: 0.6543 - recall_18: 0.3955\n",
            "Epoch 38/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.6875 - precision_18: 0.5667 - recall_18: 0.4440\n",
            "Epoch 39/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.6927 - precision_18: 0.5889 - recall_18: 0.3955\n",
            "Epoch 40/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.7057 - precision_18: 0.6040 - recall_18: 0.4552\n",
            "Epoch 41/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7005 - precision_18: 0.6033 - recall_18: 0.4142\n",
            "Epoch 42/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.7070 - precision_18: 0.6201 - recall_18: 0.4142\n",
            "Epoch 43/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7122 - precision_18: 0.6298 - recall_18: 0.4254\n",
            "Epoch 44/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.7057 - precision_18: 0.6029 - recall_18: 0.4590\n",
            "Epoch 45/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5590 - accuracy: 0.7214 - precision_18: 0.6392 - recall_18: 0.4627\n",
            "Epoch 46/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5550 - accuracy: 0.7083 - precision_18: 0.6068 - recall_18: 0.4664\n",
            "Epoch 47/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5571 - accuracy: 0.7083 - precision_18: 0.6028 - recall_18: 0.4813\n",
            "Epoch 48/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5469 - accuracy: 0.7096 - precision_18: 0.6190 - recall_18: 0.4366\n",
            "Epoch 49/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5525 - accuracy: 0.7161 - precision_18: 0.6225 - recall_18: 0.4739\n",
            "Epoch 50/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.7109 - precision_18: 0.6173 - recall_18: 0.4515\n",
            "Epoch 51/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5551 - accuracy: 0.7083 - precision_18: 0.6122 - recall_18: 0.4478\n",
            "Epoch 52/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5480 - accuracy: 0.7174 - precision_18: 0.6321 - recall_18: 0.4552\n",
            "Epoch 53/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.7096 - precision_18: 0.6098 - recall_18: 0.4664\n",
            "Epoch 54/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5490 - accuracy: 0.7135 - precision_18: 0.6290 - recall_18: 0.4366\n",
            "Epoch 55/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5489 - accuracy: 0.7318 - precision_18: 0.6615 - recall_18: 0.4739\n",
            "Epoch 56/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7305 - precision_18: 0.6649 - recall_18: 0.4590\n",
            "Epoch 57/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5490 - accuracy: 0.7188 - precision_18: 0.6262 - recall_18: 0.4813\n",
            "Epoch 58/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.7161 - precision_18: 0.6190 - recall_18: 0.4851\n",
            "Epoch 59/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5519 - accuracy: 0.7122 - precision_18: 0.6181 - recall_18: 0.4590\n",
            "Epoch 60/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7344 - precision_18: 0.6569 - recall_18: 0.5000\n",
            "Epoch 61/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5375 - accuracy: 0.7109 - precision_18: 0.6150 - recall_18: 0.4590\n",
            "Epoch 62/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5398 - accuracy: 0.7500 - precision_18: 0.6900 - recall_18: 0.5149\n",
            "Epoch 63/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7109 - precision_18: 0.6117 - recall_18: 0.4701\n",
            "Epoch 64/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.7266 - precision_18: 0.6450 - recall_18: 0.4813\n",
            "Epoch 65/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7253 - precision_18: 0.6404 - recall_18: 0.4851\n",
            "Epoch 66/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7409 - precision_18: 0.6683 - recall_18: 0.5112\n",
            "Epoch 67/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7253 - precision_18: 0.6492 - recall_18: 0.4627\n",
            "Epoch 68/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5435 - accuracy: 0.7331 - precision_18: 0.6567 - recall_18: 0.4925\n",
            "Epoch 69/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5402 - accuracy: 0.7279 - precision_18: 0.6347 - recall_18: 0.5187\n",
            "Epoch 70/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.5320 - accuracy: 0.7318 - precision_18: 0.6582 - recall_18: 0.4813\n",
            "Epoch 71/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5304 - accuracy: 0.7240 - precision_18: 0.6429 - recall_18: 0.4701\n",
            "Epoch 72/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5409 - accuracy: 0.7188 - precision_18: 0.6313 - recall_18: 0.4664\n",
            "Epoch 73/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5289 - accuracy: 0.7344 - precision_18: 0.6600 - recall_18: 0.4925\n",
            "Epoch 74/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5284 - accuracy: 0.7305 - precision_18: 0.6580 - recall_18: 0.4739\n",
            "Epoch 75/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5311 - accuracy: 0.7292 - precision_18: 0.6402 - recall_18: 0.5112\n",
            "Epoch 76/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7383 - precision_18: 0.6683 - recall_18: 0.4963\n",
            "Epoch 77/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7461 - precision_18: 0.6995 - recall_18: 0.4776\n",
            "Epoch 78/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5288 - accuracy: 0.7318 - precision_18: 0.6490 - recall_18: 0.5037\n",
            "Epoch 79/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5249 - accuracy: 0.7474 - precision_18: 0.6832 - recall_18: 0.5149\n",
            "Epoch 80/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5299 - accuracy: 0.7292 - precision_18: 0.6415 - recall_18: 0.5075\n",
            "Epoch 81/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5306 - accuracy: 0.7344 - precision_18: 0.6524 - recall_18: 0.5112\n",
            "Epoch 82/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5237 - accuracy: 0.7409 - precision_18: 0.6635 - recall_18: 0.5224\n",
            "Epoch 83/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5436 - accuracy: 0.7253 - precision_18: 0.6390 - recall_18: 0.4888\n",
            "Epoch 84/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5206 - accuracy: 0.7422 - precision_18: 0.6750 - recall_18: 0.5037\n",
            "Epoch 85/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5259 - accuracy: 0.7109 - precision_18: 0.6036 - recall_18: 0.5000\n",
            "Epoch 86/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5227 - accuracy: 0.7435 - precision_18: 0.6749 - recall_18: 0.5112\n",
            "Epoch 87/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.7318 - precision_18: 0.6582 - recall_18: 0.4813\n",
            "Epoch 88/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.5244 - accuracy: 0.7357 - precision_18: 0.6526 - recall_18: 0.5187\n",
            "Epoch 89/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5241 - accuracy: 0.7292 - precision_18: 0.6546 - recall_18: 0.4739\n",
            "Epoch 90/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.7292 - precision_18: 0.6485 - recall_18: 0.4888\n",
            "Epoch 91/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7357 - precision_18: 0.6585 - recall_18: 0.5037\n",
            "Epoch 92/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7096 - precision_18: 0.6047 - recall_18: 0.4851\n",
            "Epoch 93/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7461 - precision_18: 0.6834 - recall_18: 0.5075\n",
            "Epoch 94/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7331 - precision_18: 0.6552 - recall_18: 0.4963\n",
            "Epoch 95/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7357 - precision_18: 0.6684 - recall_18: 0.4813\n",
            "Epoch 96/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7422 - precision_18: 0.6667 - recall_18: 0.5224\n",
            "Epoch 97/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7435 - precision_18: 0.6749 - recall_18: 0.5112\n",
            "Epoch 98/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5220 - accuracy: 0.7526 - precision_18: 0.6931 - recall_18: 0.5224\n",
            "Epoch 99/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7461 - precision_18: 0.6816 - recall_18: 0.5112\n",
            "Epoch 100/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7331 - precision_18: 0.6479 - recall_18: 0.5149\n",
            "Epoch 101/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7565 - precision_18: 0.7015 - recall_18: 0.5261\n",
            "Epoch 102/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5268 - accuracy: 0.7422 - precision_18: 0.6577 - recall_18: 0.5448\n",
            "Epoch 103/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7513 - precision_18: 0.6954 - recall_18: 0.5112\n",
            "Epoch 104/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7435 - precision_18: 0.6537 - recall_18: 0.5634\n",
            "Epoch 105/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7448 - precision_18: 0.7022 - recall_18: 0.4664\n",
            "Epoch 106/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7513 - precision_18: 0.6791 - recall_18: 0.5448\n",
            "Epoch 107/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7500 - precision_18: 0.6939 - recall_18: 0.5075\n",
            "Epoch 108/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7617 - precision_18: 0.6959 - recall_18: 0.5634\n",
            "Epoch 109/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7474 - precision_18: 0.6796 - recall_18: 0.5224\n",
            "Epoch 110/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7487 - precision_18: 0.6761 - recall_18: 0.5373\n",
            "Epoch 111/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.7461 - precision_18: 0.6995 - recall_18: 0.4776\n",
            "Epoch 112/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7474 - precision_18: 0.6581 - recall_18: 0.5746\n",
            "Epoch 113/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7461 - precision_18: 0.6973 - recall_18: 0.4813\n",
            "Epoch 114/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5077 - accuracy: 0.7409 - precision_18: 0.6700 - recall_18: 0.5075\n",
            "Epoch 115/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7409 - precision_18: 0.6865 - recall_18: 0.4739\n",
            "Epoch 116/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7435 - precision_18: 0.6636 - recall_18: 0.5373\n",
            "Epoch 117/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7409 - precision_18: 0.6620 - recall_18: 0.5261\n",
            "Epoch 118/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7396 - precision_18: 0.6683 - recall_18: 0.5037\n",
            "Epoch 119/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7409 - precision_18: 0.6683 - recall_18: 0.5112\n",
            "Epoch 120/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7474 - precision_18: 0.6745 - recall_18: 0.5336\n",
            "Epoch 121/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5057 - accuracy: 0.7344 - precision_18: 0.6616 - recall_18: 0.4888\n",
            "Epoch 122/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7422 - precision_18: 0.6716 - recall_18: 0.5112\n",
            "Epoch 123/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7448 - precision_18: 0.6667 - recall_18: 0.5373\n",
            "Epoch 124/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7422 - precision_18: 0.6786 - recall_18: 0.4963\n",
            "Epoch 125/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7396 - precision_18: 0.6735 - recall_18: 0.4925\n",
            "Epoch 126/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7526 - precision_18: 0.6970 - recall_18: 0.5149\n",
            "Epoch 127/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7513 - precision_18: 0.7016 - recall_18: 0.5000\n",
            "Epoch 128/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.7526 - precision_18: 0.6741 - recall_18: 0.5634\n",
            "Epoch 129/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7396 - precision_18: 0.6635 - recall_18: 0.5149\n",
            "Epoch 130/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.7487 - precision_18: 0.6866 - recall_18: 0.5149\n",
            "Epoch 131/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.7422 - precision_18: 0.6699 - recall_18: 0.5149\n",
            "Epoch 132/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7500 - precision_18: 0.6919 - recall_18: 0.5112\n",
            "Epoch 133/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.7513 - precision_18: 0.6954 - recall_18: 0.5112\n",
            "Epoch 134/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.7617 - precision_18: 0.7249 - recall_18: 0.5112\n",
            "Epoch 135/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7500 - precision_18: 0.6881 - recall_18: 0.5187\n",
            "Epoch 136/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.7513 - precision_18: 0.6791 - recall_18: 0.5448\n",
            "Epoch 137/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4990 - accuracy: 0.7461 - precision_18: 0.6872 - recall_18: 0.5000\n",
            "Epoch 138/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.7474 - precision_18: 0.6762 - recall_18: 0.5299\n",
            "Epoch 139/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7448 - precision_18: 0.6731 - recall_18: 0.5224\n",
            "Epoch 140/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7474 - precision_18: 0.6814 - recall_18: 0.5187\n",
            "Epoch 141/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4976 - accuracy: 0.7656 - precision_18: 0.7316 - recall_18: 0.5187\n",
            "Epoch 142/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.7669 - precision_18: 0.7236 - recall_18: 0.5373\n",
            "Epoch 143/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4956 - accuracy: 0.7448 - precision_18: 0.6714 - recall_18: 0.5261\n",
            "Epoch 144/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.7565 - precision_18: 0.6995 - recall_18: 0.5299\n",
            "Epoch 145/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.7474 - precision_18: 0.6850 - recall_18: 0.5112\n",
            "Epoch 146/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.7565 - precision_18: 0.7056 - recall_18: 0.5187\n",
            "Epoch 147/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7656 - precision_18: 0.7157 - recall_18: 0.5448\n",
            "Epoch 148/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4922 - accuracy: 0.7539 - precision_18: 0.6908 - recall_18: 0.5336\n",
            "Epoch 149/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4995 - accuracy: 0.7500 - precision_18: 0.6810 - recall_18: 0.5336\n",
            "Epoch 150/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.7552 - precision_18: 0.6869 - recall_18: 0.5485\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.9349 - precision_18: 0.9615 - recall_18: 0.8459\n",
            "Accuracy: 93.49%\n",
            "Precision: 96.15%\n",
            "Recall: 84.59%\n"
          ]
        }
      ],
      "source": [
        "test_size = 0.5\n",
        "train_data, test_data = train_test_split(eval_augmented_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "\n",
        "experiments.append(\"Augmented dataset\")\n",
        "test_percentages.append(test_size)\n",
        "train_percentages.append(1-test_size)\n",
        "accuracies.append(accuracy)\n",
        "precisions.append(precision)\n",
        "recalls.append(recall)\n"
      ],
      "id": "XQiPojfz22mt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experimental Results Summary"
      ],
      "metadata": {
        "id": "SYWi6swLnJY5"
      },
      "id": "SYWi6swLnJY5"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data for the table\n",
        "# Create a dictionary to store the data\n",
        "data = {\n",
        "    \"Train\": train_percentages,\n",
        "    \"Test\": test_percentages,\n",
        "    \"Accuracy\": accuracies,\n",
        "    \"Precision\": precisions,\n",
        "    \"Recall\": recalls\n",
        "}\n",
        "\n",
        "# Create a Pandas DataFrame from the dictionary\n",
        "df = pd.DataFrame(data, index=experiments)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQhn4FBOnNxd",
        "outputId": "091c3e72-b1ab-4e2a-943f-b227b3b50386"
      },
      "id": "vQhn4FBOnNxd",
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Train  Test  Accuracy  Precision    Recall\n",
            "Original dataset       0.8   0.2  0.727273   0.618182  0.618182\n",
            "Original dataset x2    0.8   0.2  0.733766   0.629630  0.618182\n",
            "Augmented dataset      0.8   0.2  0.996753   0.990566  1.000000\n",
            "Augmented dataset      0.5   0.5  0.934896   0.961538  0.845865\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}