{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fd33643"
      },
      "source": [
        "# Program Synthesis for Dataset Augmentation with Symbolic Regression and Genetic Programming"
      ],
      "id": "0fd33643"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2649232b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ba3c41-790a-44a4-a5c2-49da0bdaf630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gplearn in /usr/local/lib/python3.9/dist-packages (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "# Ruchi Bhalani, rb44675\n",
        "!pip install gplearn"
      ],
      "id": "2649232b"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8ZMR0CqzTxX",
        "outputId": "76e61bfe-3b6f-4663-8724-715164551f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "A8ZMR0CqzTxX"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ab1fe226"
      },
      "outputs": [],
      "source": [
        "# headers\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from collections import OrderedDict"
      ],
      "id": "ab1fe226"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "cc9c4481"
      },
      "outputs": [],
      "source": [
        "# function to calculate adjusted r2\n",
        "def get_adj_r2(r2, n, p):\n",
        "    return (1-(1-r2)*((n-1)/(n-p-1)))"
      ],
      "id": "cc9c4481"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7c7806"
      },
      "source": [
        "# Assignment 2: Regression and KNN classifier"
      ],
      "id": "7a7c7806"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bb47879"
      },
      "source": [
        "**Data Prep**\n"
      ],
      "id": "8bb47879"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "589c05de",
        "outputId": "112e841e-6349-4cec-a109-446d11b3873c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    int64  \n",
            " 2   BloodPressure             768 non-null    int64  \n",
            " 3   SkinThickness             768 non-null    int64  \n",
            " 4   Insulin                   768 non-null    int64  \n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n",
            "None\n",
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "763           10      101             76             48      180  32.9   \n",
            "764            2      122             70             27        0  36.8   \n",
            "765            5      121             72             23      112  26.2   \n",
            "766            1      126             60              0        0  30.1   \n",
            "767            1       93             70             31        0  30.4   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "763                     0.171   63        0  \n",
            "764                     0.340   27        0  \n",
            "765                     0.245   30        0  \n",
            "766                     0.349   47        1  \n",
            "767                     0.315   23        0  \n",
            "\n",
            "[768 rows x 9 columns]\n",
            "[]\n",
            "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
            "FINAL STEP: proper nouns -- []\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15ab0199-ad83-4ece-aef2-1eef98c38539\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15ab0199-ad83-4ece-aef2-1eef98c38539')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15ab0199-ad83-4ece-aef2-1eef98c38539 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15ab0199-ad83-4ece-aef2-1eef98c38539');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# NOTE: THESE VALUES ARE MEANT TO BE CUSTOMIZABLE BY THE USER\n",
        "# read in data\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/medical-charges.txt\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/breast_cancer.csv\"\n",
        "USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/diabetes.csv\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/disease.csv\"\n",
        "EXPECTED_DISTINCT_PERCENTAGE = 0.85\n",
        "\n",
        "# How many entries do we want to augment this dataset by?\n",
        "# TODO: Let the user pass this in as a percentage as well\n",
        "NEW_EXAMPLES = 10\n",
        "\n",
        "# data = pd.read_csv(USER_INPUT_DATASET, header = 0)\n",
        "df = pd.read_csv(USER_INPUT_DATASET, header=\"infer\")\n",
        "print(df.info())\n",
        "print(df)\n",
        "\n",
        "\n",
        "# DATA PREPROCESSING\n",
        "df = df.dropna(axis=1, how='all')\n",
        "# df = df.dropna()\n",
        "df = df.fillna(0)\n",
        "\n",
        "\n",
        "# Separate them into numerical, categorical, and proper noun columns (regex generation)\n",
        "numerical_cols = list()\n",
        "categorical_cols = list()\n",
        "proper_noun_cols = list()\n",
        "for (index, colname) in enumerate(df):\n",
        "    if colname in df.select_dtypes(include='object').columns:\n",
        "      unique_vals = df[colname].unique()\n",
        "      if len(unique_vals) >= EXPECTED_DISTINCT_PERCENTAGE * df.shape[0]:\n",
        "        proper_noun_cols.append(colname)\n",
        "      else:\n",
        "        categorical_cols.append(colname)\n",
        "    else:\n",
        "      numerical_cols.append(colname)\n",
        "\n",
        "\n",
        "print(categorical_cols)\n",
        "print(numerical_cols)\n",
        "print(\"FINAL STEP: proper nouns --\", proper_noun_cols)\n",
        "\n",
        "df.head()\n"
      ],
      "id": "589c05de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3043bb91"
      },
      "source": [
        "There are several categorical columns. We need to transform these to be able to do regression. "
      ],
      "id": "3043bb91"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0484464d",
        "outputId": "4faee9eb-0fd7-4198-f7db-c59a0f2003ee",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-896c3f0b-c583-4189-8d97-1beea326faa8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-896c3f0b-c583-4189-8d97-1beea326faa8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-896c3f0b-c583-4189-8d97-1beea326faa8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-896c3f0b-c583-4189-8d97-1beea326faa8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# we'll deal with the proper noun columns at the end\n",
        "regression_df = df.drop(columns=proper_noun_cols)\n",
        "regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "regression_df.head()"
      ],
      "id": "0484464d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38a13dc1"
      },
      "source": [
        "An interesting thing to check with regression problems is whether any of the individual features correlate very strongly with the label."
      ],
      "id": "38a13dc1"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad660fd",
        "outputId": "e251761c-3637-46a7-bd24-cdf0d6d786fa",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.129459       0.141282      -0.081672   \n",
            "Glucose                      0.129459  1.000000       0.152590       0.057328   \n",
            "BloodPressure                0.141282  0.152590       1.000000       0.207371   \n",
            "SkinThickness               -0.081672  0.057328       0.207371       1.000000   \n",
            "Insulin                     -0.073535  0.331357       0.088933       0.436783   \n",
            "BMI                          0.017683  0.221071       0.281805       0.392573   \n",
            "DiabetesPedigreeFunction    -0.033523  0.137337       0.041265       0.183928   \n",
            "Age                          0.544341  0.263514       0.239528      -0.113970   \n",
            "Outcome                      0.221898  0.466581       0.065068       0.074752   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.073535  0.017683                 -0.033523   \n",
            "Glucose                   0.331357  0.221071                  0.137337   \n",
            "BloodPressure             0.088933  0.281805                  0.041265   \n",
            "SkinThickness             0.436783  0.392573                  0.183928   \n",
            "Insulin                   1.000000  0.197859                  0.185071   \n",
            "BMI                       0.197859  1.000000                  0.140647   \n",
            "DiabetesPedigreeFunction  0.185071  0.140647                  1.000000   \n",
            "Age                      -0.042163  0.036242                  0.033561   \n",
            "Outcome                   0.130548  0.292695                  0.173844   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.544341  0.221898  \n",
            "Glucose                   0.263514  0.466581  \n",
            "BloodPressure             0.239528  0.065068  \n",
            "SkinThickness            -0.113970  0.074752  \n",
            "Insulin                  -0.042163  0.130548  \n",
            "BMI                       0.036242  0.292695  \n",
            "DiabetesPedigreeFunction  0.033561  0.173844  \n",
            "Age                       1.000000  0.238356  \n",
            "Outcome                   0.238356  1.000000   \n",
            "\n",
            "\n",
            "=== SIGNIFICANTLY CORRELATED COLUMNS ===\n",
            "[]\n",
            "OrderedDict()\n"
          ]
        }
      ],
      "source": [
        "print(regression_df.corr(), \"\\n\\n\")\n",
        "\n",
        "# Indexing with numbers on a numpy matrix will probably be faster\n",
        "\n",
        "print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "rows, cols = regression_df.shape\n",
        "corr = regression_df.corr().values\n",
        "fields = list(regression_df.columns)\n",
        "correlated_columns = list()\n",
        "nodes = []\n",
        "correlations = OrderedDict()\n",
        "\n",
        "# data structure that organizes correlations\n",
        "correlations_per_column = list()\n",
        "for i in range(cols):\n",
        "    for j in range(i+1, cols):\n",
        "      corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "      if corr[i,j] > 0.6:\n",
        "          print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "          correlated_columns.append([fields[i], fields[j], corr[i, j]])\n",
        "          correlations[corr[i, j]] = [fields[i], fields[j]]\n",
        "          if fields[i] not in nodes:\n",
        "            nodes.append(fields[i])\n",
        "          if fields[j] not in nodes:\n",
        "            nodes.append(fields[j])\n",
        "\n",
        "print(nodes)\n",
        "print(correlations)"
      ],
      "id": "6ad660fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l74DGBikGfow"
      },
      "source": [
        "We can visualize these attributes and their correlations as a weighted graph."
      ],
      "id": "l74DGBikGfow"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ggXpUuAfWqG2",
        "outputId": "193ec849-4561-4327-870d-be373282a64a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "for key,value in correlations.items():\n",
        "  G.add_edge(value[0], value[1], weight=key)\n",
        "\n",
        "\n",
        "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0.5]\n",
        "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.5]\n",
        "\n",
        "pos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n",
        "\n",
        "# nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=700)\n",
        "\n",
        "# edges\n",
        "nx.draw_networkx_edges(G, pos, edgelist=elarge, width=6)\n",
        "nx.draw_networkx_edges(\n",
        "    G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=\"b\", style=\"dashed\"\n",
        ")\n",
        "\n",
        "# node labels\n",
        "nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
        "# edge weight labels\n",
        "edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.margins(0.15)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ggXpUuAfWqG2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBaK36IxauAO"
      },
      "source": [
        "Now, let's organize them by pair and start generating values."
      ],
      "id": "lBaK36IxauAO"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkLcoIVuar-l",
        "outputId": "780639cf-55e2-4fec-d672-868be44e7414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict()\n"
          ]
        }
      ],
      "source": [
        "# weight all the correlations so that they're iterated through correctly\n",
        "weighted_correlations = OrderedDict()\n",
        "for key,value in correlations.items():\n",
        "  mult_factor = 1\n",
        "  if value[0] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  if value[1] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  weighted_correlations[mult_factor * key] = value\n",
        "\n",
        "weighted_correlations = OrderedDict(sorted(weighted_correlations.items(), reverse=True))\n",
        "print(weighted_correlations)\n"
      ],
      "id": "FkLcoIVuar-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numerical Data Generation: Symbolic Regression\n",
        "Now, let's augment the dataset with numerical data.\n",
        "\n",
        "The code is implementing a technique for data augmentation using symbolic regression, kernel density estimation, and K-nearest neighbor regression. The goal is to generate new samples of data that are similar to the original dataset, but not identical, to increase the size and diversity of the dataset for training machine learning models. The code takes in a pandas DataFrame df and the number of new samples to generate n_samples as inputs.\n",
        "\n",
        "The code first scales each column of the input dataset to the range [0, 1], fits a symbolic regressor to the scaled data, and generates new values using the regressor. It then applies kernel density estimation and K-nearest neighbor regression to the original data and generates new values using these methods. Finally, it performs a grid search to find the best kernel and bandwidth for the kernel density estimator, and generates new values using the best estimator.\n",
        "\n",
        "The new values generated by each method are clipped to the range of the original column data, and the new values for all columns are combined to create a new DataFrame new_df. The new_df is then concatenated with the original DataFrame df to create an augmented dataset, which is returned as output.\n",
        "\n"
      ],
      "metadata": {
        "id": "dJCaBCe3meJ0"
      },
      "id": "dJCaBCe3meJ0"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YqEGUD4oDrP",
        "outputId": "798eb776-0a4f-4a21-df98-02043f7643e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}\n",
            "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}\n",
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0    48.81          10071.7        7       0.00305286       0.00301773     15.02m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-212.16250948          -inf          -inf -133.46744026          -inf\n",
            " -281.12881055          -inf          -inf -234.94439958          -inf\n",
            " -335.32241593          -inf          -inf -287.66128371          -inf\n",
            " -365.75219178          -inf          -inf -317.71711134          -inf\n",
            " -375.91694359          -inf          -inf -336.11539337          -inf\n",
            " -378.37136908          -inf          -inf -348.22868624          -inf\n",
            " -379.38704625          -inf          -inf -356.76618794          -inf\n",
            " -380.37205545          -inf          -inf -363.15711045          -inf\n",
            " -381.48333254          -inf          -inf -368.19488025          -inf\n",
            " -382.69898589          -inf          -inf -372.34241894          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          335.372        7       0.00819325         0.008165     18.22m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-2205.95343925           -inf           -inf  -559.25572027\n",
            "           -inf -1052.41953892           -inf           -inf\n",
            "  -629.35077437           -inf  -880.01955654           -inf\n",
            "           -inf  -670.25407298           -inf  -829.48189416\n",
            "           -inf           -inf  -693.18518004           -inf\n",
            "  -800.65028245           -inf           -inf  -706.60941256\n",
            "           -inf  -781.69988178           -inf           -inf\n",
            "  -714.96223286           -inf  -769.74763495           -inf\n",
            "           -inf  -720.45759718           -inf  -762.09498762\n",
            "           -inf           -inf  -724.24488325           -inf\n",
            "  -756.97375804           -inf           -inf  -726.95636458\n",
            "           -inf  -753.39007971           -inf           -inf\n",
            "  -728.95984443           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          282.023        7       0.00763926       0.00761355      5.94m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-2445.03428884           -inf           -inf  -358.32805942\n",
            "           -inf  -946.47998163           -inf           -inf\n",
            "  -420.32378069           -inf  -711.26873186           -inf\n",
            "           -inf  -465.88752342           -inf  -649.1577841\n",
            "           -inf           -inf  -498.9893832            -inf\n",
            "  -631.66409828           -inf           -inf  -523.42788794\n",
            "           -inf  -629.32370997           -inf           -inf\n",
            "  -541.62281829           -inf  -631.0017599            -inf\n",
            "           -inf  -555.3095333            -inf  -631.98932878\n",
            "           -inf           -inf  -565.74609072           -inf\n",
            "  -631.3097818            -inf           -inf  -573.8286537\n",
            "           -inf  -629.57564525           -inf           -inf\n",
            "  -580.18945499           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          987.505        7       0.00278235       0.00292524      7.28m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-13495.19767892            -inf            -inf   -344.50510743\n",
            "            -inf  -3694.16418482            -inf            -inf\n",
            "   -402.35377832            -inf  -1920.79316416            -inf\n",
            "            -inf   -442.07866858            -inf  -1313.77990581\n",
            "            -inf            -inf   -467.64354722            -inf\n",
            "  -1033.1128268             -inf            -inf   -485.03174867\n",
            "            -inf   -880.98162067            -inf            -inf\n",
            "   -497.67865917            -inf   -790.76151053            -inf\n",
            "            -inf   -507.41697048            -inf   -733.6970289\n",
            "            -inf            -inf   -515.25722072            -inf\n",
            "   -695.77605486            -inf            -inf   -521.78785443\n",
            "            -inf   -669.61889804            -inf            -inf\n",
            "   -527.37133899            -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          6071.69        7       0.00125741       0.00139989      7.59m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-309589.17899775             -inf             -inf   -1566.17194355\n",
            "             -inf  -77720.40537206             -inf             -inf\n",
            "   -1016.10823309             -inf  -34824.02519881             -inf\n",
            "             -inf    -857.45327759             -inf  -19830.41047071\n",
            "             -inf             -inf    -788.7057134              -inf\n",
            "  -12900.74227397             -inf             -inf    -753.19806481\n",
            "             -inf   -9143.29642094             -inf             -inf\n",
            "    -733.11756746             -inf   -6882.66247141             -inf\n",
            "             -inf    -721.2209064              -inf   -5419.04564976\n",
            "             -inf             -inf    -714.06064143             -inf\n",
            "   -4418.30252909             -inf             -inf    -709.81166052\n",
            "             -inf   -3704.59787663             -inf             -inf\n",
            "    -707.43120698             -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          520.195        7       0.00641139       0.00657749      6.15m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-1262.16758172           -inf           -inf  -531.60986257\n",
            "           -inf  -701.68271431           -inf           -inf\n",
            "  -524.6201116            -inf  -598.08157738           -inf\n",
            "           -inf  -522.08982663           -inf  -562.26954159\n",
            "           -inf           -inf  -520.80582564           -inf\n",
            "  -545.83285564           -inf           -inf  -520.07313566\n",
            "           -inf  -536.96645631           -inf           -inf\n",
            "  -519.6355193            -inf  -531.70618285           -inf\n",
            "           -inf  -519.37394094           -inf  -528.39056976\n",
            "           -inf           -inf  -519.22595499           -inf\n",
            "  -526.2051062            -inf           -inf  -519.15644156\n",
            "           -inf  -524.71041988           -inf           -inf\n",
            "  -519.14467698           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81            33425        7       0.00224604       0.00245896      7.28m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ -10.97266936          -inf          -inf  -14.47760995          -inf\n",
            "  -28.45302491  -15.9240256   -10.01976593  -33.2008054    -8.55711879\n",
            "  -45.99796192  -29.39019944  -18.86317953  -51.81594996  -15.64128288\n",
            "  -63.69587154  -41.63165588  -27.9746282   -69.66101286  -23.31159305\n",
            "  -81.1537541   -52.94731945  -36.556336    -86.40319724  -30.69022234\n",
            "  -97.94722307  -65.19719414  -45.00726511 -101.9322133   -37.92936599\n",
            " -113.86677799  -78.505134    -53.76679594 -116.28702527  -45.33331512\n",
            " -128.84257618  -92.03010983  -62.82540399 -129.56786715  -52.93043257\n",
            " -142.87546522 -105.13576948  -72.06519569 -141.88902599  -60.66184121\n",
            " -156.00702594 -117.41326858  -81.21217257 -153.3596588   -68.34686653]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          29984.2        7       0.00275687       0.00269347      7.54m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-1612.51579683           -inf           -inf  -329.81974483\n",
            "           -inf  -758.98201829           -inf           -inf\n",
            "  -418.1532406            -inf  -642.26084751           -inf\n",
            "           -inf  -465.94522643           -inf  -612.16400627\n",
            "           -inf           -inf  -492.95834461           -inf\n",
            "  -593.48701589           -inf           -inf  -509.11341421\n",
            "           -inf  -579.95901377           -inf           -inf\n",
            "  -519.42832445           -inf  -571.17871268           -inf\n",
            "           -inf  -526.43809699           -inf  -565.62962012\n",
            "           -inf           -inf  -531.47316777           -inf\n",
            "  -562.06768498           -inf           -inf  -535.26798929\n",
            "           -inf  -559.74470965           -inf           -inf\n",
            "  -538.24897689           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          60.5031        7                0                0      6.03m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KernelDensity, KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from gplearn.genetic import SymbolicRegressor\n",
        "\n",
        "def augment_dataset(df, n_samples):\n",
        "    # Get column names and data types\n",
        "    col_names = df.columns.tolist()\n",
        "    dtypes = df.dtypes.tolist()\n",
        "\n",
        "    # Create a list to store new data samples\n",
        "    new_data = []\n",
        "\n",
        "    # Loop through each column and generate new values\n",
        "    for col_name, dtype in zip(col_names, dtypes):\n",
        "        # Get the data from the column\n",
        "        col_data = df[col_name].values\n",
        "\n",
        "        # Scale the column data to the range [0, 1]\n",
        "        scaler = MinMaxScaler()\n",
        "        col_data_scaled = scaler.fit_transform(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Fit a symbolic regressor to the column data\n",
        "        est_gp = SymbolicRegressor(population_size=5000, tournament_size=50,\n",
        "                                    generations=50, stopping_criteria=0.01,\n",
        "                                    p_crossover=0.7, p_subtree_mutation=0.1,\n",
        "                                    p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
        "                                    max_samples=0.9, verbose=1,\n",
        "                                    parsimony_coefficient=0.001, random_state=0, warm_start=True)\n",
        "        est_gp.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the symbolic regressor\n",
        "        new_col_data = est_gp.predict(np.random.rand(n_samples, 1))\n",
        "\n",
        "        # Reshape the new column data to be a 2D array\n",
        "        new_col_data = new_col_data.reshape(-1, 1)\n",
        "\n",
        "        # Scale the new column data using the scaler fitted to the original column data\n",
        "        new_col_data_scaled = scaler.transform(new_col_data)\n",
        "\n",
        "        # Inverse transform the scaled new column data to get the original scale\n",
        "        new_col_data = scaler.inverse_transform(new_col_data_scaled)\n",
        "\n",
        "        # Fit a kernel density estimator to the original column data\n",
        "        kde = KernelDensity(kernel='gaussian', bandwidth=0.1)\n",
        "        kde.fit(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Generate new values using the kernel density estimator\n",
        "        kde_samples = kde.sample(n_samples)\n",
        "        kde_samples = np.squeeze(kde_samples)\n",
        "\n",
        "        # Fit a KNN regressor to the original column data\n",
        "        knn = KNeighborsRegressor(n_neighbors=5)\n",
        "        knn.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the KNN regressor\n",
        "        knn_samples = knn.predict(np.random.rand(n_samples, 1))\n",
        "        knn_samples = np.squeeze(knn_samples)\n",
        "\n",
        "        # Use a grid search to find the best kernel and bandwidth for the kernel density estimator\n",
        "        params = {'kernel': ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear'],\n",
        "                  'bandwidth': np.linspace(0.1, 1.0, 10)}\n",
        "        grid = GridSearchCV(KernelDensity(), params, cv=5)\n",
        "        print(\"DATA SHAPE HERE: \", print(col_data.reshape(-1, 1).shape))\n",
        "        grid.fit(col_data.reshape(-1, 1))\n",
        "        # grid.fit(col_data)\n",
        "        kde_best = grid.best_estimator_\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        if kde_best.kernel in [\"gaussian\", \"tophat\"]:\n",
        "            kde_best_samples = kde_best.sample(n_samples)\n",
        "        else:\n",
        "            # Evaluate log density for a grid of points\n",
        "            x_grid = np.linspace(col_data.min(), col_data.max(), 1000).reshape(-1, 1)\n",
        "            log_dens = kde_best.score_samples(x_grid)\n",
        "\n",
        "            # Sample from the grid according to the log densities\n",
        "            probs = np.exp(log_dens - log_dens.max())\n",
        "            probs /= probs.sum()\n",
        "            kde_best_samples = np.random.choice(x_grid.flatten(), size=n_samples, p=probs)\n",
        "            \n",
        "        kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        # kde_best_samples = kde_best.sample(n_samples)\n",
        "        # kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Clip the new values to the range of the original column data\n",
        "        kde_best_samples = np.clip(kde_best_samples, np.min(col_data), np.max(col_data))\n",
        "\n",
        "        # Add new values to the list of new data\n",
        "        new_data.append(kde_best_samples.flatten())\n",
        "\n",
        "    # Transpose the list of new data to match the shape of the original dataset\n",
        "    new_data = np.array(new_data).T\n",
        "\n",
        "    # Convert the new data to a dataframe and append it to the original dataset\n",
        "    new_df = pd.DataFrame(new_data, columns=col_names)\n",
        "    augmented_df = pd.concat([df, new_df], ignore_index=True)\n",
        "\n",
        "    # return augmented_df\n",
        "    return new_df\n",
        "\n",
        "print(np.seterr())\n",
        "np.seterr(invalid='warn')\n",
        "np.seterr(under='warn')\n",
        "print(np.seterr())\n",
        "new_df = augment_dataset(regression_df, df.shape[0])"
      ],
      "id": "4YqEGUD4oDrP"
    },
    {
      "cell_type": "code",
      "source": [
        "# def reverse_one_hot_encode(df):\n",
        "#     # Get a list of all columns with '_'\n",
        "#     oh_cols = [col for col in df.columns if '_' in col]\n",
        "\n",
        "#     # Get the original column names and values for each one-hot encoded column\n",
        "#     col_vals = {}\n",
        "#     for col in oh_cols:\n",
        "#         col_name = col.split('_')[0]\n",
        "#         col_val = col.split('_')[1]\n",
        "#         if col_name not in col_vals:\n",
        "#             col_vals[col_name] = {}\n",
        "#         col_vals[col_name][col_val] = col\n",
        "\n",
        "#     # Create a new dataframe to store the reverse one-hot encoded values\n",
        "#     new_df = pd.DataFrame(columns=col_vals.keys())\n",
        "\n",
        "#     # Iterate through each row of the original dataframe\n",
        "#     for index, row in df.iterrows():\n",
        "#         # Initialize a dictionary to store the values for this row\n",
        "#         new_row = {}\n",
        "\n",
        "#         # Iterate through each original column\n",
        "#         for col_name in df.columns:\n",
        "#             # If this is a one-hot encoded column, find the closest value to 1 and use that\n",
        "#             if col_name in oh_cols:\n",
        "#                 col_val = col_name.split('_')[1]\n",
        "#                 col_key = col_vals[col_name.split('_')[0]][col_val]\n",
        "                \n",
        "#                 # Get the numeric values for the one-hot encoded column\n",
        "#                 numeric_values = pd.to_numeric(row[col_key], errors='coerce')\n",
        "#                 # Set values < 0.5 to 0 and values >= 0.5 to the numeric value\n",
        "#                 numeric_values = np.where(numeric_values >= 0.5, numeric_values, 0)\n",
        "#                 # Set the value closest to 1 to 1, and the rest to 0\n",
        "#                 numeric_values = np.where(numeric_values == np.max(numeric_values), 1, 0)\n",
        "#                 # Combine the column name and the value (if it's 1) and add it to the new_row dictionary\n",
        "#                 new_row[col_name.split('_')[0]] = col_val if np.sum(numeric_values) > 0 else np.nan\n",
        "#             # If this is not a one-hot encoded column, use the original value\n",
        "#             else:\n",
        "#                 new_row[col_name] = row[col_name]\n",
        "\n",
        "#         # Append the values for this row to the new dataframe\n",
        "#         new_df = new_df.append(new_row, ignore_index=True)\n",
        "\n",
        "#     # Return the new dataframe\n",
        "#     return new_df"
      ],
      "metadata": {
        "id": "er5lGyrRnbUt"
      },
      "id": "er5lGyrRnbUt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "x0qeIOSFMs8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75d0b34-308e-4d2b-8ca8-21bfadf59c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Old:  768\n",
            "New:  768\n",
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  0.000000 -0.824551      -0.635139      -0.857652   \n",
            "Glucose                     -0.824551  0.000000      -0.721007      -0.859591   \n",
            "BloodPressure               -0.635139 -0.721007       0.000000      -0.591221   \n",
            "SkinThickness               -0.857652 -0.859591      -0.591221       0.000000   \n",
            "Insulin                     -0.882690 -0.550096      -0.584159      -0.409961   \n",
            "BMI                         -0.913420 -0.768766      -0.612830      -0.529938   \n",
            "DiabetesPedigreeFunction    -0.941543 -0.787318      -0.712160      -0.708398   \n",
            "Age                         -0.450684 -0.679051      -0.503688      -0.800700   \n",
            "Outcome                     -0.652560 -0.356666      -0.555795      -0.727136   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.882690 -0.913420                 -0.941543   \n",
            "Glucose                  -0.550096 -0.768766                 -0.787318   \n",
            "BloodPressure            -0.584159 -0.612830                 -0.712160   \n",
            "SkinThickness            -0.409961 -0.529938                 -0.708398   \n",
            "Insulin                   0.000000 -0.666502                 -0.802731   \n",
            "BMI                      -0.666502  0.000000                 -0.775112   \n",
            "DiabetesPedigreeFunction -0.802731 -0.775112                  0.000000   \n",
            "Age                      -0.926053 -0.877163                 -0.946705   \n",
            "Outcome                  -0.663926 -0.481190                 -0.635608   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies              -0.450684 -0.652560  \n",
            "Glucose                  -0.679051 -0.356666  \n",
            "BloodPressure            -0.503688 -0.555795  \n",
            "SkinThickness            -0.800700 -0.727136  \n",
            "Insulin                  -0.926053 -0.663926  \n",
            "BMI                      -0.877163 -0.481190  \n",
            "DiabetesPedigreeFunction -0.946705 -0.635608  \n",
            "Age                       0.000000 -0.636698  \n",
            "Outcome                  -0.636698  0.000000  \n",
            "     Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
            "0       0.119119   81.074074      48.116116       0.000000    0.000000   \n",
            "1       4.067067  123.901902      74.006006      27.945946   88.072072   \n",
            "2       1.973974  102.985986      65.945946      16.648649    0.846847   \n",
            "3      10.941942  184.060060      92.080080      45.189189  326.882883   \n",
            "4       1.038038   96.014014      61.915916       0.099099    0.000000   \n",
            "..           ...         ...            ...            ...         ...   \n",
            "763     9.138138  170.912913      88.172172      41.126126  248.972973   \n",
            "764     2.229229  107.965966      68.632633      20.018018    4.234234   \n",
            "765     9.002002  166.132132      87.927928      40.927928  229.495495   \n",
            "766     9.036036  167.128128      88.050050      41.027027  232.036036   \n",
            "767     4.730731  125.893894      74.982983      29.036036   94.846847   \n",
            "\n",
            "           BMI  DiabetesPedigreeFunction        Age   Outcome  \n",
            "0    22.030831                  0.136609  21.720721  0.004004  \n",
            "1    33.650751                  0.474194  32.951952  0.070070  \n",
            "2    28.881882                  0.293680  25.804805  0.030030  \n",
            "3    45.270671                  1.182186  59.138138  0.993994  \n",
            "4    26.329530                  0.230382  23.882883  0.017017  \n",
            "..         ...                       ...        ...       ...  \n",
            "763  42.516817                  0.952440  52.951952  0.986987  \n",
            "764  30.359560                  0.338222  27.126126  0.039039  \n",
            "765  41.777978                  0.907898  51.270270  0.984985  \n",
            "766  41.979479                  0.919620  51.930931  0.985986  \n",
            "767  34.120921                  0.497638  34.033033  0.077077  \n",
            "\n",
            "[768 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# reprocess the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# just brings columns back to normal and replaces values with NaN\n",
        "def reverse_one_hot_encode(df):\n",
        "    oh_cols = [col for col in df.columns if \"_\" in col and col.split(\"_\")[0] in categorical_cols]\n",
        "    print(oh_cols)\n",
        "    new_df = df.drop(columns=oh_cols).copy()\n",
        "    for col in oh_cols:\n",
        "        col_name, val = col.split(\"_\")\n",
        "        mask = df[col] == 1\n",
        "        new_df[col_name] = np.nan\n",
        "        new_df.loc[mask, col_name] = val\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_significantly_correlated_colmns(df):\n",
        "  print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "  rows, cols = df.shape\n",
        "  corr = df.corr().values\n",
        "  fields = list(df.columns)\n",
        "  for i in range(cols):\n",
        "      for j in range(i+1, cols-1):\n",
        "        corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "        if corr[i,j] > 0.75:\n",
        "            print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "\n",
        "new_df = reverse_one_hot_encode(new_df)\n",
        "corr1 = regression_df.corr()\n",
        "corr2 = new_df.corr()\n",
        "# print a matrix to see the difference in correlations\n",
        "diff = np.abs(corr1) - np.abs(corr2)\n",
        "\n",
        "print(\"Old: \", len(regression_df))\n",
        "print(\"New: \", len(new_df))\n",
        "print(diff)\n",
        "\n",
        "# print(print_significantly_correlated_colmns(regression_df))\n",
        "# print(print_significantly_correlated_colmns(new_df))\n",
        "new_df.head()\n",
        "print(new_df)"
      ],
      "id": "x0qeIOSFMs8x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIh2Sn5ohWnz"
      },
      "source": [
        "## Categorical Variable Generation: Hill Climbing Algorithm\n",
        "In this implementation, we use a hill-climbing algorithm to search for the best categorical variable distribution that matches the original distribution and is correlated with the numerical variables in the generated data. The algorithm starts with the original distribution and iteratively perturbs it by swapping two values, then computes the correlation with the numerical variables and updates the distribution if the correlation improves. The algorithm repeats this process for a fixed number of iterations (in this case, 10), then moves on to the next categorical variable.\n",
        "\n",
        "This is an example of an explicit search or optimization technique for program synthesis, as we are searching for the best program (i.e., categorical variable distribution) that satisfies certain constraints (i.e., matching the original distribution and being correlated with the numerical variables)."
      ],
      "id": "cIh2Sn5ohWnz"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Hp-B6m8qSRXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47306dd1-8c64-4ded-df3a-28c4b87f4b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.129459       0.141282      -0.081672   \n",
            "Glucose                      0.129459  1.000000       0.152590       0.057328   \n",
            "BloodPressure                0.141282  0.152590       1.000000       0.207371   \n",
            "SkinThickness               -0.081672  0.057328       0.207371       1.000000   \n",
            "Insulin                     -0.073535  0.331357       0.088933       0.436783   \n",
            "BMI                          0.017683  0.221071       0.281805       0.392573   \n",
            "DiabetesPedigreeFunction    -0.033523  0.137337       0.041265       0.183928   \n",
            "Age                          0.544341  0.263514       0.239528      -0.113970   \n",
            "Outcome                      0.221898  0.466581       0.065068       0.074752   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.073535  0.017683                 -0.033523   \n",
            "Glucose                   0.331357  0.221071                  0.137337   \n",
            "BloodPressure             0.088933  0.281805                  0.041265   \n",
            "SkinThickness             0.436783  0.392573                  0.183928   \n",
            "Insulin                   1.000000  0.197859                  0.185071   \n",
            "BMI                       0.197859  1.000000                  0.140647   \n",
            "DiabetesPedigreeFunction  0.185071  0.140647                  1.000000   \n",
            "Age                      -0.042163  0.036242                  0.033561   \n",
            "Outcome                   0.130548  0.292695                  0.173844   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.544341  0.221898  \n",
            "Glucose                   0.263514  0.466581  \n",
            "BloodPressure             0.239528  0.065068  \n",
            "SkinThickness            -0.113970  0.074752  \n",
            "Insulin                  -0.042163  0.130548  \n",
            "BMI                       0.036242  0.292695  \n",
            "DiabetesPedigreeFunction  0.033561  0.173844  \n",
            "Age                       1.000000  0.238356  \n",
            "Outcome                   0.238356  1.000000  \n",
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.407731       0.354059       0.248327   \n",
            "Glucose                      0.407731  1.000000       0.403984       0.344517   \n",
            "BloodPressure                0.354059  0.403984       1.000000       0.403289   \n",
            "SkinThickness                0.248327  0.344517       0.403289       1.000000   \n",
            "Insulin                      0.271020  0.520677       0.289113       0.572506   \n",
            "BMI                          0.316300  0.482088       0.488181       0.563837   \n",
            "DiabetesPedigreeFunction     0.300471  0.407428       0.283742       0.416063   \n",
            "Age                          0.692784  0.495683       0.410407       0.221713   \n",
            "Outcome                      0.427271  0.582300       0.245082       0.301332   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies               0.271020  0.316300                  0.300471   \n",
            "Glucose                   0.520677  0.482088                  0.407428   \n",
            "BloodPressure             0.289113  0.488181                  0.283742   \n",
            "SkinThickness             0.572506  0.563837                  0.416063   \n",
            "Insulin                   1.000000  0.420642                  0.457183   \n",
            "BMI                       0.420642  1.000000                  0.397927   \n",
            "DiabetesPedigreeFunction  0.457183  0.397927                  1.000000   \n",
            "Age                       0.298877  0.326380                  0.350375   \n",
            "Outcome                   0.344417  0.444561                  0.376873   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.692784  0.427271  \n",
            "Glucose                   0.495683  0.582300  \n",
            "BloodPressure             0.410407  0.245082  \n",
            "SkinThickness             0.221713  0.301332  \n",
            "Insulin                   0.298877  0.344417  \n",
            "BMI                       0.326380  0.444561  \n",
            "DiabetesPedigreeFunction  0.350375  0.376873  \n",
            "Age                       1.000000  0.440934  \n",
            "Outcome                   0.440934  1.000000  \n",
            "1.0     282\n",
            "2.0     220\n",
            "0.0     178\n",
            "3.0     159\n",
            "4.0     134\n",
            "5.0     119\n",
            "6.0     104\n",
            "8.0      85\n",
            "7.0      75\n",
            "9.0      62\n",
            "10.0     49\n",
            "11.0     24\n",
            "13.0     22\n",
            "12.0     16\n",
            "15.0      3\n",
            "14.0      3\n",
            "17.0      1\n",
            "Name: Pregnancies, dtype: int64\n",
            "100.0    40\n",
            "99.0     37\n",
            "106.0    30\n",
            "95.0     30\n",
            "112.0    29\n",
            "         ..\n",
            "190.0     2\n",
            "178.0     1\n",
            "160.0     1\n",
            "67.0      1\n",
            "169.0     1\n",
            "Name: Glucose, Length: 136, dtype: int64\n",
            "70.0     118\n",
            "74.0     103\n",
            "78.0      91\n",
            "64.0      91\n",
            "72.0      89\n",
            "68.0      88\n",
            "80.0      81\n",
            "60.0      75\n",
            "0.0       74\n",
            "62.0      73\n",
            "76.0      73\n",
            "66.0      60\n",
            "88.0      52\n",
            "58.0      46\n",
            "82.0      46\n",
            "86.0      46\n",
            "84.0      45\n",
            "90.0      40\n",
            "50.0      25\n",
            "92.0      22\n",
            "54.0      22\n",
            "52.0      22\n",
            "56.0      19\n",
            "85.0      15\n",
            "75.0      14\n",
            "94.0      12\n",
            "48.0      11\n",
            "65.0      11\n",
            "96.0       8\n",
            "44.0       7\n",
            "110.0      5\n",
            "106.0      5\n",
            "100.0      5\n",
            "104.0      5\n",
            "98.0       5\n",
            "108.0      5\n",
            "30.0       4\n",
            "46.0       3\n",
            "102.0      3\n",
            "114.0      3\n",
            "40.0       2\n",
            "55.0       2\n",
            "24.0       2\n",
            "38.0       2\n",
            "79.0       2\n",
            "122.0      1\n",
            "95.0       1\n",
            "61.0       1\n",
            "69.0       1\n",
            "Name: BloodPressure, dtype: int64\n",
            "0.0     426\n",
            "32.0     64\n",
            "23.0     52\n",
            "30.0     48\n",
            "31.0     47\n",
            "27.0     44\n",
            "33.0     43\n",
            "18.0     41\n",
            "39.0     40\n",
            "28.0     39\n",
            "40.0     36\n",
            "25.0     35\n",
            "19.0     34\n",
            "29.0     34\n",
            "41.0     33\n",
            "37.0     32\n",
            "22.0     32\n",
            "26.0     31\n",
            "20.0     28\n",
            "17.0     27\n",
            "24.0     26\n",
            "13.0     25\n",
            "35.0     24\n",
            "15.0     24\n",
            "36.0     23\n",
            "21.0     22\n",
            "42.0     19\n",
            "45.0     18\n",
            "46.0     17\n",
            "16.0     17\n",
            "12.0     17\n",
            "34.0     16\n",
            "38.0     13\n",
            "14.0     13\n",
            "44.0     11\n",
            "43.0     10\n",
            "10.0     10\n",
            "11.0     10\n",
            "50.0      8\n",
            "7.0       8\n",
            "47.0      7\n",
            "48.0      7\n",
            "54.0      6\n",
            "49.0      5\n",
            "52.0      3\n",
            "63.0      3\n",
            "60.0      2\n",
            "51.0      2\n",
            "8.0       2\n",
            "56.0      1\n",
            "99.0      1\n",
            "Name: SkinThickness, dtype: int64\n",
            "0.0      564\n",
            "1.0       79\n",
            "2.0       33\n",
            "3.0       18\n",
            "105.0     15\n",
            "        ... \n",
            "510.0      1\n",
            "270.0      1\n",
            "178.0      1\n",
            "392.0      1\n",
            "232.0      1\n",
            "Name: Insulin, Length: 277, dtype: int64\n",
            "22.0    140\n",
            "21.0    115\n",
            "25.0    102\n",
            "24.0     98\n",
            "23.0     77\n",
            "28.0     66\n",
            "27.0     66\n",
            "26.0     65\n",
            "29.0     63\n",
            "30.0     47\n",
            "37.0     46\n",
            "41.0     45\n",
            "31.0     43\n",
            "32.0     34\n",
            "33.0     34\n",
            "38.0     32\n",
            "45.0     31\n",
            "42.0     29\n",
            "36.0     28\n",
            "46.0     27\n",
            "40.0     27\n",
            "34.0     24\n",
            "39.0     24\n",
            "43.0     22\n",
            "51.0     20\n",
            "35.0     20\n",
            "58.0     18\n",
            "50.0     17\n",
            "47.0     16\n",
            "52.0     15\n",
            "44.0     13\n",
            "53.0     11\n",
            "49.0     10\n",
            "54.0     10\n",
            "66.0      9\n",
            "62.0      9\n",
            "48.0      9\n",
            "57.0      9\n",
            "60.0      8\n",
            "55.0      7\n",
            "59.0      7\n",
            "63.0      7\n",
            "56.0      6\n",
            "61.0      5\n",
            "65.0      5\n",
            "67.0      5\n",
            "68.0      4\n",
            "69.0      3\n",
            "72.0      3\n",
            "64.0      2\n",
            "70.0      2\n",
            "81.0      1\n",
            "Name: Age, dtype: int64\n",
            "0.0    1002\n",
            "1.0     534\n",
            "Name: Outcome, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0          6.0    148.0           72.0           35.0      0.0  33.6   \n",
              "1          1.0     85.0           66.0           29.0      0.0  26.6   \n",
              "2          8.0    183.0           64.0            0.0      0.0  23.3   \n",
              "3          1.0     89.0           66.0           23.0     94.0  28.1   \n",
              "4          0.0    137.0           40.0           35.0    168.0  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction   Age  Outcome  \n",
              "0                     0.627  50.0      1.0  \n",
              "1                     0.351  31.0      0.0  \n",
              "2                     0.672  32.0      1.0  \n",
              "3                     0.167  21.0      0.0  \n",
              "4                     2.288  33.0      1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-342a6b1a-801e-4327-b03f-6065b3af2b82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-342a6b1a-801e-4327-b03f-6065b3af2b82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-342a6b1a-801e-4327-b03f-6065b3af2b82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-342a6b1a-801e-4327-b03f-6065b3af2b82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import random\n",
        "import copy\n",
        "\n",
        "def generate_categorical_variables(original_data, generated_data):\n",
        "    # Extract the original categorical variables\n",
        "    original_categorical_data = original_data.select_dtypes(include=['object'])\n",
        "    \n",
        "    # Extract the generated numerical variables\n",
        "    generated_numerical_data = generated_data.select_dtypes(include=['int64', 'float64'])\n",
        "    \n",
        "    # Generate new categorical variables for the generated data using hill-climbing algorithm\n",
        "    new_categorical_data = copy.deepcopy(original_categorical_data)\n",
        "    for col in new_categorical_data.columns:\n",
        "        current_distribution = original_categorical_data[col].value_counts(normalize=True)\n",
        "        for _ in range(10):\n",
        "            # Perturb the distribution by swapping two values\n",
        "            i, j = random.sample(range(len(current_distribution)), 2)\n",
        "            new_distribution = current_distribution.copy()\n",
        "            new_distribution[i], new_distribution[j] = new_distribution[j], new_distribution[i]\n",
        "            new_distribution /= new_distribution.sum()\n",
        "\n",
        "            # Compute the correlation with the numerical variables\n",
        "            new_correlation = abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))\n",
        "            current_correlation=abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))*-1\n",
        "            # Update the categorical variable if the correlation improves\n",
        "            if new_correlation.sum() > current_correlation.sum():\n",
        "                current_correlation = new_correlation\n",
        "                new_categorical_data[col] = np.random.choice(new_distribution.index, size=len(new_categorical_data), p=new_distribution.values)\n",
        "    \n",
        "    # Replace the NaNs in the generated data with the new categorical data\n",
        "    generated_data.update(new_categorical_data)\n",
        "    \n",
        "    return generated_data\n",
        "\n",
        "\n",
        "final_df = generate_categorical_variables(df, new_df)\n",
        "# final_df.head()\n",
        "\n",
        "final_df = pd.concat([df, final_df], axis=0)\n",
        "final_df_normalized = pd.get_dummies(final_df, columns=categorical_cols)\n",
        "complete_encoded = pd.concat([final_df_normalized, regression_df], axis=0)\n",
        "print(regression_df.corr())\n",
        "print(complete_encoded.corr())\n",
        "\n",
        "# make sure that all columns are rounded appropriately\n",
        "for col in final_df.columns:\n",
        "  if col in numerical_cols:\n",
        "    # print(col)\n",
        "    # all(print(x) for x in col)\n",
        "    if all((x*1.0).is_integer() for x in df[col]):\n",
        "      final_df[col] = np.round(final_df[col], 0)\n",
        "      # print(final_df[col])\n",
        "      print(final_df[col].value_counts())\n",
        "\n",
        "# print(final_df)\n",
        "final_df.head()\n"
      ],
      "id": "Hp-B6m8qSRXx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVkqlquMVDX"
      },
      "source": [
        "Let's view both distributions to evaluate the quality of these generated values."
      ],
      "id": "LkVkqlquMVDX"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "0o2tCRcLOTao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "b777195d-30ac-435d-fcb4-5357b49f4ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The distributions are not similar.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAHDCAYAAADWY9A/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACALklEQVR4nO3de1yUZf7/8ddwVhQQUBBFMU9oknhE7FwUph0oK3Uty/zp1mZrstuaW1lt29pJO+nm17bz5mq25prrukt0TvIAkkfUSsUTeEBBQY5z//64ZRRFAx24Z4b38/GYx9zOXHPP+yaai8/c131dNsMwDERERERERMQpvKwOICIiIiIi4klUZImIiIiIiDiRiiwREREREREnUpElIiIiIiLiRCqyREREREREnEhFloiIiIiIiBOpyBIREREREXEiFVkiIiIiIiJOpCJLRERERETEiVRkiVhkx44d2Gw23n33XaujiIiIAOqbRJxFRZZIA3j33Xex2WysWbPG6fv+61//Wq/Ob8GCBdx111107doVm83GVVdd5fRMIiLi+lylbzp06BAvvvgiV1xxBa1btyYkJIRBgwaxYMECp+cSsYqP1QFEmqqOHTty/PhxfH196/W6v/71r4SHh3PvvffWqf0bb7xBZmYmAwYM4NChQ+eRVEREmorG6JsyMjJ47LHHGDp0KI8//jg+Pj7885//ZOTIkWzatImnn376PNOLuA4VWSIWsdlsBAQENPj7fPDBB7Rr1w4vLy969erV4O8nIiLuqzH6posvvpht27bRsWNHx2O/+c1vSEpK4vnnn+cPf/gDgYGBDZpBpKFpuKCIRWob956Xl8fYsWNp3749/v7+tG3blltuuYUdO3YAEBMTw8aNG/nqq6+w2Wx1Gv4XHR2Nl5f+VxcRkV/WGH1Tp06dahRYYBZ3KSkplJWV8fPPPzfAkYk0Lp3JEnEhw4cPZ+PGjTz00EPExMSwf/9+0tLSyM3NJSYmhldeeYWHHnqIFi1a8NhjjwEQERFhcWoREfFkjdU35eXlARAeHu7U/CJWUJEl4iKOHDnCihUrePHFF/n973/veHzq1KmO7ZSUFB5//HHCw8O56667rIgpIiJNSGP1TQUFBfztb3/j8ssvp23bthecW8RqGkMk4iKaNWuGn58fX375JYcPH7Y6joiISKP0TXa7ndGjR3PkyBFef/31BnkPkcamIkvERfj7+/P888/zn//8h4iICK644gpeeOEFx/AJERGRxtYYfdNDDz3E8uXL+dvf/kbv3r2dtl8RK6nIEnEhDz/8MFu3bmX69OkEBATwxBNP0KNHD9auXWt1NBERaaIasm96+umn+etf/8pzzz3H3Xff7YS0Iq5BRZaIi+ncuTO/+93v+N///seGDRsoLy9nxowZjudtNpuF6UREpClqiL5p9uzZPPXUUzz88MNMmTLFmXFFLKciS8RFlJSUUFpaWuOxzp0707JlS8rKyhyPBQYGcuTIkUZOJyIiTVFD9U0LFizgt7/9LaNHj2bmzJnOiiviMjS7oEgDevvtt1m+fPkZj0+aNOmMx7Zu3cq1117LnXfeSc+ePfHx8eGTTz4hPz+fkSNHOtr169ePN954gz//+c906dKFNm3acM0115w1w9dff83XX38NwIEDByguLubPf/4zAFdccQVXXHHFhR6miIi4Eav7plWrVjFmzBjCwsK49tpr+fDDD2s8P3jwYC666KILPEoRa6nIEmlAb7zxRq2P33vvvWc8Fh0dzahRo0hPT+eDDz7Ax8eH2NhYPvroI4YPH+5oN23aNHbu3MkLL7zA0aNHufLKK89ZZH3++ec8/fTTNR574oknAHjyySdVZImINDFW902bNm2ivLycAwcOcN99953x/DvvvKMiS9yezTAMw+oQIiIiIiIinkLXZImIiIiIiDiRiiwREREREREnUpElIiIiIiLiRCqyREREREREnEhFloiIiIiIiBOpyBIREREREXGiJrNOlt1uZ+/evbRs2RKbzWZ1HBGRJsMwDI4ePUpUVBReXvpu71Tqm0RErNHQfVOTKbL27t1LdHS01TFERJqsXbt20b59e6tjuBT1TSIi1mqovqnJFFktW7YEzB9kUFCQxWlERJqOoqIioqOjHZ/DcpL6JhERazR039RkiqzqYRhBQUHqyERELKDhcGdS3yQiYq2G6ps0OF5ERERERMSJVGSJiIiIiIg4kYosERERERERJ1KRJSIiIiIi4kQqskRERERERJxIRZaIiIiIiIgTqcgSERERERFxIhVZIiIiIiIiTqQiS0RERERExIlUZImIiIiIiDiRiiwREREREREnUpElIiIiIiLiRCqyREREREREnEhFloiIiIiIiBOpyBIREREREXEiFVkiIiIiIiJO5GN1ABGpm5fTtlr6/pOv62bp+4uIiJxOfaO4Kp3JEhERERERcSIVWSIiIiIiIk6kIktERERERMSJVGSJiIiIiIg4kYosERERERERJ1KRJSIiIiIi4kQqskRERERERJxIRZaIiIiIiIgTqcgSERERERFxIhVZIiIiIiIiTqQiS0RERERExIlUZImIiIiIiDiRiiwREREREREnUpElIiIeYfbs2cTExBAQEEBCQgKrVq06Z/uFCxcSGxtLQEAAcXFxLFu2rMbzTz31FLGxsQQGBtKqVSuSkpJYuXJljTYFBQWMHj2aoKAgQkJCGDduHMeOHXP6sYmIiHtRkSUiIm5vwYIFpKam8uSTT5KVlUXv3r1JTk5m//79tbZfsWIFo0aNYty4caxdu5aUlBRSUlLYsGGDo023bt2YNWsW69ev59tvvyUmJobrr7+eAwcOONqMHj2ajRs3kpaWxtKlS/n666+ZMGFCgx+viIi4NpthGIbVIRpDUVERwcHBFBYWEhQUZHUckXp7OW2rpe8/+bpulr6/uK/G+PxNSEhgwIABzJo1CwC73U50dDQPPfQQjz766BntR4wYQXFxMUuXLnU8NmjQIOLj45kzZ845j+Ozzz7j2muvZfPmzfTs2ZPVq1fTv39/AJYvX87QoUPZvXs3UVFRv5hbfZPIhVHfKOeroT9/dSZLRETcWnl5OZmZmSQlJTke8/LyIikpiYyMjFpfk5GRUaM9QHJy8lnbl5eXM3fuXIKDg+ndu7djHyEhIY4CCyApKQkvL68zhhWKiEjT4mN1ABERkQtx8OBBqqqqiIiIqPF4REQEOTk5tb4mLy+v1vZ5eXk1Hlu6dCkjR46kpKSEtm3bkpaWRnh4uGMfbdq0qdHex8eH0NDQM/ZTraysjLKyMse/i4qK6naQIiLiVnQmS0RE5CyuvvpqsrOzWbFiBUOGDOHOO+8863VedTF9+nSCg4Mdt+joaCemFRERV6EiS0RE3Fp4eDje3t7k5+fXeDw/P5/IyMhaXxMZGVmn9oGBgXTp0oVBgwbx1ltv4ePjw1tvveXYx+kFV2VlJQUFBWd936lTp1JYWOi47dq1q17HKiIi7kFFloiIuDU/Pz/69etHenq64zG73U56ejqJiYm1viYxMbFGe4C0tLSztj91v9XD/RITEzly5AiZmZmO5z///HPsdjsJCQm1vt7f35+goKAaNxER8Ty6JktERNxeamoq99xzD/3792fgwIG88sorFBcXM3bsWADGjBlDu3btmD59OgCTJk3iyiuvZMaMGQwbNoz58+ezZs0a5s6dC0BxcTHPPvssN998M23btuXgwYPMnj2bPXv2cMcddwDQo0cPhgwZwvjx45kzZw4VFRVMnDiRkSNH1mlmQRER8VwqskRExO2NGDGCAwcOMG3aNPLy8oiPj2f58uWOyS1yc3Px8jo5eGPw4MHMmzePxx9/nD/+8Y907dqVxYsX06tXLwC8vb3Jycnhvffe4+DBg4SFhTFgwAC++eYbLr74Ysd+PvzwQyZOnMi1116Ll5cXw4cP57XXXmvcgxcREZejdbJE3ITWAhF3pc/fs9PPRuTCqG+U89XQn786kyUidaKOTERERKRuNPGFiIiIiIiIE6nIEhERERERcSIVWSIiIiIiIk6kIktERERERMSJVGSJiIiIiIg4kYosERERERERJ1KRJSIiIiIi4kTnVWTNnj2bmJgYAgICSEhIYNWqVedsv3DhQmJjYwkICCAuLo5ly5Y5nquoqGDKlCnExcURGBhIVFQUY8aMYe/evTX2ERMTg81mq3F77rnnzie+iIiIiIhIg6l3kbVgwQJSU1N58sknycrKonfv3iQnJ7N///5a269YsYJRo0Yxbtw41q5dS0pKCikpKWzYsAGAkpISsrKyeOKJJ8jKymLRokVs2bKFm2+++Yx9/elPf2Lfvn2O20MPPVTf+CIiIiIiIg2q3kXWzJkzGT9+PGPHjqVnz57MmTOH5s2b8/bbb9fa/tVXX2XIkCE88sgj9OjRg2eeeYa+ffsya9YsAIKDg0lLS+POO++ke/fuDBo0iFmzZpGZmUlubm6NfbVs2ZLIyEjHLTAw8DwOWUREREREpOHUq8gqLy8nMzOTpKSkkzvw8iIpKYmMjIxaX5ORkVGjPUBycvJZ2wMUFhZis9kICQmp8fhzzz1HWFgYffr04cUXX6SysvKs+ygrK6OoqKjGTUREREREpKH51KfxwYMHqaqqIiIiosbjERER5OTk1PqavLy8Wtvn5eXV2r60tJQpU6YwatQogoKCHI//9re/pW/fvoSGhrJixQqmTp3Kvn37mDlzZq37mT59Ok8//XR9Dk9EREREROSC1avIamgVFRXceeedGIbBG2+8UeO51NRUx/Yll1yCn58fv/71r5k+fTr+/v5n7Gvq1Kk1XlNUVER0dHTDhRcREREREaGeRVZ4eDje3t7k5+fXeDw/P5/IyMhaXxMZGVmn9tUF1s6dO/n8889rnMWqTUJCApWVlezYsYPu3buf8by/v3+txZeIiIiIiEhDqtc1WX5+fvTr14/09HTHY3a7nfT0dBITE2t9TWJiYo32AGlpaTXaVxdY27Zt47PPPiMsLOwXs2RnZ+Pl5UWbNm3qcwgiIiIiIiINqt7DBVNTU7nnnnvo378/AwcO5JVXXqG4uJixY8cCMGbMGNq1a8f06dMBmDRpEldeeSUzZsxg2LBhzJ8/nzVr1jB37lzALLBuv/12srKyWLp0KVVVVY7rtUJDQ/Hz8yMjI4OVK1dy9dVX07JlSzIyMpg8eTJ33XUXrVq1ctbPQkRERERE5ILVu8gaMWIEBw4cYNq0aeTl5REfH8/y5csdk1vk5ubi5XXyBNngwYOZN28ejz/+OH/84x/p2rUrixcvplevXgDs2bOHJUuWABAfH1/jvb744guuuuoq/P39mT9/Pk899RRlZWV06tSJyZMn17jmSkRERERExBXYDMMwrA7RGIqKiggODqawsPAXr/cScUUvp221OoKlJl/XzeoIcp70+Xt2+tmIXBir+0b1Te6roT9/670YsYiIiIiIiJydiiwREREREREnUpElIiIiIiLiRCqyREREREREnEhFloiIiIiIiBOpyBIREREREXEiFVkiIiIiIiJOpCJLRERERETEiVRkiYiIiIiIOJGKLBERERERESdSkSUiIiIiIuJEKrJEREREREScSEWWiIiIiIiIE6nIEhERERERcSIVWSIiIiIiIk6kIktERERERMSJVGSJiIiIiIg4kYosERERERERJ1KRJSIiIiIi4kQqskRERERERJxIRZaIiIiIiIgTqcgSERERERFxIhVZIiIiIiIiTuRjdQARERERcU8vp221OoKIS9KZLBERERERESdSkSUiIiIiIuJEKrJEREREREScSEWWiIiIiIiIE6nIEhERjzB79mxiYmIICAggISGBVatWnbP9woULiY2NJSAggLi4OJYtW+Z4rqKigilTphAXF0dgYCBRUVGMGTOGvXv31thHTEwMNputxu25555rkOMTERH3oSJLRETc3oIFC0hNTeXJJ58kKyuL3r17k5yczP79+2ttv2LFCkaNGsW4ceNYu3YtKSkppKSksGHDBgBKSkrIysriiSeeICsri0WLFrFlyxZuvvnmM/b1pz/9iX379jluDz30UIMeq4iIuD4VWSIi4vZmzpzJ+PHjGTt2LD179mTOnDk0b96ct99+u9b2r776KkOGDOGRRx6hR48ePPPMM/Tt25dZs2YBEBwcTFpaGnfeeSfdu3dn0KBBzJo1i8zMTHJzc2vsq2XLlkRGRjpugYGBDX68IiLi2lRkiYiIWysvLyczM5OkpCTHY15eXiQlJZGRkVHrazIyMmq0B0hOTj5re4DCwkJsNhshISE1Hn/uuecICwujT58+vPjii1RWVp7/wYiIiEfQYsQiIuLWDh48SFVVFRERETUej4iIICcnp9bX5OXl1do+Ly+v1valpaVMmTKFUaNGERQU5Hj8t7/9LX379iU0NJQVK1YwdepU9u3bx8yZM2vdT1lZGWVlZY5/FxUV1ekYRUTEvajIEhEROYeKigruvPNODMPgjTfeqPFcamqqY/uSSy7Bz8+PX//610yfPh1/f/8z9jV9+nSefvrpBs8sIiLW0nBBERFxa+Hh4Xh7e5Ofn1/j8fz8fCIjI2t9TWRkZJ3aVxdYO3fuJC0trcZZrNokJCRQWVnJjh07an1+6tSpFBYWOm67du36haMTERF3pCJLRETcmp+fH/369SM9Pd3xmN1uJz09ncTExFpfk5iYWKM9QFpaWo321QXWtm3b+OyzzwgLC/vFLNnZ2Xh5edGmTZtan/f39ycoKKjGTUREPI+GC4qIiNtLTU3lnnvuoX///gwcOJBXXnmF4uJixo4dC8CYMWNo164d06dPB2DSpElceeWVzJgxg2HDhjF//nzWrFnD3LlzAbPAuv3228nKymLp0qVUVVU5rtcKDQ3Fz8+PjIwMVq5cydVXX03Lli3JyMhg8uTJ3HXXXbRq1cqaH4SIiLgEFVkiIuL2RowYwYEDB5g2bRp5eXnEx8ezfPlyx+QWubm5eHmdHLwxePBg5s2bx+OPP84f//hHunbtyuLFi+nVqxcAe/bsYcmSJQDEx8fXeK8vvviCq666Cn9/f+bPn89TTz1FWVkZnTp1YvLkyTWu0xIRkabJZhiGYXWIxlBUVERwcDCFhYUaniFu6eW0rVZHsNTk67pZHUHOkz5/z04/G3F36pvUN7mrhv781TVZIiIiIiIiTqQiS0RERERExIlUZImIiIiIiDiRiiwREREREREnUpElIiIiIiLiRCqyREREREREnEhFloiIiIiIiBOpyBIREREREXEiFVkiIiIiIiJOpCJLRERERETEiVRkiTRRzcoLiDi6keblh8AwrI4jIiIi4jF8rA4gIo3DZlTSc/+/6XzoK9oUb6Fl+X7Hc2XegRwJiGZreBLrIodT7tPCwqQiIiIi7k1FloinMwy6HvqcwTv/Smhp7smHsVHiG0rzigL8q4qJKM4hojiHAbvfY23USNZGjaTMJ8jC4CIiIiLuSUWWiAcLLNvPjVseJeroegBKfFuRFTWKPUF9OBDYjQrv5njbywku3UPbo+vov+cDQo/vJHHXm8TlL2ZJ7Ivkt7zY4qMQERERcS8qskQ8VHjxNlI2PUzL8v2UezUns91oMqNGU+ETWKNdlZcfBc07UdC8E5va3EiXQ18weOcbhJbmcuf6CaR1eYycNkMtOgoRERER96MiS8QDdTycwbAtU/GvKuZQsxgW93yFooB2v/g6w+bNtvAkdoYMYsjWaXQ+/A03bHuS1iU/8k3Hh8Bma4T0IiIiIu7tvGYXnD17NjExMQQEBJCQkMCqVavO2X7hwoXExsYSEBBAXFwcy5YtczxXUVHBlClTiIuLIzAwkKioKMaMGcPevXtr7KOgoIDRo0cTFBRESEgI48aN49ixY+cTX8SjXVTwNSmbJuNfVcyuoH4suOStOhVYpyr3acGSHi+xsv19APTf8wGJuf/XEHFFREREPE69i6wFCxaQmprKk08+SVZWFr179yY5OZn9+/fX2n7FihWMGjWKcePGsXbtWlJSUkhJSWHDhg0AlJSUkJWVxRNPPEFWVhaLFi1iy5Yt3HzzzTX2M3r0aDZu3EhaWhpLly7l66+/ZsKECedxyCKeK7x4KzdseRwvqsgJT+aTi187/8krbF6s6PgAn3WeCsCg3W/RK2+x88KKiIiIeCibYdRvgZyEhAQGDBjArFmzALDb7URHR/PQQw/x6KOPntF+xIgRFBcXs3TpUsdjgwYNIj4+njlz5tT6HqtXr2bgwIHs3LmTDh06sHnzZnr27Mnq1avp378/AMuXL2fo0KHs3r2bqKioX8xdVFREcHAwhYWFBAVpxjRxPy+nbT3n883LDzLqh3sJKs8nN3gAn/R8DbuXc0YED975Bgm738aON//qOYMdrS51yn7rY/J13Rr9PcU59Pl7dvrZiLv7pb7J06lvcl8N/flbrzNZ5eXlZGZmkpSUdHIHXl4kJSWRkZFR62syMjJqtAdITk4+a3uAwsJCbDYbISEhjn2EhIQ4CiyApKQkvLy8WLlyZa37KCsro6ioqMZNxFN5V5Vy8+bfE1SeT0GzjiyNfc5pBRbAig73s6n1ULyoYljOVMKKf3TavkVEREQ8Tb2KrIMHD1JVVUVERESNxyMiIsjLy6v1NXl5efVqX1paypQpUxg1apSjqszLy6NNmzY12vn4+BAaGnrW/UyfPp3g4GDHLTo6uk7HKOKOrt4+g7bHNnLcJ5jFPV52/vpWNhtpXR4nN7g/fvbjDNk2DS97hXPfQ0RERMRDnNfEFw2loqKCO++8E8MweOONNy5oX1OnTqWwsNBx27Vrl5NSiriWjocziMtfjIGNf3efTmGzhvlCwe7ly3+6PcNxn2DaFG8jcdfcBnkfEREREXdXryIrPDwcb29v8vPzazyen59PZGRkra+JjIysU/vqAmvnzp2kpaXVGBsZGRl5xsQalZWVFBQUnPV9/f39CQoKqnET8TR+lce47sc/A7C27Qh2hQxo0Pcr8Qvnsy5/BKD/7veJKvqhQd9PRERExB3Vq8jy8/OjX79+pKenOx6z2+2kp6eTmJhY62sSExNrtAdIS0ur0b66wNq2bRufffYZYWFhZ+zjyJEjZGZmOh77/PPPsdvtJCQk1OcQRDzKldtfpmX5fg4HRPNdxwcb5T1/DLuGTa2H4YWd5G1P4ltZ3CjvKyIiIuIu6j1cMDU1lTfffJP33nuPzZs388ADD1BcXMzYsWMBGDNmDFOnTnW0nzRpEsuXL2fGjBnk5OTw1FNPsWbNGiZOnAiYBdbtt9/OmjVr+PDDD6mqqiIvL4+8vDzKy8sB6NGjB0OGDGH8+PGsWrWK7777jokTJzJy5Mg6zSwo4oliDn9Hr/1LMLDxv67TqPQOaLT3/uKi31PkH0lI6R4uzb2wob0iIiIinqbeRdaIESN46aWXmDZtGvHx8WRnZ7N8+XLH5Ba5ubns27fP0X7w4MHMmzePuXPn0rt3bz7++GMWL15Mr169ANizZw9Llixh9+7dxMfH07ZtW8dtxYoVjv18+OGHxMbGcu211zJ06FAuu+wy5s7VNSHSNHlXlXLNT88DkBU1ir1B8Y36/uU+LUjr8jgAvfd9TFjJT436/iIiIiKurN7rZLkrrUUi7u7UtUgG7H6Xy3bO5qhfG97t+89GPYt1qps2P0KXgi/ZETKIT3q+BjZbg72X1iJxX/r8PTv9bMTdaZ0s9U3uyqXWyRIR6zUrL2DA7ncB+Lbjg5YVWABfx0yi0uZLzJHv6XT4W8tyiIiIiLgSFVkibiZx11z8q4rJa9GDnNZDLM1S2Kw9a6NGAXDl9le0dpaIiIgIKrJE3EpYyU/E5X0CwNcxk8Fm/f/Cq9qPpdg3lFalucTv+8jqOCIiIiKWs/4vNBGps8t3vIYXdraFXc2e4D5WxwHMSTC+6/gbAAbufge/ymMWJxIRERGxloosETcRVfQDnQ6voMrmwzcdH7I6Tg2b2txIQbOONKsspPe+hVbHEREREbGUiiwRN5Gw628AbGxzE4XNoi1OU5Nh82Zl+3EA9Nv7oRYoFhERkSZNRZaIO9idScyR77HjzZr2Y6xOU6stra+nIKADzSoLic/TtVkiIiLSdKnIEnEHX78IwOY2QygMaG9xmNoZNm9WRp84m7XnQ3yrSixOJCIiImINFVkirm7fOtj6H+x4sar9WKvTnNOW1tdz+MTZLF2bJSIiIk2ViiwRV/fNSwBsDb+OI806Whzm3AybDyuj7wOg/54P8KkqtTiRiIiISONTkSXiyg5sgU1LAFjV/l5rs9RRTutkjgS0o1llIT33/9vqOCIiIiKNTkWWiCv7/q+AAbE3ciiwi9Vp6sSw+ZDddgQAffbOA8NucSIRERGRxqUiS8RVlRTAD/PN7UG/sTZLPW1oczNl3oGElubS6fAKq+OIiIiINCoVWSKuKvNdqCyFyEug42Cr09RLhU8g6yNSAOi7d561YUREREQamYosEVdUVQGr3jS3Bz0ANpu1ec5DdtsR2PGiQ+Fqwou3WR1HREREpNGoyBJxRZuXwNG9ENgaeg23Os15ORrQlm1h1wA6myUiIiJNi4osEVf0/Rvmff9x4ONvbZYLkNXuVwB0P/BfmpcftDiNiIiISONQkSXianavgd2rwdsP+t9ndZoLktcyjr0t4/AxKuiVv8TqOCIiIiKNQkWWiKupvhar13BoGWFtFidYF2kOd+yVvxibUWVxGhEREZGGpyJLxJUcPwybFpvbA/6fpVGcZWvYtZT6BBFcto+OR763Oo6IiIhIg1ORJeJK1n1kTtve5mJo18/qNE5R5R3AptZDAYjL+8TiNOLJZs+eTUxMDAEBASQkJLBq1apztl+4cCGxsbEEBAQQFxfHsmXLHM9VVFQwZcoU4uLiCAwMJCoqijFjxrB3794a+ygoKGD06NEEBQUREhLCuHHjOHbsWIMcn4iIuA8VWSKuwjDMtbEA+t3rltO2n836yNsAuKjgWwLL9lucRjzRggULSE1N5cknnyQrK4vevXuTnJzM/v21/76tWLGCUaNGMW7cONauXUtKSgopKSls2LABgJKSErKysnjiiSfIyspi0aJFbNmyhZtvvrnGfkaPHs3GjRtJS0tj6dKlfP3110yYMKHBj1dERFybzTAMw+oQjaGoqIjg4GAKCwsJCgqyOo7ImXatgreuA58A+N0WaBZS4+mX07Zak8tJ7lg/gfZFa1kRPYGVHcbX+/WTr+vWAKmkMTTG529CQgIDBgxg1qxZANjtdqKjo3nooYd49NFHz2g/YsQIiouLWbp0qeOxQYMGER8fz5w5c2p9j9WrVzNw4EB27txJhw4d2Lx5Mz179mT16tX0798fgOXLlzN06FB2795NVFTUL+ZW3yTuztK+yTAAAxt2DLwt+XJSfZP7aujPXx+n71FEzk/1WayLbzujwPIE6yNupX3RWnrl/4tV0fdh2LytjiQeory8nMzMTKZOnep4zMvLi6SkJDIyMmp9TUZGBqmpqTUeS05OZvHixWd9n8LCQmw2GyEhIY59hISEOAosgKSkJLy8vFi5ciW33nrr+R+UiNRkGLQu3kp04WqiCzOJKvqBgKqjjqdLvVuyN+gS9gbFsyu4H3ktennUiBBxPyqyRFxBaSFsWGRu97vH2iwNZFv4NVy1fQZB5fnEHF7B9tDLrY4kHuLgwYNUVVUREVFzNs6IiAhycnJqfU1eXl6t7fPy8mptX1paypQpUxg1apTjG8+8vDzatGlTo52Pjw+hoaFn3U9ZWRllZWWOfxcVFZ374ESaOsMg5vAKEna/RdTR9WdtFlB1lIsOf8dFh78DID+wB6ui7+XH0KvApqtjpPGpyBJxBes+gsrj0DoWohOsTtMgqrz82dRmGP32zqNX/hIVWeI2KioquPPOOzEMgzfeeOOC9jV9+nSefvppJyUT8WztCjO5cvurRBRvBqDSy5/c4AHsCu7HnqC+FPlHYpwooILK9tGuKJt2RdnEHP6OiOLN3JQzhUPNYvjyot+TG+KZfau4LhVZIq4g633zvu89Hj28YWObm+i3dx6dDn9Ds4rDHPdtZXUk8QDh4eF4e3uTn59f4/H8/HwiIyNrfU1kZGSd2lcXWDt37uTzzz+vMW4/MjLyjIk1KisrKSgoOOv7Tp06tcYwxaKiIqKjo3/5IEWaEC97JYN2zWXg7nexYVDu1Yx1kcPJbDeaEr/wWl9T6hvC/hY9WBs1imYVh4nfO5/4fR8RdnwHwzdOJKvtKL6NeZAqL/9GPhppqnT+VMRq+Rshbx14+cIlI6xO06AOBXYhP7AH3kYV3Q/81+o44iH8/Pzo168f6enpjsfsdjvp6ekkJibW+prExMQa7QHS0tJqtK8usLZt28Znn31GWFjYGfs4cuQImZmZjsc+//xz7HY7CQm1f2vu7+9PUFBQjZuInBRcups7148nYfc72DDY0OZm3uq/hG86TTprgXW6476tyOj4AG/1/5TsyNsB6LvvH4z64V5CS35uyPgiDiqyRKz2wz/M+27JEBh27rYeYGPEjQD03P9vi5OIJ0lNTeXNN9/kvffeY/PmzTzwwAMUFxczduxYAMaMGVNjYoxJkyaxfPlyZsyYQU5ODk899RRr1qxh4sSJgFlg3X777axZs4YPP/yQqqoq8vLyyMvLo7y8HIAePXowZMgQxo8fz6pVq/juu++YOHEiI0eOrNPMgiJSU5tjmxn1w720PbaBUu+WLO0+nbSuT1DqG3Je+yv3acEXnaewuMfLFPuG0rrkR0asG0dU0Q/ODS5SCxVZIlaqqjSvxwLoPcraLI1kS/j1VNl8iCjOIaz4R6vjiIcYMWIEL730EtOmTSM+Pp7s7GyWL1/umNwiNzeXffv2OdoPHjyYefPmMXfuXHr37s3HH3/M4sWL6dWrFwB79uxhyZIl7N69m/j4eNq2beu4rVixwrGfDz/8kNjYWK699lqGDh3KZZddxty5cxv34EU8QFRRNrdveIBmlYXktejB3/vMY1t4klP2vT30Mv4eP489QfEEVB3jto0T6XD4e6fsW+RstE6WiJW2pcGHt0OzUHNtLB+/szZ193WyTnVjzh/oeugL1kSN5ptOD9fpNVqLxH3p8/fs9LMRd+eMvqnDkZXcvPn3+NpL2RXUl3/1mEmFT6AT0tXkU1XKjTl/oNORDKpsPizr9iw/hl9zQftU3+S+GvrzV2eyRKxUPVQw7vZzFlieZlPrYQD0OPAfbEalxWlERMQqUUXZ3LIpFV97KdtDElnc89UGKbAAKr0DWNJjBlvDkvA2Khm69Y90OLKyQd5LREWWiFVKCyHnxHVJTWSoYLUdrS6lxLcVgRUFxGjIhohIkxR8fBc3b/49PkY5P7W6nE97vESld0CDvqfdy5dl3f9MTngy3kYVN+ZM0dB1aRAqskSssnExVJaaa2NF9bE6TaOye/mwufUNAPTcv9TiNCIi0tj8KwpJ2fTwiWuwerKs+1+o8mqcER2GzZv/dZ3G7qA++FcVk7LpYQLLDjTKe0vToSJLxCrVQwV7j/LotbHOZnProQBcVPANfpXHLE4jIiKNxctewU05fyC0NJci/0j+1WNGg5/BOl2Vlx9LYl+koFlHgsrzuWVzKj5Vxxs1g3g2FVkiVji8E3IzABtccqfVaSxxILAbh5p1wscop0vBl1bHERGRRnL5jteILsqizDuQxT1ervP6V85W5hvM4p6vUOLbiojiHK76eYYlOcQzqcgSscKGf5r3MZdBUBNdT8dmI6d1MoAWJhYRaSI6FXxD333zAVje7U8cCuxiaZ7CgPb8u9tfMLARt/9f6o/EaVRkiVihusiKu93aHBbbEm4WWR2OrKJ5+SGL04iISEMKLDvA9dv+BEBW25H8HHqFxYlMu0P6s7L9fQBc+9N0go/vsjiReAIVWSKNbX8O5G8AL1/ocbPVaSxV2Kw9+1r0wgs73Q5+ZnUcERFpIDajiiHbptG88gj7A7vxbcxDVkeq4fsO/489QfH4VxUzbMtjeNkrrI4kbk5Flkhj2/Cxed8lCZqHWpvFBTiGDB7UEA0REU/Vb8/f6VC4hnKvZizr9myjzSRYV4bNh2XdnqHUJ4iI4s0k7pprdSRxcyqyRBqTYcD6E0VWEx8qWG1reBJ2vIg6up7g0t1WxxEREScLOb6TxFyzaPnyot9zuHmMtYHO4ph/JGldHgOg/+4PCC/eanEicWcqskQa054sOLwdfJtD9xusTuMSSvzC2RUyAIDuB/5ncRoREXEqw07Sj3/BxyhnR8ggNra5yepE5/Rj2DVsC7sGL6q4ftufsRmVVkcSN6UiS6QxVQ8V7H4D+AVam8WF5JyYACP2wHLzbJ+IiHiEXvlLiC7KosIrgPTOj7rFupBfXPQIpd4tiSjeTJ+9862OI25KRZZIY7FXwYZF5nYvDRU81Y9hV1Np8yPs+HbCSn6yOo6IiDhBYPlBLt/xGgArOtxPUUA7ixPVTbFfOF93mgTA4Nw5BB/XUHapPxVZIo1l5wo4lgcBwdDlWqvTuJRynxbsaJUIoFkGRUQ8xJU/zyCg6ij5gT1YGzXC6jj1srHNzeQG98fXXsY1Pz+vURZSbyqyRBrLpsXmfexN4ONvaRRXtDU8CYBuhz5TZyYi4ubaFWbS/dBn2PEirctjGDYfqyPVj83GZ53/SKXNl5gj39Pp8LdWJxI3oyJLpDHYq2DTEnP74lutzeKifm51OZU2P0KP7yS85Eer44iIyHmyGVVctX0GAOsjb+VAi+4WJzo/hc2iWRs1CoArt7+stbOkXlRkiTSGnSugeD8EhMBFV1qdxiVV+ASyo9VgALpqyKCIiNu6OP9T2hRvo9S7JSs63G91nAuysv19FPuG0qp0F332aRIMqTsVWSKNwTFU8Ebw9rU0iitzDBk8qCGDIiLuyK/yGINz3wDg+w7/j1LfEGsDXaAKn0C+7fggAAm73qJ5+SGLE4m7UJEl0tBqDBVMsTSKq/u51WVUevkTWppLeMk2q+OIiEg9Jex+m8CKAgoCOvBD5B1Wx3GKTW1uJD+wB/5VxQze+YbVccRNqMgSaWinDhXspKGC51LhE8j2E0MGNcugiIh7CSrdS/yJdaW+7jQZu5eHjNywefHFRb8DoNf+JVpqROpERZZIQzt1qKCPn6VR3MHWsOohg+kaMigi4kYG7ZqLj1FBbvAAtre61Oo4TrUvqDfbwq7GhkHizjlWxxE3oCJLpCFpqGC9bQ81hwy2Ks2ldfFWq+OIiEgdhJb8TI/9/wEwr2Gy2SxO5HwrOjyAHS+6FnxJxNGNVscRF6ciS6Qh5WacGCoYrKGCdVTh3dwxZLDLoc8tTiMiInUxOHcOXtjZFnoV+S0vtjpOgyho3onNbYYCcNnO2RanEVd3XkXW7NmziYmJISAggISEBFatWnXO9gsXLiQ2NpaAgADi4uJYtmxZjecXLVrE9ddfT1hYGDabjezs7DP2cdVVV2Gz2Wrc7r/fvacFlSZg07/Mew0VrJcfQ68GoOuhLyxOIiIivyTi6Ea6HvoCAxsZHT37b7OM6AlU2nzpULia6CPn/vtXmrZ6F1kLFiwgNTWVJ598kqysLHr37k1ycjL79++vtf2KFSsYNWoU48aNY+3ataSkpJCSksKGDRscbYqLi7nssst4/vnnz/ne48ePZ9++fY7bCy+8UN/4Io3HbofNS83tnrdYm8XN/Bx6OVU2H8KObye0ZLvVcURE5BwuPTFl++bWQznUvLPFaRrW0YC2rI+8DYBLd/5V1w7LWdW7yJo5cybjx49n7Nix9OzZkzlz5tC8eXPefvvtWtu/+uqrDBkyhEceeYQePXrwzDPP0LdvX2bNmuVoc/fddzNt2jSSkpLO+d7NmzcnMjLScQsKCqpvfJHGsycTju4Fv5Zw0VVWp3Er5T4t2BmSAGjIoIiIK2tXmEnHIyupsvmQ0WG81XEaxcr291Hu1Yy2xzbC1uVWxxEXVa8iq7y8nMzMzBrFkJeXF0lJSWRkZNT6moyMjDOKp+Tk5LO2P5cPP/yQ8PBwevXqxdSpUykpKan3PkQazeYTE150SwYff2uzuKEfw64BoKuKLBERlzVo198A2BBxC0UB7SxO0ziO+4XyQ9sTa4B99YLOZkmtfOrT+ODBg1RVVREREVHj8YiICHJycmp9TV5eXq3t8/Ly6hX0V7/6FR07diQqKop169YxZcoUtmzZwqJFi2ptX1ZWRllZmePfRUVF9Xo/kQtiGCeLrB43WZvFTf0UegV2vGlTvJXg0t1AN6sjiYjIqXJX0qFwDVU2b1a3u8fqNI0qM2o08fs+wndvFvyUDl3OPRpLmp56FVlWmjBhgmM7Li6Otm3bcu211/LTTz/RufOZ43+nT5/O008/3ZgRRU7K3wCHd4BPAHS9zuo0bqnUN4TdwX3oULiGLoe+AK6xOpKIiJzq6xcB2NT6Ro4GtLU4TOM67hfKusjb6Ld3nnk2q/O1HjltvZy/eg0XDA8Px9vbm/z8/BqP5+fnExkZWetrIiMj69W+rhISzOs1fvzxx1qfnzp1KoWFhY7brl27Luj9ROqlem2sLkngF2htFje27cSQwS6aZVBExLXsyYIf07Djzer291qdxhKZ7e4Gb3/YtRK2f211HHEx9Sqy/Pz86NevH+np6Y7H7HY76enpJCYm1vqaxMTEGu0B0tLSztq+rqqneW/btvZvTvz9/QkKCqpxE2k0mz817zVU8IL8FHoVAFFH10PhHmvDiIjISV+/BEBO62QKm7W3OIw1iv3Cod+95j++0ozXUlO9ZxdMTU3lzTff5L333mPz5s088MADFBcXM3bsWADGjBnD1KlTHe0nTZrE8uXLmTFjBjk5OTz11FOsWbOGiRMnOtoUFBSQnZ3Npk2bANiyZQvZ2dmO67Z++uknnnnmGTIzM9mxYwdLlixhzJgxXHHFFVxyySUX9AMQcbqD2+DAZvDygW5DrE7j1or9W7O35Yn/x3OWWhtGRERMeRtgy78BG6vaj7U6jbUunQTefrDzW9i5wuo04kLqXWSNGDGCl156iWnTphEfH092djbLly93TG6Rm5vLvn37HO0HDx7MvHnzmDt3Lr179+bjjz9m8eLF9OrVy9FmyZIl9OnTh2HDhgEwcuRI+vTpw5w5cwDzDNpnn33G9ddfT2xsLL/73e8YPnw4n3766QUdvEiDqJ7wotOV0CzE0iieYFuYuTCxiiwRERfx7Uzz/uIUDjePsTSK5YLbQfxoc/vbVyyNIq7FZhhNY97JoqIigoODKSws1NBBaVhzr4K9a+HGV6C/877hezltq9P25U6CS3dzX+atYPOGR36E5qFWR5J60ufv2elnI27n8A54rS8YVfDrb3h5Q4DViSw1+bpucOgneL0fYMADGRDR0+pYUgcN/flb7zNZInIOhbvNAgsbxA6zOo1HKAxoz4HmXc0OXYs+iohYK+Ov5udx52ugrS7ZACCsM/S82dxe8bq1WcRlqMgScaacZeZ9dAK0aGNtFg/yY9hV5sZmDRkUEbFMSQGs/cDcHvxba7O4mksnmffrPzK/cJUmT0WWiDNVXzfU40Zrc3iYn0KvPLHxOZSXWBtGRKSpWv03qCiByEvgoqusTuNa2vWDmMvBXgnfv2F1GnEBKrJEnKWkAHZ8a25rqKBTHQjsBiEdoPI4/JT+yy8QERHnqjgOK//P3L50khberU312azMd+H4YUujiPVUZIk4y9b/muPU21wMoRdZncaz2GwQe2LNMQ0ZFBFpfNnzoOSg+YVXzxSr07imLknQpieUH4M1b1udRiymIkvEWTRUsGFV/1y3/geqKqzNIiLSlNjtkDHb3E6cCN4+1uZxVTbbyWvVVs6FynJr84ilVGSJOEN5Cfx4Yhibhgo2jOgEaB4OpYWw8zur04iINB3b/gcFP0FA8Mk1oaR2vYZDiwg4lgebFludRiykIkvEGX7+wrxeKLiDeUGwOJ+XN3S/wdzWkEERkcbz/V/N+373gn8LS6O4PB8/GDDe3M6YDU1jOVqphYosEWeo/qM/dpguBm5IPU5cl7VlmTouEZHGkLcBtn9lLghfXTzIufUfCz4BsC8bcr+3Oo1YREWWyIWqqjSvEwJdj9XQOl0JvoFQtMfsvEREpGGtPDEdec+bISTa2izuIjAcLrnT3K4+CyhNjooskQu163tzqtZmrSB6kNVpPJtvAHS51tzO+be1WUREPN2xA7Buobk96DfWZnE31T+vnKVweIelUcQaKrJELlT1H/vdbtCMS40h9sTZwpxl1uYQEfF0me9AVZm50G77AVancS9tekDna8Cww6o3rU4jFlCRJXIhDONkkRU71NosTUXX68xrA/ZvhILtVqcREfFMleWw+m/m9qDf6Hrj81F9NivrfSg7Zm0WaXQqskQuxP5NcGSneYFr52usTtM0NA+FmEvN7S06myUnzZ49m5iYGAICAkhISGDVqlXnbL9w4UJiY2MJCAggLi6OZctq/j4tWrSI66+/nrCwMGw2G9nZ2Wfs46qrrsJms9W43X///c48LBFrbPoXHMuHlm2h5y1Wp3FPna+F0M5QVgTrFlidRhqZiiyRC1F9Fuuiq8Ev0NosTUn3E2uR6bosOWHBggWkpqby5JNPkpWVRe/evUlOTmb//v21tl+xYgWjRo1i3LhxrF27lpSUFFJSUtiwYYOjTXFxMZdddhnPP//8Od97/Pjx7Nu3z3F74YUXnHpsIpZYNde8738fePtam8VdeXnBwAnm9qo3NStuE6MiS+RCaKigNap/3rkZUHzI2iziEmbOnMn48eMZO3YsPXv2ZM6cOTRv3py333671vavvvoqQ4YM4ZFHHqFHjx4888wz9O3bl1mzZjna3H333UybNo2kpKRzvnfz5s2JjIx03IKCgpx6bCKNbu9a2L0KvHyh7z1Wp3Fv8aPMWXEPbIYd31idRhqRiiyR81W4+8Q04jboNsTqNE1LSAeIjDMvKN663Oo0YrHy8nIyMzNrFENeXl4kJSWRkZFR62syMjLOKJ6Sk5PP2v5cPvzwQ8LDw+nVqxdTp06lpKSk3vsQcSmrTlyLdXEKtIywNIrbCwiG3iPN7eqzg9IkaCo0kfO15cTaWNEJ0KKNtVmaotgbIW+9eV1Wn9FWpxELHTx4kKqqKiIiav4xGBERQU5OTq2vycvLq7V9Xl5evd77V7/6FR07diQqKop169YxZcoUtmzZwqJFi2ptX1ZWRllZmePfRUVF9Xo/kQZXUgDrT0zbXj3UTS7MwAmw5i1z9MuRXVpvrInQmSyR86WhgtbqfuLn/mM6lOvMgVhjwoQJJCcnExcXx+jRo3n//ff55JNP+Omnn2ptP336dIKDgx236Gj9sSUuJut9c9r2tr01bbuztImFTleYoy/W1D6EWTyPiiyR81FaCDu+NberJ2GQxhUZB8EdoPI4/Pyl1WnEQuHh4Xh7e5Ofn1/j8fz8fCIjI2t9TWRkZL3a11VCQgIAP/74Y63PT506lcLCQsdt165dF/R+Ik5lr4LVb5nbAydo2nZnqj4rmPUeVJRam0UahYoskfOxLQ3sFRDeDcK7WJ2mabLZoPsN5vYWzTLYlPn5+dGvXz/S09Mdj9ntdtLT00lMTKz1NYmJiTXaA6SlpZ21fV1VT/Petm3bWp/39/cnKCioxk3EZWz9LxTmQrNW0Gu41Wk8S7cbIKg9lByCTYutTiONQEWWyPmoHirYXUMFLVU9VHPLcvMbWGmyUlNTefPNN3nvvffYvHkzDzzwAMXFxYwdOxaAMWPGMHXqVEf7SZMmsXz5cmbMmEFOTg5PPfUUa9asYeLEiY42BQUFZGdns2nTJgC2bNlCdna247qtn376iWeeeYbMzEx27NjBkiVLGDNmDFdccQWXXHJJIx69iJOsOXEWq89d4NvM2iyextsH+t9rblefLRSPpiJLpL4qy+HHz8ztWA0VtFTHS82Zm0oOwu7VVqcRC40YMYKXXnqJadOmER8fT3Z2NsuXL3dMbpGbm8u+ffsc7QcPHsy8efOYO3cuvXv35uOPP2bx4sX06tXL0WbJkiX06dOHYcPM/89HjhxJnz59mDNnDmCeQfvss8+4/vrriY2N5Xe/+x3Dhw/n008/bcQjF3GSgu3mNa4A/cZam8VT9RkDXj7m9Ph5661OIw1MswuK1NeOb8zV2wPbQLv+Vqdp2rx9oev15kxYOf+GDoOsTiQWmjhxYo0zUaf68ssvz3jsjjvu4I477jjr/u69917uvffesz4fHR3NV199Vd+YIq4p8x3AgM7XQFhnq9N4ppYR0OMm2PiJeTbrplesTiQNSGeyROpryzLzvvsQczV3sVb1kM3q/y4iIlI/lWWw9u/mdv9x1mbxdNU/33UfQamWcPBkOpMlUh+GcXJ9LM0q6Bq6JIGXLxz6EQ5shdbdrE4kIuJeNv3LnJAhqB10G2J1GrfyctrW+r3AaMOYZp0IO76d9I9eZ13bs59Nr4vJ16nPc1X6Gl6kPvZlQ9Ee8A2Ei660Oo0ABASZ64+AZhkUETkf1RMx9L3HnKBBGo7NxrrI2wDonfdP88tb8UgqskTqo/osVpdrNPOSK6meZTBHQwZFROolfyPs+h5s3tB3jNVpmoTNbYZR4RVAeMlPtCvKtjqONBAVWSL1Uf1HvIYKupbq67J2r4Zj+63NIiLiTta8bd7HDoOg2td3E+cq82lJTngyAHH5iyxOIw1FRZZIXR3eCfnrzW/7uiVbnUZOFRQFUX2AU66ZExGRcysvhh8WmNv977M2SxOzPvJWALoe/JyAiiPWhpEGoSJLpK6q/3jvkAjNQ63NImeqPruoWQZFROpmwz+h/Ci06gSddJ1xY8pv0ZP8wO74GOX03K/riT2RiiyRuspZat5XX/8jrqX6v8vPX5rfzoqIyLmtece873evliRpbDYb609MgBGX/4kmwPBA+j9KpC5KCmDnCnO7u4osl9SmJ4R0hMpS+Olzq9OIiLi2fT/A3ixzCYz40VanaZJywpMp92pO6PGdtC/KsjqOOJmKLJG62JYGRpX5h3xoJ6vTSG1sNoi90dzWLIMiIudWfRarx03QorW1WZqoCp9AclqfmAAjTxNgeBoVWSJ1Ub3+UqxmFXRp1UMGty6Hqkprs4iIuKqyo7B+obndf6y1WZq46iGDXQ99TrPyAovTiDOpyBL5JRWl8GO6ua2hgq4tehA0awXHC2DXSqvTiIi4pvUfQ/kxCOsCMZdbnaZJ298ilrwWPfA2Krl4/1Kr44gTqcgS+SXbvzY7o5bV04SLy/L2gW5DzO0czdYkIlKrzHfN+373mkOtxVLrI8yzWb3yF2sCDA+iIkvkl1QPFex+gzojd1B9tnHLv9VZiYicbu9a2JcN3n7Q+1dWpxFgS+vrKfdqTqvSXbQvzLQ6jjiJiiyRc7HbT66Ppanb3UPna8DbHw7vgP2brU4jIuJaMt8z73vcBIFh1mYRACq8m5PT2hyFEZf/icVpxFlUZImcy94sOJYP/kEQc4XVaaQu/FtA56vNbQ0ZFBE5qeyYeT0WmEMFxWWsj7wVgC6HviCg4oi1YcQpVGSJnEv1H+ldksDHz9osUnenDhkUERHTxkVQfhRCL9KEFy5mf4tY8gN74GNU0HO/+i5P4GN1ABGXlnNy6vaX07Zam0XqrvsN8KnNvPagcDcEt7c6kYiI9TThhUtbH5lCxE+bicv/hKyoX+m/kZvTmSyRszm4DQ5uAS9f6Hqd1WmkPlq0gegEc7v6mjoRkaZs3zrYk2n2aZrwwiVtCb+ecq9mhB7fSbuitVbHkQukIkvkbKrPYnW6AgKCrc0i9Ve9cHSO1h0RESHrxIQXscOgRWtrs0ityn1asKX19YAmwPAEKrJEzuaUoYLihqr/u+34Fo4fsTSKiIilyktg3Ufmtia8cGnrI8wJMLoe/Bz/ikKL08iFUJElUpujebB7tbndXVO3u6WwztC6B9grYVua1WlERKyz8RMoK4KQjtDpSqvTyDnkt+jJ/sCu+Bjl9Dig4e7uTEWWSG22/AcwoF1/CGprdRo5X9Vrm2nIoIg0ZdVDBfuOAS/96efSbDY2RKQAEJe/GAzD0jhy/vR/mkhtNFTQM1T/9/vxM6gotTaLiIgV9m+GXSvB5g197rI6jdRBTusbqPDyJ7zkJ9oeXW91HDlPKrJETldaBNu/Mrdjb7Q2i1yYtn2gZRSUH4PtX1udRkSk8WW9b953vwFaRlqbReqkzKclW8OTAOiVv9jaMHLeVGSJnO7HNKgqh7Cu0Lqb1WnkQnh5nTJk8FNrs4iINLaKUvjhH+Z233uszSL1suHEBBjdD6bhV3nM4jRyPlRkiZxOQwU9i2Mq92Vgr7I2i4hIY9r8KRw/DEHtocu1VqeRetjb8hIONeuEr72U2AP/tTqOnAcVWSKnqiw7OROdiizPEHO5uc5ZyUHzugQRkaaiesKLPneBl7e1WaR+bDbWn5gAo5fWzHJLKrJETrX9a3Oa2xaR5syC4v68faHbDeb2Zs0yKCJNxKGfYMc3YPPShBduanOboVTafIko3kKbY5utjiP1pCJL5FSbT1y3EztM09x6kh4nJjDJ+VTT4YpI01B9FqtLEoREW5tFzkupbwg/hl0DQFzeYmvDSL3pr0iRavaqk9dj9bjJ2iziXJ2vBZ9mcCQX8tZZnUZEpGFVlsPaD81tTXjh1qqHDMYeWI5vVYm1YaRezqvImj17NjExMQQEBJCQkMCqVavO2X7hwoXExsYSEBBAXFwcy5Ytq/H8okWLuP766wkLC8Nms5GdnX3GPkpLS3nwwQcJCwujRYsWDB8+nPz8/POJL1K7XSvN63YCQiDmMqvTiDP5NT950beGDIqIp9uyzOzPWkRCt2Sr08gF2B3cj8MB0fjZS+h2MM3qOFIP9S6yFixYQGpqKk8++SRZWVn07t2b5ORk9u/fX2v7FStWMGrUKMaNG8fatWtJSUkhJSWFDRs2ONoUFxdz2WWX8fzzz5/1fSdPnsynn37KwoUL+eqrr9i7dy+33XZbfeOLnF31UMHuN5jX8YhnqT47maMiS0Q8XOa75n2f0erP3J3NxoYTZ7M0ZNC91LvImjlzJuPHj2fs2LH07NmTOXPm0Lx5c95+++1a27/66qsMGTKERx55hB49evDMM8/Qt29fZs2a5Whz9913M23aNJKSkmrdR2FhIW+99RYzZ87kmmuuoV+/frzzzjusWLGC77//vr6HIHImwzh5hkMLEHumbsng5QP7N5kXhIuIeKLDO+DnL8ztvmMsjSLOsanNMKps3rQ9toHw4m1Wx5E6qleRVV5eTmZmZo1iyMvLi6SkJDIyMmp9TUZGxhnFU3Jy8lnb1yYzM5OKiooa+4mNjaVDhw712o/IWe37AQpzwbc5dL7G6jTSEJq1Mqdzh5NnLUVEPE3W++b9RVdDqxhLo4hzlPiF8VPoVQD0yl9saRapu3oVWQcPHqSqqoqIiIgaj0dERJCXl1fra/Ly8urV/mz78PPzIyQkpM77KSsro6ioqMZN5Kyqh5B1STKv3xHPVD3LoIosEfFEVRUnJ7zod6+lUcS5qifA6LF/GT5VpdaGkTrx2NkFp0+fTnBwsOMWHa3pS+Ucqv/o1qyCni32RsAGe9ZA4R6r04iIONfW/8KxPGgeDt2HWp1GnCg3ZCCF/lEEVB2j26HPrI4jdVCvIis8PBxvb+8zZvXLz88nMjKy1tdERkbWq/3Z9lFeXs6RI0fqvJ+pU6dSWFjouO3atavO7ydNzMFtcCAHvHyh6/VWp5GG1DISohPMbU2AISKepnptrPhfgY+ftVnEuWxebIi4BYBemgDDLdSryPLz86Nfv36kp6c7HrPb7aSnp5OYmFjraxITE2u0B0hLSztr+9r069cPX1/fGvvZsmULubm5Z92Pv78/QUFBNW4itdr0L/P+oqugWYiVSaQx9LzZvN+0xNocIiLOdCQXtp2Y4ltDBT3SxoibseNNu6M/EFaiCZxcXb2HC6ampvLmm2/y3nvvsXnzZh544AGKi4sZO3YsAGPGjGHq1KmO9pMmTWL58uXMmDGDnJwcnnrqKdasWcPEiRMdbQoKCsjOzmbTpk2AWUBlZ2c7rrcKDg5m3LhxpKam8sUXX5CZmcnYsWNJTExk0KBBF/QDEHEUWdV/fItnqx4SuvM7OFb70hMiIm4n6wPAgE5XQFhnq9NIAyj2C+enUHMCp7i8TyxOI7+k3kXWiBEjeOmll5g2bRrx8fFkZ2ezfPlyx+QWubm57Nu3z9F+8ODBzJs3j7lz59K7d28+/vhjFi9eTK9evRxtlixZQp8+fRg2bBgAI0eOpE+fPsyZM8fR5uWXX+bGG29k+PDhXHHFFURGRrJo0aLzPnARAAq2Q946sHlD92FWp5HGENIBovoAhoYMiohnqKqEtR+Y2/3GWptFGtT6yFsBcwIMb02A4dJshmEYVodoDEVFRQQHB1NYWKihg3LSd69C2jTodCXcc+7hYy+nbW2kUFKbydd1c97Ovn0ZPnvKnOJ4zGLn7Vdqpc/fs9PPRpwiZxnMH2VOeJG6uVGvx1Lf2MgMO/dlphBcto/lXZ9myOiHrU7kthr689djZxcUqRPHUMFbrM0hjavHiaGhO76BkgJrs4iIXKjMd8x7TXjh+WxebDgxnXtcnkZ0uTIVWdJ0HdkFezIB24mpvaXJCOsMEb3AXglb/mN1GhGR83dklya8aGI2trnJMQEG+zdbHUfOQkWWNF3Va2N1HAwtI87dVjxP9dmszZplUETc2FpNeNHUFPu35ucTE2CQ+a6lWeTsfKwOIGIZDRV0K84e9x9WcgljgMpt6fzff7Io92lxzvZOvSZMRMQZqioh631zW2exmpR1kbfSpeBL+OEfcO2T4Nfc6khyGp3JkqapaB/sWmluV0/pLU3KoWYXcahZJ3yMCi4q+NrqOCIi9bd1ORzdZ054Eau+rCnZGTKIQv8oKC2EjZrO3RWpyJKmafOngAHtB0BQlNVpxAo2G9vCrwWg26H0X2gs7mD27NnExMQQEBBAQkICq1atOmf7hQsXEhsbS0BAAHFxcSxbtqzG84sWLeL6668nLCwMm81Gdnb2GfsoLS3lwQcfJCwsjBYtWjB8+HDy8/OdeVgiZ7fmbfO+792a8KKpsXk5pnN3/B6IS1GRJU1T9bc+F99qbQ6x1NbwJAA6Hs7Ar/KYxWnkQixYsIDU1FSefPJJsrKy6N27N8nJyezfX/uC0ytWrGDUqFGMGzeOtWvXkpKSQkpKChs2bHC0KS4u5rLLLuP5558/6/tOnjyZTz/9lIULF/LVV1+xd+9ebrvtNqcfn8gZCrbDT+mADfreY3UascDGNjeBly/sWQP71lkdR06jIkuanqK9kJthbvdMsTSKWOtQ886OIYOdNWTQrc2cOZPx48czduxYevbsyZw5c2jevDlvv137N7yvvvoqQ4YM4ZFHHqFHjx4888wz9O3bl1mzZjna3H333UybNo2kpKRa91FYWMhbb73FzJkzueaaa+jXrx/vvPMOK1as4Pvvv2+Q4xRxqJ7woPM1ENrJ0ihijRK/MOhxYnbk6mn8xWWoyJKmZ9O/AAOiB0FwO6vTiMW2hl8HQLeDaRYnkfNVXl5OZmZmjWLIy8uLpKQkMjIyan1NRkbGGcVTcnLyWdvXJjMzk4qKihr7iY2NpUOHDmfdT1lZGUVFRTVuIvVWWQZr/25u97/P2ixirer//us+grKj1maRGlRkSdOjoYJyiq0nrsvqeOR7/CvVQbmjgwcPUlVVRUREzaUYIiIiyMvLq/U1eXl59Wp/tn34+fkREhJS5/1Mnz6d4OBgxy06OrrO7yfisPlTKDkILdtCtyFWpxErxVwOYV2g/Bis/9jqNHIKFVnStBTuPjGroE1TtwsABc0v4mDzi/A2KjXLoDS4qVOnUlhY6Ljt2rXL6kjijtacGBrWdwx4azWeJs1mO3k2a81bYBjW5hEHFVnStFSvjdVxMAS1tTaLuIytYeZwr24HP7M4iZyP8PBwvL29z5jVLz8/n8jIyFpfExkZWa/2Z9tHeXk5R44cqfN+/P39CQoKqnETqZf9ObDzW7B5mUWWSO9R4BMAeeth9xqr08gJKrKkadmwyLzXUEE5xbbqWQaPfI9/pa6RcTd+fn7069eP9PSTU/Hb7XbS09NJTEys9TWJiYk12gOkpaWdtX1t+vXrh6+vb439bNmyhdzc3HrtR6Re1rxl3ncfCsHtrc0irqF5KPQabm6v/pu1WcRBRZY0HYd3mtOc2rygx81WpxEXUtC8Ewead8HbqKTzoS+tjiPnITU1lTfffJP33nuPzZs388ADD1BcXMzYsWMBGDNmDFOnTnW0nzRpEsuXL2fGjBnk5OTw1FNPsWbNGiZOnOhoU1BQQHZ2Nps2bQLMAio7O9txvVVwcDDjxo0jNTWVL774gszMTMaOHUtiYiKDBg1qxKOXJqPsGGT/w9weMM7aLOJaqn8fNi6C4kPWZhFARZY0JY6hgpdCy4hzt5Ump3qWwdiD/7M4iZyPESNG8NJLLzFt2jTi4+PJzs5m+fLljsktcnNz2bdvn6P94MGDmTdvHnPnzqV37958/PHHLF68mF69ejnaLFmyhD59+jBs2DAARo4cSZ8+fZgzZ46jzcsvv8yNN97I8OHDueKKK4iMjGTRokWNdNTS5Kz/CMqPQmhn6HSV1WnElbTrB1F9oKoc1n5gdRoBbIbRNK6QKyoqIjg4mMLCQo2Bb6r+7wrY9wMMm3le3wC+nLa1AUKJqwg+vpv7sm7FjhdvDlhmrj9yisnXdbMomfvT5+/Z6WcjdWYYMOcyyN8AyX+BxAetTgSob7Rajb5p7d/hXw9CSEf47Vrw8rYumBto6M9fncmSpuHgNrPA8vLRAsRSq8Jm7dnX4mK8sNP1UPovv0BEpDHtWmUWWD7NIP5XVqcRV3TxbRAQDEd2wo/qx6ymIkuahuq1IzpfA4Fh524rTdaW1tcDEHvgvxYnERE5TfWEBnHDoVkra7OIa/JrDvF3mduaAMNyKrLE8xkGrF9obsfdYW0WcWlbw67DwEbU0XW0LN33yy8QEWkMxw7ApsXm9oD/Z2kUcXHVa2Zt+x8UbLc2SxOnIks8375sKPjJHGLRfajVacSFFfu3ZldwPwC6awIMEXEVWe+aExpUT24gcjbhXcxROxgnp/sXS6jIEs9XPVSw+w3g38LaLOLytoSbQwa7H9SQQRFxAVUVsPptc3vgr63NIu6h+vck6wMoL7E2SxOmIks8m70KNvzT3I673dos4ha2hV1Dlc2HNsXbCC3RUAsRsVjOUji6FwJbw8UpVqcRd9D1OnOGwdIjJy+XkEanIks8284VcHSfOdtOlySr04gbKPMNZmeIuZBs9wPLLU4jIk3eyrnmfb+x4ONvbRZxD17eMHC8ub1qrnltujQ6FVni2aq/wel5izonqbOc1kMA6HFguTonEbFO3nrIXWEuP9J/rNVpxJ30ucu8Fj1/g/mFszQ6FVniuSpKT87G1EtDBaXufgq9kjLvQILL9hJ19Aer44hIU7XqxFmsHjdBUJS1WcS9NGsFl9xpblf/HkmjUpElnmvrcigthKB2EHO51WnEjVR6B/Bj2NUA9Ni/zOI0ItIklRTAuhOjMTThhZyPgRPM+82fQuEea7M0QSqyxHOtW2DeX3IneOlXXepnc2tzuv9uBz/D215mcRoRaXKy3ofK4xAZBx0GWZ1G3FFkL+h4KRhVWpzYAvrLUzxT8UFzIT6AS0Zam0Xc0q7gfhz1a0NA1VE6FXxndRwRaUqqKk4O8Up4AGw2a/OI+xr0gHmf+Y6mc29kKrLEM234J9groW08tIm1Oo24I5vXKRNgaMigiDSizZ9C0R5z2vZew61OI+6s+1AI6QDHD58c4SONQkWWeKYf5pv3vUdZm0Pc2uY25pDBToe/M6+PEBFpDN+/Yd73Hwe+AdZmEffm5Q0J95vbK+doxtxGpCJLPM+BLbA3C2ze+gZQLsih5p3JD+yOt1F5clFrEZGGtHsN7F4F3n4wYJzVacQT9LkL/FrAgRz4+Qur0zQZKrLE81Sfxep6HbRobW0WcXvVE2A4fq9ERBrS93817+PugBZtrM0iniEg2Cy04ORZUmlwKrLEs9jtsO4jc/uSEdZmEY+wpXUydrxhzxrYn2N1HBHxZIV7YONic7t6iJeIMwycANjMScEObrM6TZOgIks8y/YvoWg3+AdD9xusTiMeoMQvjO2hl5r/yP67tWFExLOtmmtOtx1zObS9xOo04knCOp/8uyhjtrVZmggVWeJZ1p74IzjudvBtZm0W8Rgb2txsbvww35xaWUTE2cqOwpp3zO3qabdFnClxonn/wz/MpW6kQanIEs9RUgCbl5rbfe+2Not4lB2tLjWnUi4+ANvSrI4jIp4o630oK4SwrtBNIzGkAXQcDFF9obIUVr1pdRqPpyJLPMf6j6GqDCLizPWxRJzE7uUDvU8sar1WQwZFxMmqKk5OSDB4InjpzzNpADYbDH7I3F79phYnbmD6v1g8x9oPzPs+d5kfJCLOFH9iZqaty+FovrVZRMSzbFwMhbvMM+aXjLQ6jXiyHjebixOXHDKHDUqDUZElnmHfD5C3zlxX5JI7rU4jnqhNLLTrb16Uvm6B1WlExFMYBqx4zdwe+GstPiwNy9vn5LVZGbPAXmVtHg+mIks8Q/UQrthh0DzU2iziuarXGVn7d/MPIxGRC7X9K/NLQt/mWnxYGkf8aAgIgYKfYcsyq9N4LBVZ4v4qSk+ujdVHE15IA+p1G/g0g4NbYNdKq9OIiCf47sRZrD536UtCaRz+LU4W9N+9qi8NG4iKLHF/mz+F0iMQ1B4uusrqNOLJAoKh13Bzu3qqZRGR87XvB/gpHWxeMOg3VqeRpmTgr8HbH3avhh3fWp3GI6nIEve35m3zvu8Y8PK2Not4vv5jzfuNn5jLBoiInK9vZpr3vYZDaCdrs0jT0jLi5BD4b2dam8VDqcgS97Z/M+SuAJu31saSxtGun7lMQFWZZmYSkfN38EfY9C9z+7LJ1maRpunS35p/P/30Oexda3Uaj6MiS9xb9ZCt2KEQFGVtFmkabDbof6+5veYdjWUXkfPz3cuAAd2GQMTFVqeRpqhVzMkh8N++bGkUT6QiS9xXefHJMwn977M2izQtcXeCbyAc2gY7v7M6jYi4m8Ld8MOJpSAu/521WaRpqz6LumkJHNxmbRYPoyJL3NeGf0JZEbTqBJ2usjqNNCUBQRB3u7mtCTBEpL5WzAJ7BXS8DKIHWp1GmrKIntDtBsCA716xOo1HUZEl7qt6wov+Y8FLv8rSyKonwNj0Lyg+aG0WEXEfxQch6z1z+/JUa7OIwMnfwx/mw5Fca7N4EP1lKu5pT5Z5kaa3n7monkhji+pj3uwVkPW+1WlExF2seA0qSqBtPHS+xuo0IubZ1E5Xgr3y5IyXcsFUZIl7Wv2Wed/zFggMtzaLNF0DJ5j3q/8GVZXWZhER11d8CFb9zdy+6lFzIh0RV3DVo+b92r/DkV3WZvEQKrLE/RQfhPULze3qP3JFrHDxbdA8HIr2QM5Sq9OIiKvLeB0qiqFtb3NWQRFX0XEwxFxujs7QTINOoSJL3M+ad8w1iqL6QvsBVqeRpsw34OS1WavmWptFRFxb8SFY9aa5faXOYokLcpzN+gAK91ibxQOoyBL3UlluDs0CGPSAOimxXv/7wMvHnMp93zqr04iIq8qYBeXHIPIS6H6D1WlEzhRzmTnjZVW5zmY5gYoscS+b/gXH8qBFBPRMsTqNiLkIdo+bze1V/2dtFhFxTSUFJ892XzlFXxCK67pqinmf9Z7OZl0gFVniXla+Yd73Hwc+ftZmEamWcL95v/5jc0iQiMipvnvVPIsVEQexw6xOI3J2MZdDx0vNs1lfv2h1Grd2XkXW7NmziYmJISAggISEBFatWnXO9gsXLiQ2NpaAgADi4uJYtmxZjecNw2DatGm0bduWZs2akZSUxLZtNVedjomJwWaz1bg999xz5xNf3NWu1bAn05y2vfo6GBFXED3QnI65shQytTixiJziaB6sPHGW+5rHdBZLXJvNBtc8bm6v/QAKfrY2jxurd5G1YMECUlNTefLJJ8nKyqJ3794kJyezf//+WtuvWLGCUaNGMW7cONauXUtKSgopKSls2LDB0eaFF17gtddeY86cOaxcuZLAwECSk5MpLS2tsa8//elP7Nu3z3F76KGH6htf3Fn1Waxet0OLNtZmETmVzWZeIwjmkKDKMmvziIjr+PolqDwO7QdqRkFxDx0HQ5ckc92sL3VC43zVu8iaOXMm48ePZ+zYsfTs2ZM5c+bQvHlz3n777Vrbv/rqqwwZMoRHHnmEHj168Mwzz9C3b19mzZoFmGexXnnlFR5//HFuueUWLrnkEt5//3327t3L4sWLa+yrZcuWREZGOm6BgYH1P2JxT4d3wMZPzO1B91saRaRWvYZDUDs4lg/rFlidRkRcweEdkPmuuX3tNJ3FEvdRfTZr3UeQv8naLG6qXkVWeXk5mZmZJCUlndyBlxdJSUlkZGTU+pqMjIwa7QGSk5Md7bdv305eXl6NNsHBwSQkJJyxz+eee46wsDD69OnDiy++SGXl2Rf/LCsro6ioqMZN3FjGbDDscNHV5voiIq7G2xcG/cbc/u41sNutzSMi1vvyeXPdoYuuhk6XW51GpO6i+pyY1MmAL561Oo1bqleRdfDgQaqqqoiIiKjxeEREBHl5ebW+Ji8v75ztq+9/aZ+//e1vmT9/Pl988QW//vWv+ctf/sIf/vCHs2adPn06wcHBjlt0dHTdD1RcS/FByPrA3L7sYUujiJxTv3vAPxgObYOty61OIyJW2p8D6+ab29c+YW0WkfNx9WNg84KcpbA70+o0bsdtZhdMTU3lqquu4pJLLuH+++9nxowZvP7665SV1X7tw9SpUyksLHTcdu3a1ciJxWlWvWmOZ28bD52utDqNyNn5tzw5Kct3r1qbRUSslf4ncwRG7I3Qrp/VaUTqr00sXDLS3E6bBoZhbR43U68iKzw8HG9vb/Lz82s8np+fT2RkZK2viYyMPGf76vv67BMgISGByspKduzYUevz/v7+BAUF1biJGyovPrn20KWTNJ5dXF/C/eYMmLu+h13nnnlVRDzUjm9hy7/B5m1eiyXirq7+I3j7w85vNUKjnnzq09jPz49+/fqRnp5OSkoKAHa7nfT0dCZOnFjraxITE0lPT+fhhx92PJaWlkZiYiIAnTp1IjIykvT0dOLj4wEoKipi5cqVPPDAA2fNkp2djZeXF23aaJY5j7b273D8MLSKgZ63WJ1G5JcFtYVL7jR/d799BUbNszqRiDQmux3+d2LSgH73QuvuDfp2L6dtbdD9SxMXEg2Jv4FvXzbPZnW5DrzrVT40WfUeLpiamsqbb77Je++9x+bNm3nggQcoLi5m7FhziMyYMWOYOnWqo/2kSZNYvnw5M2bMICcnh6eeeoo1a9Y4ijKbzcbDDz/Mn//8Z5YsWcL69esZM2YMUVFRjkIuIyODV155hR9++IGff/6ZDz/8kMmTJ3PXXXfRqlUrJ/wYxCVVVcAKcxZKBj8EXt7W5hGpq8GTAJv5TXbehl9sLs6hNRzFJWz4J+xdC34t4aqpv9xexNVdNhmah8HBrZD1ntVp3Ea9i6wRI0bw0ksvMW3aNOLj48nOzmb58uWOiStyc3PZt2+fo/3gwYOZN28ec+fOpXfv3nz88ccsXryYXr16Odr84Q9/4KGHHmLChAkMGDCAY8eOsXz5cgICAgBz6N/8+fO58sorufjii3n22WeZPHkyc+fOvdDjF1f2wz+gMBcC20D8aKvTiNRd625w8a3m9tcvWpulidAajuISKkoh/Wlz+7JJ0KK1tXlEnCEgGK581Nz+cjqUHbU2j5uwGUbTuIqtqKiI4OBgCgsLdX2WO6iqgNf7wpFcuP5ZGFz7cNTGpCEZTdvk67rV7wX5m+CNRMAGv8mANj0aJJc7aIzP34SEBAYMGOBYg9FutxMdHc1DDz3Eo48+ekb7ESNGUFxczNKlSx2PDRo0iPj4eObMmYNhGERFRfG73/2O3//+9wAUFhYSERHBu+++y8iR5sXgMTExPPzwwzWGxNeH+iYP892r5pCqllHwUCb4NW/wt1Tf1LTVu286X1UVMDsBCn6Cy3/vETNmNvTnr9vMLihNzA/zzQIrsDX0v8/qNCL1F9Hz5BojX79kdRqPpjUcxSUczYevTpy5vubxRimwRBqNty9c9ydze8XrULDd2jxuQEWWuJ6qipNDrC59WB2VuK8rHjHvN/wTDujb5oaiNRzFJXz2FJQfhai+0HuU1WlEnC92GFx0FVSVnZzcRc5KRZa4nnUL4MhOncUS99f2Eug+FDDgmxlWp5EGoDUcBYBdq+GHEzOJDn0RvPTnlXggmw2GPG8uTZCzFH5MtzqRS9OngLiWqspTzmJN0lkscX9Xnjirsf4jOLDF2iweSms4iqXsdvjPibPW8aOhfX9r84g0pDaxkPBrc3v5o1BZbm0eF6YiS1xL9t/h8A5oHq6zWOIZovpA7I1g2OHzZ6xO45FOXcOxWvUajtVrMp6ueg3HU51tDcdq1Ws4nm2foDUcm6S1H5hTtvsHQdJTVqcRaXhXTjH/Tju4FVZppu+zUZElrqO8BL6Ybm5f8Qj4BVqbR8RZrnkCbF6w+VPYvcbqNB5JaziKJUoKTk7ZftWj0ELFtTQBzUIg6Ulz+8vpULjH0jiuSks2i+tY+QYcy4OQjtB/rNVpRJynTSz0/pV5pvazp+CeT82x7eI0I0aM4MCBA0ybNo28vDzi4+PPWMPR65TrZKrXcHz88cf54x//SNeuXWtdw7G4uJgJEyZw5MgRLrvsslrXcHzqqacoKyujU6dOTJ48mdTU1MY9eLFO2hNQcghax8LACVanEWk88XdB1vuwezUsnwIj/m51IpejdbLENZQUwKu9oawIbvsbXHKH1YnOoLVImrYLXovkyC5z7beqcrhrEXS51jnB3IA+f89OPxs3tv0beO9Gc/u+/0KHQZbEUN/UtDXaOlm1yd8I/3cF2Cth5D8gdqh1Wc5DQ3/+6kyWuIZvZpgFVmQc9BpudRoR5wuJhgHj4fvZ5tmsi67WDGTiFFb/kW3pH3lWqSiFpQ+b2/3GWlZgiVgq4mJInAjfvQLLHoFOV4B/C6tTuQz18GK9I7knL5xMekp/eIrnuvx34NcS8taZsw2KiHv6diYc+hFaRGiyC2narpxiXuZRtBu++IvVaVyKzmTVg74tbCBpT5pDqDpdAZ2bzhAqaYICw+DyVPNC+c+eMmcd1Ld+Iu5lfw58M9PcHvKcOQmASFPl1xyGzYQPh5vX1ve6TcsYnKBTBmKtHd/BxkXmzGvXP6vJAMTzDfoNtIqBo/vg25etTiMi9VFVCYvvB3sFdE2Gi2+1OpGI9bomQdyd5lIlix8wh9OKiiyxUFUl/OfEQq39xkLbS6zNI9IYfAPMLxQAVrxurgsnIu7h25fNNbECguGmV/XFoEi1G543h88e3ApfPGt1GpegIkusk/Uu5G+AgBC45nGr04g0nthh0OlKqCqD/z1hdRoRqYu89fDV8+b2DS9CUFtr84i4kuahcOMr5vaK1yF3paVxXIGKLLFGSQF8/mdz+5rHzf85RZoKm828lsPmBZuXwM9fWZ1IRM6lshw+ecAcJhh7I1xyp9WJRFxP7FDoPQowzGGD5SVWJ7KUiiyxRvqf4PhhaHOxOVRQpKmJ6An9x5nb/07VGHYRV/bldMhfD81C4caXNUxQ5GyGTIeWbaHgJ/hf0x6lpCJLGt/ODMh8x9we+gJ4a5JLaaKueRxaRJpTQX870+o0IlKbn786OUnNjS9DizbW5hFxZc1awS2zze01b8HmpdbmsZCKLGlcFaXw6W/N7b5jIOYya/OIWKlZiHmxMJhTQu/PsTSOiJym+BAsmgAY0PceuDjF6kQirq/LtTD4IXN7yUQo3GNtHouoyJLG9c0Mc+aZFhFw3Z+sTiNivZ63QLcbzGs9Pp0EdrvViUQEwDDgX7+BY3kQ3t28jlJE6uaaadA23rw0ZNEEsFdZnajRqciSxpO/8eSQqKEvmqeURZo6mw2GvQR+LWDX9yeH0oqItVb+H2xdDt7+cPtb5qKrIlI3Pn5w+9vgGwg7v4WvX7I6UaPTxTDSOKoqYclDYK80Z2bqcbPViURcR3B78/qs5Y9C2jTofA2EdrI6lUjTlbsS/veYuX39nyEyzto8ImfxctpWS99/8nXdzv5kWGcYNsNcwPvL6dCun7lwcROhM1nSOL55CfZkgn+weRZLMzOJ1DRwAnS8FMqPwSf3N8mhFSIu4Wg+fDTG/FLw4ttg4HirE4m4r/hR0O9ewIB/joPDOywO1HhUZEnD27UavnrB3L5xJgRFWZtHxBV5eUPKG+DX0hw2uOI1qxOJND1VFbDwHvM6rNY94ObX9aWgyIW64QWI6gulR2DB3VBx3OpEjUJFljSssmOwaDwYVRB3B8TdbnUiEdfVquPJ2QY/fxb2rbM2j0hT89/HIDcD/INg5Ifg38LqRCLuz8cf7nwfmodB3jpYOtmcWMbDqciShvXfqXB4OwS1h6FN76JHkXqL/5V53aK9wvyCorzY6kQiTcPqt2DV/5nbt/6feT2JiDhHSLQ5EYbNC374x8m15zyYiixpOOs/hqz3ARvcOsdcE0hEzs1mg5teNZc5OJAD//5dk/jGT8RSP34Gyx4xt695HGKHWptHxBNddJU5dBAg/WnYuNjKNA1ORZY0jP2bzdkEAS5PhU6XW5tHxJ0Ehtf8xi/rfasTiXiu/I3w0b3msPbev4LLf291IhHPNXA8JNxvbn/ya9idaW2eBqQiS5yvtAgW3AUVJea3Flc/ZnUiEfcTcxlcO83cXvYI7PvB2jwinqhoL8wbAeVHoeNl5llkTXQh0rCS/wJdk6GyFP4xEg79ZHWiBqEiS5zLMOBfv4FDP0JQOxj+ljlrmojU3+BJ0G0IVJWZU0qXFFidSMRzlBTAB7dC4S4I6wIjPjAXUBWRhuXlbS7wHRkHxfvhgxTzCw8PoyJLnOubl2Dzp+Dla84kExhudSIR9+XlZV7PGNLBXFvkozFQWW51KhH3V3YU/j7cvO6xZRTc/Qk0D7U6lUjT4d8SRv8TWnWCI7nwwW0e90WiiixxnvUfw+d/NreHvgDt+1ubR8QTNGsFo+ab62ft+Ab+3TSmvhVpMBWl8I9RsDcLmoXCmMXmFxki0rhaRpj//7VsCwc2w4d3mF+AeAgVWeIcOzNg8QPmduJE6H+ftXlEPEnExXDHO+ZEGGv/Dt+9anUiEfdUXmJeA7LjG/OLi7v+Ca27W51KpOlqFWOeSW7WCvasMc9olRZZncopVGTJhTv0E8z/FVSVm+v7XPeM1YlEPE/X62DIc+b2Z0/Bhn9aGkfE7ZQXw7w74ecvwDcQfjUf2vW1OpWItOkBdy2CgGDYvcq8VvL4EatTXTAVWXJhjuyC91PgeAFE9YXb3jSvIxER50v4NQycABiwaAJs/a/ViUTcQ9lR+PvtJ85gtTDPYMVcZnUqEanWri/c8+kpZ7RS3P4aLf01LOfvaB68fzMU5kJoZ/jVAvBrbnUqEc825DmIuwPsleZEGNu/sTqRiGs7dgDeuxlyV4B/ENy9GDomWp1KRE7XtrdZaDUPg71r4Z2hULjb6lTnTUWWnJ/ig/D+LVDws3nB8D1LoEUbq1OJeD4vb0h5A7oPPbnGyK7VVqcScU2HfoK3rjtlkot/QfQAq1OJyNlExsE9S09OhvG36yB/k9WpzouKLKm/o/nmt4LVU9+OWQLB7a1OJdJ0ePvC7e9Apyug/Jg5rEJntERq2p0Jb10Ph7dDSEcYl6ZrsETcQURP8//X8O5wdC+8PcQt+zgVWVI/h3fA28mwfyO0iDDPYIV2sjqVSNPjGwAj/3Gy0Prwdtj6P6tTibiGH+bDu0Oh5CC0jYf/9xmEd7E6lYjUVUg03LccogdBWaH5ZeLqv7nVEiYqsqTu9m82v02o/lbwvuUQ3tXqVCJNl38L+NVC6HaDOXRw/ijNOihNW1UlLP8jfPJr8/+Jrslw7781nF3EHTU/sY5dr+Hmdcj//h18Ogkqy6xOVicqsqRufv7KLLCO7oM2PeG+/0LoRVanEhHfABjxAfS63eyEPr4Pvn7Jrb7tE3GKo/nw91vh+9nmv694xFzI27+FtblE5Pz5NoPhb0HS04ANst6Dd4fBkVyrk/0iH6sDiBtY9Sb8ZwoYVdB+oDmLYPNQq1OJNKqX07Za+v6Tr+t29ie9feG2uRAYDivnwOfPwMFtcPNr4OPfeCFFrLL1f7D4AXN4oG8g3PoG9LzF6lQi4gw2G1z2MET0Mr9I3L0a5lwGN7/u0v+fq8iSs6ssh+VTYM3b5r8vGQk3vWp+c24Bq//IFXFpXt5ww/PmEN5lf4B1883ZP+94F4LbWZ1OpGFUlEL60/D9X81/R/SC29+G1t2tzSUiztc1Ce7/Gj4eZ66l9dEY6HcvJE93ySWENFxQalc97e2atwGbeZr21jmWFVgiUkcD/h/c9TH4B8PuVea3fVq0WDzRzgyYc+nJAivhfvh/6SqwRDxZqxhzToDLJgM2yHwXsj+0OFTtdCZLzvTDfPPiwvJj5srbt/4fdEu2OpWI1FXna2DCF/DxWNj3A8y7ExInwrXTNHxQ3F/ZUUj/kzmUHcOc6fam16D7EKuTiTQ51o0y+hUdLu7McNsX0P8+izKcm85kyUlH8+Gje8xZmcqPQcdL4f7vVGCJuKOwzuY6IwN/bf47Yxb83xWwe421uUTOl90O2fPg9X6wai5gQJ+74MGVKrBEmqDckARzeLCXt9VRaqUzWWLOQrb27/C/x6C0EGzecOUUuOL3LvuLKyJ14OMPQ18w19L6dJK5gPhb18Gg38DVj7nkGHaRWu1aBcsfhT2Z5r9bdYIbX4bOV1ubS0TkLFRkNXV718J/H4Od35n/btvbnK2lbW9rc4mI8/S4ETokmn+krv/IPKu1cTFc97S5/ojNZnVCkdrtWwef/xm2nbiu0K+FOTX7oAc09FVEXJqKrKaqcI85zfMP/zD/7dMMrv6j+Q23t34tRDxOYBgMf9MsqpY9AoW58M9xsPL/IPlZiB5odUKRk/ZkwrevwOYl5r9t3hA/Cq55AlpGWhpNRKQu9Nd0U3NkF3z3CmR9AFUnVsy+ZCRc+wQEt7c0mog0gu5D4KIrzbNZ37xszkD41nXQJQmufBSiB1idUJoqux1+/AxWvAY7vjnxoM38YuCqqRDexdJ4IiL1oSKrqdi/2ZzmNvsfYK8wH+t4KVz/Z2jX19psItK4fJuZQ67i74IvnjUnE/jxM/N20dXmGe0uSeCluZGkERw7ANl/N6diPrzDfMzLB3rdDpdOgoieVqYTETkvKrI8WVUFbF1uDgdyfCsIxFxuTmwRc5muxRBpyoLawi2z4PLfwTcvmV/C/PyFeQvrAgMnQNwd0DzU6qTiaSpKzf5p/UJzHbfqL//8g6Hv3eY1VxpdISJuTEWWpzEMc12cH+abnVfJQfNxmxfEDjPXyukwyNqMIuJaQjvBLbPNs1ur3oSs9+HQj/CfP8D/HofuQyF+tLn+lq7ZlPNVXgI/pUPOvyFnGZQVnnyuXT9zrZuLb9OslyLiEdRbegK7Hfasgc2fmp1XwU8nnwtsDX3uNjuvkGjrMorIBWm0BR+97sE3/nZ6Hvg3cXmLaV2yDTYtNm+9bofb32qcHOL+DAMO/QQ/fW4WVz9/BZXHTz4f1A7iboe4OyGyl3U5RUQagIosd3Uk1+ywfv7CvK8+YwXg7W+eteo9ylxDxNvXupwi4nYqfAL5oe2d/BB5B62Lt3Lx/k/pcyQNYodaHU1cmd0OB7dC7grYmQG5GVC4q2abkA4Qe5PZR3VI9Ijr/hrtCxARcSsqstyAT9VxWhdvhe8/h10rzVvRnpqN/IOg6/XmejhdksC/pTVhRcRz2GwcaNGdL1t0p8/Vs3QNpzh42StpdXwH4SU/wv8+MNdc3JsN5UdrNvT2M4eod77G7Jsieun3SESahPMqsmbPns2LL75IXl4evXv35vXXX2fgwLOvsbJw4UKeeOIJduzYQdeuXXn++ecZOvTkN6KGYfDkk0/y5ptvcuTIES699FLeeOMNunbt6mhTUFDAQw89xKeffoqXlxfDhw/n1VdfpUWLFudzCC7Jt7KYkNLdtCrNJbRkO2El2wkr+ZHQ4zuxYcD6UxrbvM1ZAS+62jxb1a4/+PhZll1EPJwbfL6ob3Iyw05g+UFCSvcQXLaHkOO5hB7fSasTN2+j0mx36okcn2bQvr95lqpjIkQngF9gg8bUmSQRcUX1LrIWLFhAamoqc+bMISEhgVdeeYXk5GS2bNlCmzZtzmi/YsUKRo0axfTp07nxxhuZN28eKSkpZGVl0auXOQb7hRde4LXXXuO9996jU6dOPPHEEyQnJ7Np0yYCAgIAGD16NPv27SMtLY2KigrGjh3LhAkTmDdv3gX+CBqBYeBXVUyzisO0KD9I84pDBJYfpGVZPi3L8mhZnk9w6R4CKwrOuotjfq1p0bEvtB8AHRLMi4QbuOM6nToyEXFV6pvqzsteQbOKIzSrPEzzisM0qzhMYPkhAisOEVh+gBZl+2lZnk+Lsv34GBVn3U+ZdyCHmncmqnt/iOoDUX2hdawmRxERAWyGYRj1eUFCQgIDBgxg1qxZANjtdqKjo3nooYd49NFHz2g/YsQIiouLWbp0qeOxQYMGER8fz5w5czAMg6ioKH73u9/x+9//HoDCwkIiIiJ49913GTlyJJs3b6Znz56sXr2a/v37A7B8+XKGDh3K7t27iYqK+sXcRUVFBAcHU1hYSFBQUH0O2bR/Mwu+WY9PVSk+9jJ87aX4Vh3H134cn6pS/KqK8asqxr+qGP/Ko/hXHiWgsujE7QjeRlWd3ua4TzCHm3WgoFknDjXvREHzTuwP7E6JXziTr+tW/9xOpCJLpOm6kM+fC/78rYMm2TeVl8DW//Df7O0n+qRSfO3H8a0qwbfqOH5VJfhVHcO/8hh+VcWOPsnXXlrnt7DjzVH/CAoDojgS0IGC5h053KwjBc06UeTfFmw29U0iYhlX7pvq9XVTeXk5mZmZTJ061fGYl5cXSUlJZGRk1PqajIwMUlNTazyWnJzM4sWLAdi+fTt5eXkkJSU5ng8ODiYhIYGMjAxGjhxJRkYGISEhjk4MICkpCS8vL1auXMmtt956xvuWlZVRVlbm+HdhoTlVbFFRUX0O+aQP7uGGgzn1flkVUHxiu9wWQLFfGMf9QinxCeOof2uO+bfhmG8bigIiKQxoR4V3LUNMKoCKY0xfnHV+2UVELtB5f3ae8tp6fqdXZ022bzp2AD4cS2IdmxvA8RM3O14c9wmmzCeIEt9WlPiGUewXeuI+nKP+bTjm14Ziv3AMWy1/KlQBJcXnn92JSouPWfr+ImIdV+6b6lVkHTx4kKqqKiIiImo8HhERQU5O7QVIXl5ere3z8vIcz1c/dq42pw/38PHxITQ01NHmdNOnT+fpp58+4/HoaCunMT8KHLDw/UVEzs8fnbCPo0ePEhwc7IQ91aS+6XwV/nKTOnDG74aIyPlwxufPoUOHGqRv8tiB01OnTq3xLaXdbqegoICwsDBs5zGzUVFREdHR0ezatavBhrs0Nk87Jk87HvC8Y/K04wHPO6aGOB7DMDh69Gidhs95OvVN9aPjc286Pvfm6cdXWFhIhw4dCA0NbZD916vICg8Px9vbm/z8/BqP5+fnExkZWetrIiMjz9m++j4/P5+2bdvWaBMfH+9os3///hr7qKyspKCg4Kzv6+/vj7+/f43HQkJCzn2AdRAUFORxv2iedkyedjzgecfkaccDnndMzj6ehviWsJr6Js/7/Tudjs+96fjcm6cfn1cDrddXr736+fnRr18/0tPTHY/Z7XbS09NJTKx9VHhiYmKN9gBpaWmO9p06dSIyMrJGm6KiIlauXOlok5iYyJEjR8jMzHS0+fzzz7Hb7SQkJNTnEERExMOobxIREZdj1NP8+fMNf39/49133zU2bdpkTJgwwQgJCTHy8vIMwzCMu+++23j00Ucd7b/77jvDx8fHeOmll4zNmzcbTz75pOHr62usX7/e0ea5554zQkJCjH/961/GunXrjFtuucXo1KmTcfz4cUebIUOGGH369DFWrlxpfPvtt0bXrl2NUaNG1Tf+eSssLDQAo7CwsNHes6F52jF52vEYhucdk6cdj2F43jG56/Gob3Kv/151peNzbzo+96bjuzD1LrIMwzBef/11o0OHDoafn58xcOBA4/vvv3c8d+WVVxr33HNPjfYfffSR0a1bN8PPz8+4+OKLjX//+981nrfb7cYTTzxhREREGP7+/sa1115rbNmypUabQ4cOGaNGjTJatGhhBAUFGWPHjjWOHj16PvHPS2lpqfHkk08apaWljfaeDc3TjsnTjscwPO+YPO14DMPzjsmdj0d9k+fR8bk3HZ970/FdmHqvkyUiIiIiIiJn1zBXeomIiIiIiDRRKrJEREREREScSEWWiIiIiIiIE6nIEhERERERcSIVWXU0e/ZsYmJiCAgIICEhgVWrVlkdqU6mT5/OgAEDaNmyJW3atCElJYUtW7bUaFNaWsqDDz5IWFgYLVq0YPjw4Wcs0umqnnvuOWw2Gw8//LDjMXc8nj179nDXXXcRFhZGs2bNiIuLY82aNY7nDcNg2rRptG3blmbNmpGUlMS2bdssTHx2VVVVPPHEE3Tq1IlmzZrRuXNnnnnmGU6dY8fVj+frr7/mpptuIioqCpvNxuLFi2s8X5f8BQUFjB49mqCgIEJCQhg3bhzHjh1rxKOo6VzHVFFRwZQpU4iLiyMwMJCoqCjGjBnD3r17a+zD1Y6pqXPXful0nt5Pnc5T+q1TeVIfdjpP6NNO54l93Klcpr9rkDkLPcz8+fMNPz8/4+233zY2btxojB8/3ggJCTHy8/OtjvaLkpOTjXfeecfYsGGDkZ2dbQwdOtTo0KGDcezYMUeb+++/34iOjjbS09ONNWvWGIMGDTIGDx5sYeq6WbVqlRETE2NccsklxqRJkxyPu9vxFBQUGB07djTuvfdeY+XKlcbPP/9s/Pe//zV+/PFHR5vnnnvOCA4ONhYvXmz88MMPxs0333zGej2u4tlnnzXCwsKMpUuXGtu3bzcWLlxotGjRwnj11VcdbVz9eJYtW2Y89thjxqJFiwzA+OSTT2o8X5f8Q4YMMXr37m18//33xjfffGN06dKlUddPOt25junIkSNGUlKSsWDBAiMnJ8fIyMgwBg4caPTr16/GPlztmJoyd+6XTufJ/dTpPKXfOpWn9WGn84Q+7XSe2MedylX6OxVZdTBw4EDjwQcfdPy7qqrKiIqKMqZPn25hqvOzf/9+AzC++uorwzDMXzZfX19j4cKFjjabN282ACMjI8OqmL/o6NGjRteuXY20tDTjyiuvdHRW7ng8U6ZMMS677LKzPm+3243IyEjjxRdfdDx25MgRw9/f3/jHP/7RGBHrZdiwYcZ9991X47HbbrvNGD16tGEY7nc8p39A1yX/pk2bDMBYvXq1o81//vMfw2azGXv27Gm07GdTW6d6ulWrVhmAsXPnTsMwXP+YmhpP6pdO5yn91Ok8qd86laf1YafztD7tdJ7Yx53Kyv5OwwV/QXl5OZmZmSQlJTke8/LyIikpiYyMDAuTnZ/CwkIAQkNDAcjMzKSioqLG8cXGxtKhQweXPr4HH3yQYcOG1cgN7nk8S5YsoX///txxxx20adOGPn368Oabbzqe3759O3l5eTWOKTg4mISEBJc8psGDB5Oens7WrVsB+OGHH/j222+54YYbAPc7ntPVJX9GRgYhISH079/f0SYpKQkvLy9WrlzZ6JnPR2FhITabjZCQEMAzjslTeFq/dDpP6adO50n91qk8rQ87naf3aadrKn3cqRqqv/NxdlBPc/DgQaqqqoiIiKjxeEREBDk5ORalOj92u52HH36YSy+9lF69egGQl5eHn5+f4xerWkREBHl5eRak/GXz588nKyuL1atXn/GcOx7Pzz//zBtvvEFqaip//OMfWb16Nb/97W/x8/PjnnvuceSu7XfQFY/p0UcfpaioiNjYWLy9vamqquLZZ59l9OjRAG53PKerS/68vDzatGlT43kfHx9CQ0Pd4hhLS0uZMmUKo0aNIigoCHD/Y/IkntQvnc5T+qnTeVq/dSpP68NO5+l92umaQh93qobs71RkNSEPPvggGzZs4Ntvv7U6ynnbtWsXkyZNIi0tjYCAAKvjOIXdbqd///785S9/AaBPnz5s2LCBOXPmcM8991icrv4++ugjPvzwQ+bNm8fFF19MdnY2Dz/8MFFRUW55PE1NRUUFd955J4Zh8MYbb1gdR5oYT+inTueJ/dapPK0PO536NM/V0P2dhgv+gvDwcLy9vc+Y5Sc/P5/IyEiLUtXfxIkTWbp0KV988QXt27d3PB4ZGUl5eTlHjhyp0d5Vjy8zM5P9+/fTt29ffHx88PHx4auvvuK1117Dx8eHiIgItzoegLZt29KzZ88aj/Xo0YPc3FwAR253+R185JFHePTRRxk5ciRxcXHcfffdTJ48menTpwPudzynq0v+yMhI9u/fX+P5yspKCgoKXPoYqzucnTt3kpaW5vhWD9z3mDyRp/RLp/OUfup0nthvncrT+rDTeXqfdjpP7uNO1Rj9nYqsX+Dn50e/fv1IT093PGa320lPTycxMdHCZHVjGAYTJ07kk08+4fPPP6dTp041nu/Xrx++vr41jm/Lli3k5ua65PFde+21rF+/nuzsbMetf//+jB492rHtTscDcOmll54xXfHWrVvp2LEjAJ06dSIyMrLGMRUVFbFy5UqXPKaSkhK8vGp+tHh7e2O32wH3O57T1SV/YmIiR44cITMz09Hm888/x263k5CQ0OiZ66K6w9m2bRufffYZYWFhNZ53x2PyVO7eL53O0/qp03liv3UqT+vDTufpfdrpPLWPO1Wj9Xf1nqajCZo/f77h7+9vvPvuu8amTZuMCRMmGCEhIUZeXp7V0X7RAw88YAQHBxtffvmlsW/fPsetpKTE0eb+++83OnToYHz++efGmjVrjMTERCMxMdHC1PVz6ixNhuF+x7Nq1SrDx8fHePbZZ41t27YZH374odG8eXPj73//u6PNc889Z4SEhBj/+te/jHXr1hm33HKLy04Pe8899xjt2rVzTHe7aNEiIzw83PjDH/7gaOPqx3P06FFj7dq1xtq1aw3AmDlzprF27VrHzEN1yT9kyBCjT58+xsqVK41vv/3W6Nq1q6XT257rmMrLy42bb77ZaN++vZGdnV3js6KsrMxlj6kpc+d+6XRNoZ86nbv3W6fytD7sdJ7Qp53OE/u4U7lKf6ciq45ef/11o0OHDoafn58xcOBA4/vvv7c6Up0Atd7eeecdR5vjx48bv/nNb4xWrVoZzZs3N2699VZj37591oWup9M7K3c8nk8//dTo1auX4e/vb8TGxhpz586t8bzdbjeeeOIJIyIiwvD39zeuvfZaY8uWLRalPbeioiJj0qRJRocOHYyAgADjoosuMh577LEaH16ufjxffPFFrf/f3HPPPYZh1C3/oUOHjFGjRhktWrQwgoKCjLFjxxpHjx614GhM5zqm7du3n/Wz4osvvnDZY2rq3LVfOl1T6KdO5wn91qk8qQ87nSf0aafzxD7uVK7S39kM45Qlq0VEREREROSC6JosERERERERJ1KRJSIiIiIi4kQqskRERERERJxIRZaIiIiIiIgTqcgSERERERFxIhVZIiIiIiIiTqQiS0RERERExIlUZImIiIiIiDiRiiwREREREREnUpElIiIiIiLiRCqyREREREREnEhFloiIiIiIiBP9f5z2cM4ukP+MAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def compare_distributions(list1, list2):\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n",
        "\n",
        "    # Plot histograms\n",
        "    ax1.hist(list1, density=True, alpha=0.5)\n",
        "    ax2.hist(list2, density=True, alpha=0.5)\n",
        "\n",
        "    # Fit normal distributions\n",
        "    mu1, std1 = norm.fit(list1)\n",
        "    mu2, std2 = norm.fit(list2)\n",
        "\n",
        "    # Plot normal distribution curves\n",
        "    x1 = np.linspace(min(list1), max(list1), 100)\n",
        "    ax1.plot(x1, norm.pdf(x1, mu1, std1))\n",
        "    ax1.set_title('List 1')\n",
        "    ax2.set_title('List 2')\n",
        "    x2 = np.linspace(min(list2), max(list2), 100)\n",
        "    ax2.plot(x2, norm.pdf(x2, mu2, std2))\n",
        "\n",
        "    # Check if distributions are similar\n",
        "    if np.allclose(mu1, mu2) and np.allclose(std1, std2):\n",
        "        print('The distributions are similar.')\n",
        "    else:\n",
        "        print('The distributions are not similar.')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "compare_distributions(list(df[numerical_cols[2]]), list(new_df[numerical_cols[2]]))"
      ],
      "id": "0o2tCRcLOTao"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8av1-IvPy-t"
      },
      "source": [
        "It seems like the criteria for determining if the distributions are similar is too strict, especially given that we are working off a small dataset."
      ],
      "id": "_8av1-IvPy-t"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "jtmfU3j2P9sA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "29035752-3cc3-4dc4-fb2d-22e94e4563a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL DIVERGENCE:  0.06242586714580429\n",
            "The distributions are similar\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_distributions_tolerance(list1, list2, tolerance=0.07):\n",
        "    \"\"\"Compares the distributions of two lists and plots them side by side.\n",
        "\n",
        "    Args:\n",
        "        list1 (list): The first list to compare.\n",
        "        list2 (list): The second list to compare.\n",
        "        tolerance (float): The tolerance for accepting similarity between the distributions.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the distributions are similar, False otherwise.\n",
        "    \"\"\"\n",
        "    # Calculate the histograms for the two lists\n",
        "    hist1, bins1 = np.histogram(list1, density=True)\n",
        "    hist2, bins2 = np.histogram(list2, density=True)\n",
        "\n",
        "    # Normalize the histograms to have unit area\n",
        "    hist1 = hist1 / np.sum(hist1)\n",
        "    hist2 = hist2 / np.sum(hist2)\n",
        "\n",
        "    # Calculate the KL divergence between the two histograms\n",
        "    kl_div = stats.entropy(hist2, hist1)\n",
        "\n",
        "    # Plot the histograms side by side\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    ax[0].bar(bins1[:-1], hist1, width=np.diff(bins1), align='edge')\n",
        "    ax[1].bar(bins2[:-1], hist2, width=np.diff(bins2), align='edge')\n",
        "    ax[0].set_title('Original Dataset')\n",
        "    ax[1].set_title('Augmented Dataset')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Check if the KL divergence is within the tolerance\n",
        "    print(\"KL DIVERGENCE: \", kl_div)\n",
        "    similar = True if kl_div <= tolerance else False\n",
        "\n",
        "    if similar:\n",
        "      print(\"The distributions are similar\")\n",
        "    else:\n",
        "      print(\"The distrubutions are NOT similar\")\n",
        "    return similar\n",
        "\n",
        "\n",
        "compare_distributions_tolerance(list(df[numerical_cols[2]]), list(new_df[numerical_cols[2]]))\n",
        "# new_df.to_csv('augmented_dataset.csv', index=False)"
      ],
      "id": "jtmfU3j2P9sA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PUT IT ALL TOGETHER"
      ],
      "metadata": {
        "id": "W3ptvFuiWa7k"
      },
      "id": "W3ptvFuiWa7k"
    },
    {
      "cell_type": "code",
      "source": [
        "# def prepare_for_regression(df, categorical_cols, proper_noun_cols=None):\n",
        "#   regression_df = df.drop(columns=proper_noun_cols)\n",
        "#   regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "#   return regression_df\n",
        "\n",
        "# def separate_columns(df):\n",
        "#   numerical_cols = list()\n",
        "#   categorical_cols = list()\n",
        "#   proper_noun_cols = list()\n",
        "#   for (index, colname) in enumerate(df):\n",
        "#       if colname in df.select_dtypes(include='object').columns:\n",
        "#         unique_vals = df[colname].unique()\n",
        "#         if len(unique_vals) >= 0.85 * df.shape[0]:\n",
        "#           proper_noun_cols.append(colname)\n",
        "#         else:\n",
        "#           categorical_cols.append(colname)\n",
        "#       else:\n",
        "#         numerical_cols.append(colname)\n",
        "#   return numerical_cols, categorical_cols, proper_noun_cols\n",
        "\n",
        "# def run_data_augmenter(dataset, num_samples=None, percentage_augmentation=None):\n",
        "#     df = pd.read_csv(dataset, header=\"infer\")\n",
        "#     print(df.info())\n",
        "#     print(df)\n",
        "\n",
        "\n",
        "#     # DATA PREPROCESSING\n",
        "#     df = df.dropna(axis=1, how='all')\n",
        "#     # df = df.dropna()\n",
        "#     df = df.fillna(0)\n",
        "#     new_data = 0\n",
        "#     if num_samples != None:\n",
        "#       new_data = num_samples\n",
        "#     else:\n",
        "#       new_data = int(df.shape[0] * percentage_augmentation)\n",
        "    \n",
        "#     numerical_cols, categorical_cols, _ = separate_columns(df)\n",
        "#     regression_df = prepare_for_regression(df, categorical_cols)\n",
        "    \n",
        "#     np.seterr(invalid='warn') \n",
        "#     np.seterr(under='warn')\n",
        "    \n",
        "#     new_df = augment_dataset(regression_df, new_data)\n",
        "#     new_df = reverse_one_hot_encode(new_df, categorical_cols)\n",
        "\n",
        "    \n",
        "#     num_cat_generated = generate_categorical_variables(df, new_df)\n",
        "#     # final_df.head()\n",
        "\n",
        "#     final_df = pd.concat([df, num_cat_generated], axis=0)\n",
        "\n",
        "#     # make sure that all columns are rounded appropriately\n",
        "#     for col in final_df.columns:\n",
        "#       if col in numerical_cols:\n",
        "#         # print(col)\n",
        "#         # all(print(x) for x in col)\n",
        "#         if all((x*1.0).is_integer() for x in df[col]):\n",
        "#           final_df[col] = np.round(final_df[col], 0)\n",
        "#           # print(final_df[col])\n",
        "#           print(final_df[col].value_counts())\n",
        "    \n",
        "#     return final_df"
      ],
      "metadata": {
        "id": "3wxGi_QLUdV6"
      },
      "id": "3wxGi_QLUdV6",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkDfuWoz7FJ"
      },
      "source": [
        "##Downstream ML task\n",
        "\n",
        "To demonstrate the usefulness of this dataset augmentation, we're going to be predicting the diabetes diagnoses of patients."
      ],
      "id": "okkDfuWoz7FJ"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "8j2Gng6G0RcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d29ad99-6412-4796-c4fc-4f8553219a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "609            1      111             62             13      182  24.0   \n",
            "610            3      106             54             21      158  30.9   \n",
            "611            3      174             58             22      194  32.9   \n",
            "612            7      168             88             42      321  38.2   \n",
            "613            6      105             80             28        0  32.5   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "609                     0.138   23        0  \n",
            "610                     0.292   24        0  \n",
            "611                     0.593   36        1  \n",
            "612                     0.787   40        1  \n",
            "613                     0.878   26        0  \n",
            "\n",
            "[614 rows x 9 columns]\n",
            "Epoch 1/150\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 12.9732 - accuracy: 0.3583 - precision_3: 0.3157 - recall_3: 0.7277\n",
            "Epoch 2/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 3.4489 - accuracy: 0.4837 - precision_3: 0.2570 - recall_3: 0.2582\n",
            "Epoch 3/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 2.7109 - accuracy: 0.5033 - precision_3: 0.3034 - recall_3: 0.3333\n",
            "Epoch 4/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 2.1148 - accuracy: 0.5261 - precision_3: 0.3227 - recall_3: 0.3333\n",
            "Epoch 5/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5575 - accuracy: 0.5749 - precision_3: 0.3824 - recall_3: 0.3662\n",
            "Epoch 6/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2904 - accuracy: 0.5782 - precision_3: 0.3905 - recall_3: 0.3850\n",
            "Epoch 7/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1622 - accuracy: 0.6075 - precision_3: 0.4307 - recall_3: 0.4085\n",
            "Epoch 8/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1136 - accuracy: 0.6189 - precision_3: 0.4426 - recall_3: 0.3803\n",
            "Epoch 9/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9635 - accuracy: 0.6173 - precision_3: 0.4461 - recall_3: 0.4272\n",
            "Epoch 10/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9063 - accuracy: 0.6303 - precision_3: 0.4628 - recall_3: 0.4085\n",
            "Epoch 11/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9320 - accuracy: 0.6221 - precision_3: 0.4492 - recall_3: 0.3944\n",
            "Epoch 12/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7846 - accuracy: 0.6401 - precision_3: 0.4792 - recall_3: 0.4319\n",
            "Epoch 13/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7808 - accuracy: 0.6384 - precision_3: 0.4772 - recall_3: 0.4413\n",
            "Epoch 14/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8132 - accuracy: 0.6287 - precision_3: 0.4603 - recall_3: 0.4085\n",
            "Epoch 15/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7398 - accuracy: 0.6612 - precision_3: 0.5145 - recall_3: 0.4178\n",
            "Epoch 16/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7009 - accuracy: 0.6531 - precision_3: 0.5000 - recall_3: 0.4131\n",
            "Epoch 17/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6758 - accuracy: 0.6580 - precision_3: 0.5083 - recall_3: 0.4319\n",
            "Epoch 18/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6753 - accuracy: 0.6645 - precision_3: 0.5189 - recall_3: 0.4507\n",
            "Epoch 19/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6650 - accuracy: 0.6515 - precision_3: 0.4971 - recall_3: 0.3991\n",
            "Epoch 20/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.6547 - precision_3: 0.5031 - recall_3: 0.3850\n",
            "Epoch 21/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.6775 - precision_3: 0.5429 - recall_3: 0.4460\n",
            "Epoch 22/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7346 - accuracy: 0.6678 - precision_3: 0.5249 - recall_3: 0.4460\n",
            "Epoch 23/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6324 - accuracy: 0.6971 - precision_3: 0.5860 - recall_3: 0.4319\n",
            "Epoch 24/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.6629 - precision_3: 0.5161 - recall_3: 0.4507\n",
            "Epoch 25/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6533 - accuracy: 0.6889 - precision_3: 0.5705 - recall_3: 0.4178\n",
            "Epoch 26/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6415 - accuracy: 0.6596 - precision_3: 0.5123 - recall_3: 0.3897\n",
            "Epoch 27/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6217 - accuracy: 0.6987 - precision_3: 0.5833 - recall_3: 0.4601\n",
            "Epoch 28/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6300 - accuracy: 0.6808 - precision_3: 0.5548 - recall_3: 0.4038\n",
            "Epoch 29/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6136 - accuracy: 0.6873 - precision_3: 0.5686 - recall_3: 0.4085\n",
            "Epoch 30/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.6808 - precision_3: 0.5480 - recall_3: 0.4554\n",
            "Epoch 31/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5928 - accuracy: 0.6971 - precision_3: 0.5818 - recall_3: 0.4507\n",
            "Epoch 32/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6338 - accuracy: 0.6759 - precision_3: 0.5427 - recall_3: 0.4178\n",
            "Epoch 33/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.6873 - precision_3: 0.5614 - recall_3: 0.4507\n",
            "Epoch 34/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6019 - accuracy: 0.6938 - precision_3: 0.5817 - recall_3: 0.4178\n",
            "Epoch 35/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.6824 - precision_3: 0.5577 - recall_3: 0.4085\n",
            "Epoch 36/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6641 - accuracy: 0.6873 - precision_3: 0.5629 - recall_3: 0.4413\n",
            "Epoch 37/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.7052 - precision_3: 0.6039 - recall_3: 0.4366\n",
            "Epoch 38/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.7166 - precision_3: 0.6242 - recall_3: 0.4601\n",
            "Epoch 39/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6566 - accuracy: 0.7036 - precision_3: 0.5987 - recall_3: 0.4413\n",
            "Epoch 40/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7199 - precision_3: 0.6258 - recall_3: 0.4789\n",
            "Epoch 41/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5926 - accuracy: 0.6987 - precision_3: 0.5795 - recall_3: 0.4789\n",
            "Epoch 42/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.7036 - precision_3: 0.6099 - recall_3: 0.4038\n",
            "Epoch 43/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7231 - precision_3: 0.6483 - recall_3: 0.4413\n",
            "Epoch 44/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5817 - accuracy: 0.7052 - precision_3: 0.6081 - recall_3: 0.4225\n",
            "Epoch 45/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.7068 - precision_3: 0.6122 - recall_3: 0.4225\n",
            "Epoch 46/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5868 - accuracy: 0.7296 - precision_3: 0.6407 - recall_3: 0.5023\n",
            "Epoch 47/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6889 - precision_3: 0.5604 - recall_3: 0.4789\n",
            "Epoch 48/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5645 - accuracy: 0.7166 - precision_3: 0.6242 - recall_3: 0.4601\n",
            "Epoch 49/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5589 - accuracy: 0.7296 - precision_3: 0.6556 - recall_3: 0.4648\n",
            "Epoch 50/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.7231 - precision_3: 0.6443 - recall_3: 0.4507\n",
            "Epoch 51/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.7150 - precision_3: 0.6159 - recall_3: 0.4742\n",
            "Epoch 52/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.7182 - precision_3: 0.6235 - recall_3: 0.4742\n",
            "Epoch 53/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5638 - accuracy: 0.7280 - precision_3: 0.6620 - recall_3: 0.4413\n",
            "Epoch 54/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7296 - precision_3: 0.6460 - recall_3: 0.4883\n",
            "Epoch 55/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5624 - accuracy: 0.7313 - precision_3: 0.6412 - recall_3: 0.5117\n",
            "Epoch 56/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.7313 - precision_3: 0.6446 - recall_3: 0.5023\n",
            "Epoch 57/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5955 - accuracy: 0.7150 - precision_3: 0.6218 - recall_3: 0.4554\n",
            "Epoch 58/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5519 - accuracy: 0.7280 - precision_3: 0.6369 - recall_3: 0.5023\n",
            "Epoch 59/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5432 - accuracy: 0.7280 - precision_3: 0.6369 - recall_3: 0.5023\n",
            "Epoch 60/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5561 - accuracy: 0.7150 - precision_3: 0.6145 - recall_3: 0.4789\n",
            "Epoch 61/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5695 - accuracy: 0.7166 - precision_3: 0.6226 - recall_3: 0.4648\n",
            "Epoch 62/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.7378 - precision_3: 0.6688 - recall_3: 0.4836\n",
            "Epoch 63/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.7248 - precision_3: 0.6294 - recall_3: 0.5023\n",
            "Epoch 64/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.7150 - precision_3: 0.6234 - recall_3: 0.4507\n",
            "Epoch 65/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5686 - accuracy: 0.7215 - precision_3: 0.6250 - recall_3: 0.4930\n",
            "Epoch 66/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6069 - accuracy: 0.7020 - precision_3: 0.6000 - recall_3: 0.4225\n",
            "Epoch 67/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7394 - precision_3: 0.6532 - recall_3: 0.5305\n",
            "Epoch 68/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7166 - precision_3: 0.6291 - recall_3: 0.4460\n",
            "Epoch 69/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7296 - precision_3: 0.6497 - recall_3: 0.4789\n",
            "Epoch 70/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5395 - accuracy: 0.7410 - precision_3: 0.6667 - recall_3: 0.5070\n",
            "Epoch 71/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5194 - accuracy: 0.7280 - precision_3: 0.6554 - recall_3: 0.4554\n",
            "Epoch 72/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5414 - accuracy: 0.7459 - precision_3: 0.6629 - recall_3: 0.5446\n",
            "Epoch 73/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5706 - accuracy: 0.7313 - precision_3: 0.6579 - recall_3: 0.4695\n",
            "Epoch 74/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5545 - accuracy: 0.7296 - precision_3: 0.6497 - recall_3: 0.4789\n",
            "Epoch 75/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5244 - accuracy: 0.7394 - precision_3: 0.6667 - recall_3: 0.4977\n",
            "Epoch 76/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5614 - accuracy: 0.7345 - precision_3: 0.6562 - recall_3: 0.4930\n",
            "Epoch 77/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.7492 - precision_3: 0.6725 - recall_3: 0.5399\n",
            "Epoch 78/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5396 - accuracy: 0.7410 - precision_3: 0.6552 - recall_3: 0.5352\n",
            "Epoch 79/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5284 - accuracy: 0.7329 - precision_3: 0.6561 - recall_3: 0.4836\n",
            "Epoch 80/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5452 - accuracy: 0.7296 - precision_3: 0.6442 - recall_3: 0.4930\n",
            "Epoch 81/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5459 - accuracy: 0.7345 - precision_3: 0.6582 - recall_3: 0.4883\n",
            "Epoch 82/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5541 - accuracy: 0.7329 - precision_3: 0.6433 - recall_3: 0.5164\n",
            "Epoch 83/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5106 - accuracy: 0.7476 - precision_3: 0.6883 - recall_3: 0.4977\n",
            "Epoch 84/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5445 - accuracy: 0.7378 - precision_3: 0.6646 - recall_3: 0.4930\n",
            "Epoch 85/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5494 - accuracy: 0.7166 - precision_3: 0.6154 - recall_3: 0.4883\n",
            "Epoch 86/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5828 - accuracy: 0.7150 - precision_3: 0.6011 - recall_3: 0.5305\n",
            "Epoch 87/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6488 - accuracy: 0.7052 - precision_3: 0.6143 - recall_3: 0.4038\n",
            "Epoch 88/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7476 - precision_3: 0.6726 - recall_3: 0.5305\n",
            "Epoch 89/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.7166 - precision_3: 0.6102 - recall_3: 0.5070\n",
            "Epoch 90/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7362 - precision_3: 0.6735 - recall_3: 0.4648\n",
            "Epoch 91/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5441 - accuracy: 0.7410 - precision_3: 0.6776 - recall_3: 0.4836\n",
            "Epoch 92/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7280 - precision_3: 0.6353 - recall_3: 0.5070\n",
            "Epoch 93/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.7296 - precision_3: 0.6442 - recall_3: 0.4930\n",
            "Epoch 94/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7278 - accuracy: 0.6954 - precision_3: 0.5833 - recall_3: 0.4272\n",
            "Epoch 95/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7590 - precision_3: 0.7019 - recall_3: 0.5305\n",
            "Epoch 96/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5237 - accuracy: 0.7394 - precision_3: 0.6779 - recall_3: 0.4742\n",
            "Epoch 97/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5072 - accuracy: 0.7622 - precision_3: 0.7006 - recall_3: 0.5493\n",
            "Epoch 98/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.7296 - precision_3: 0.6599 - recall_3: 0.4554\n",
            "Epoch 99/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7362 - precision_3: 0.6584 - recall_3: 0.4977\n",
            "Epoch 100/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7524 - precision_3: 0.7075 - recall_3: 0.4883\n",
            "Epoch 101/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5552 - accuracy: 0.7427 - precision_3: 0.6647 - recall_3: 0.5211\n",
            "Epoch 102/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.7362 - precision_3: 0.6667 - recall_3: 0.4789\n",
            "Epoch 103/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.7671 - precision_3: 0.7108 - recall_3: 0.5540\n",
            "Epoch 104/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.7329 - precision_3: 0.6815 - recall_3: 0.4319\n",
            "Epoch 105/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7296 - precision_3: 0.6313 - recall_3: 0.5305\n",
            "Epoch 106/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5463 - accuracy: 0.7329 - precision_3: 0.6601 - recall_3: 0.4742\n",
            "Epoch 107/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7378 - precision_3: 0.6529 - recall_3: 0.5211\n",
            "Epoch 108/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5327 - accuracy: 0.7443 - precision_3: 0.6842 - recall_3: 0.4883\n",
            "Epoch 109/150\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5319 - accuracy: 0.7362 - precision_3: 0.6624 - recall_3: 0.4883\n",
            "Epoch 110/150\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5386 - accuracy: 0.7427 - precision_3: 0.6730 - recall_3: 0.5023\n",
            "Epoch 111/150\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5319 - accuracy: 0.7541 - precision_3: 0.6914 - recall_3: 0.5258\n",
            "Epoch 112/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6125 - accuracy: 0.7541 - precision_3: 0.6824 - recall_3: 0.5446\n",
            "Epoch 113/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7622 - precision_3: 0.7219 - recall_3: 0.5117\n",
            "Epoch 114/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7492 - precision_3: 0.6954 - recall_3: 0.4930\n",
            "Epoch 115/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7394 - precision_3: 0.6710 - recall_3: 0.4883\n",
            "Epoch 116/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7476 - precision_3: 0.6883 - recall_3: 0.4977\n",
            "Epoch 117/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7606 - precision_3: 0.7037 - recall_3: 0.5352\n",
            "Epoch 118/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7524 - precision_3: 0.6685 - recall_3: 0.5681\n",
            "Epoch 119/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7573 - precision_3: 0.7133 - recall_3: 0.5023\n",
            "Epoch 120/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7492 - precision_3: 0.6725 - recall_3: 0.5399\n",
            "Epoch 121/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.7215 - precision_3: 0.6296 - recall_3: 0.4789\n",
            "Epoch 122/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7622 - precision_3: 0.7161 - recall_3: 0.5211\n",
            "Epoch 123/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5169 - accuracy: 0.7378 - precision_3: 0.6831 - recall_3: 0.4554\n",
            "Epoch 124/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5653 - accuracy: 0.7345 - precision_3: 0.6344 - recall_3: 0.5540\n",
            "Epoch 125/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7443 - precision_3: 0.7029 - recall_3: 0.4554\n",
            "Epoch 126/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7345 - precision_3: 0.6524 - recall_3: 0.5023\n",
            "Epoch 127/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7557 - precision_3: 0.6957 - recall_3: 0.5258\n",
            "Epoch 128/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5561 - accuracy: 0.7231 - precision_3: 0.6303 - recall_3: 0.4883\n",
            "Epoch 129/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7427 - precision_3: 0.6752 - recall_3: 0.4977\n",
            "Epoch 130/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7622 - precision_3: 0.7055 - recall_3: 0.5399\n",
            "Epoch 131/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7573 - precision_3: 0.7133 - recall_3: 0.5023\n",
            "Epoch 132/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7541 - precision_3: 0.7039 - recall_3: 0.5023\n",
            "Epoch 133/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5018 - accuracy: 0.7410 - precision_3: 0.6627 - recall_3: 0.5164\n",
            "Epoch 134/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.7329 - precision_3: 0.6581 - recall_3: 0.4789\n",
            "Epoch 135/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.7541 - precision_3: 0.6742 - recall_3: 0.5634\n",
            "Epoch 136/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7459 - precision_3: 0.6815 - recall_3: 0.5023\n",
            "Epoch 137/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7801 - precision_3: 0.7566 - recall_3: 0.5399\n",
            "Epoch 138/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7427 - precision_3: 0.6608 - recall_3: 0.5305\n",
            "Epoch 139/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7443 - precision_3: 0.6750 - recall_3: 0.5070\n",
            "Epoch 140/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7638 - precision_3: 0.7208 - recall_3: 0.5211\n",
            "Epoch 141/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.7524 - precision_3: 0.6894 - recall_3: 0.5211\n",
            "Epoch 142/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.7752 - precision_3: 0.7451 - recall_3: 0.5352\n",
            "Epoch 143/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5018 - accuracy: 0.7427 - precision_3: 0.6871 - recall_3: 0.4742\n",
            "Epoch 144/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.7590 - precision_3: 0.7181 - recall_3: 0.5023\n",
            "Epoch 145/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.7573 - precision_3: 0.7000 - recall_3: 0.5258\n",
            "Epoch 146/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7492 - precision_3: 0.6810 - recall_3: 0.5211\n",
            "Epoch 147/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5218 - accuracy: 0.7362 - precision_3: 0.6584 - recall_3: 0.4977\n",
            "Epoch 148/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5641 - accuracy: 0.7248 - precision_3: 0.6294 - recall_3: 0.5023\n",
            "Epoch 149/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5864 - accuracy: 0.7264 - precision_3: 0.6552 - recall_3: 0.4460\n",
            "Epoch 150/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5145 - accuracy: 0.7573 - precision_3: 0.6758 - recall_3: 0.5775\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6240 - accuracy: 0.6688 - precision_3: 0.5526 - recall_3: 0.3818\n",
            "Accuracy: 66.88%\n",
            "Precision: 55.26%\n",
            "Recall: 38.18%\n"
          ]
        }
      ],
      "source": [
        "eval_original_df = df\n",
        "eval_augmented_df = final_df\n",
        "\n",
        "if 'id' in eval_original_df.columns:\n",
        "  eval_original_df = df.drop(columns=['id'])\n",
        "  eval_augmented_df = final_df.drop(columns=['id'])\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras.metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "# data = pd.read_csv('path/to/dataset.csv')\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "test_size = 0.2\n",
        "train_data, test_data = train_test_split(eval_original_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_data)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "\n"
      ],
      "id": "8j2Gng6G0RcV"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "woXDJmn21s5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e7fa39-86c3-414c-d933-e8a38ec018e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin        BMI  \\\n",
            "0            6.0    148.0           72.0           35.0      0.0  33.600000   \n",
            "1            1.0     85.0           66.0           29.0      0.0  26.600000   \n",
            "2            8.0    183.0           64.0            0.0      0.0  23.300000   \n",
            "3            1.0     89.0           66.0           23.0     94.0  28.100000   \n",
            "4            0.0    137.0           40.0           35.0    168.0  43.100000   \n",
            "..           ...      ...            ...            ...      ...        ...   \n",
            "455          2.0    107.0           68.0           19.0      3.0  30.090891   \n",
            "456         10.0    180.0           90.0           44.0    294.0  44.128829   \n",
            "457          2.0    100.0           64.0           13.0      1.0  27.874374   \n",
            "458          3.0    116.0           72.0           25.0     64.0  32.307407   \n",
            "459          8.0    161.0           86.0           40.0    207.0  40.568969   \n",
            "\n",
            "     DiabetesPedigreeFunction   Age  Outcome  \n",
            "0                    0.627000  50.0      1.0  \n",
            "1                    0.351000  31.0      0.0  \n",
            "2                    0.672000  32.0      1.0  \n",
            "3                    0.167000  21.0      0.0  \n",
            "4                    2.288000  33.0      1.0  \n",
            "..                        ...   ...      ...  \n",
            "455                  0.328845  27.0      0.0  \n",
            "456                  1.088412  57.0      1.0  \n",
            "457                  0.267892  25.0      0.0  \n",
            "458                  0.406208  30.0      0.0  \n",
            "459                  0.846945  49.0      1.0  \n",
            "\n",
            "[1228 rows x 9 columns]\n",
            "Epoch 1/150\n",
            "123/123 [==============================] - 1s 2ms/step - loss: 2.5314 - accuracy: 0.6963 - precision_4: 0.5757 - recall_4: 0.4965\n",
            "Epoch 2/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.6626 - accuracy: 0.7085 - precision_4: 0.5868 - recall_4: 0.5594\n",
            "Epoch 3/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.6003 - accuracy: 0.7296 - precision_4: 0.6228 - recall_4: 0.5734\n",
            "Epoch 4/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.7402 - precision_4: 0.6463 - recall_4: 0.5664\n",
            "Epoch 5/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7590 - precision_4: 0.6727 - recall_4: 0.6037\n",
            "Epoch 6/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7459 - precision_4: 0.6496 - recall_4: 0.5921\n",
            "Epoch 7/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7638 - precision_4: 0.6787 - recall_4: 0.6154\n",
            "Epoch 8/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7484 - precision_4: 0.6485 - recall_4: 0.6107\n",
            "Epoch 9/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7598 - precision_4: 0.6718 - recall_4: 0.6107\n",
            "Epoch 10/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4989 - accuracy: 0.7647 - precision_4: 0.6759 - recall_4: 0.6270\n",
            "Epoch 11/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7573 - precision_4: 0.6642 - recall_4: 0.6177\n",
            "Epoch 12/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.7834 - precision_4: 0.7128 - recall_4: 0.6364\n",
            "Epoch 13/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.7858 - precision_4: 0.7005 - recall_4: 0.6760\n",
            "Epoch 14/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7769 - precision_4: 0.6972 - recall_4: 0.6387\n",
            "Epoch 15/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.7793 - precision_4: 0.6965 - recall_4: 0.6527\n",
            "Epoch 16/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.7923 - precision_4: 0.7266 - recall_4: 0.6503\n",
            "Epoch 17/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7834 - precision_4: 0.7032 - recall_4: 0.6573\n",
            "Epoch 18/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.8062 - precision_4: 0.7406 - recall_4: 0.6853\n",
            "Epoch 19/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7907 - precision_4: 0.7299 - recall_4: 0.6364\n",
            "Epoch 20/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.7866 - precision_4: 0.7147 - recall_4: 0.6480\n",
            "Epoch 21/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7858 - precision_4: 0.7107 - recall_4: 0.6527\n",
            "Epoch 22/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.7875 - precision_4: 0.7090 - recall_4: 0.6643\n",
            "Epoch 23/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7964 - precision_4: 0.7266 - recall_4: 0.6690\n",
            "Epoch 24/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.7956 - precision_4: 0.7247 - recall_4: 0.6690\n",
            "Epoch 25/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4506 - accuracy: 0.8168 - precision_4: 0.7488 - recall_4: 0.7156\n",
            "Epoch 26/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.7989 - precision_4: 0.7264 - recall_4: 0.6807\n",
            "Epoch 27/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.8070 - precision_4: 0.7474 - recall_4: 0.6760\n",
            "Epoch 28/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4565 - accuracy: 0.7940 - precision_4: 0.7211 - recall_4: 0.6690\n",
            "Epoch 29/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4424 - accuracy: 0.8094 - precision_4: 0.7396 - recall_4: 0.7016\n",
            "Epoch 30/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.8029 - precision_4: 0.7429 - recall_4: 0.6667\n",
            "Epoch 31/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.8086 - precision_4: 0.7474 - recall_4: 0.6830\n",
            "Epoch 32/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8111 - precision_4: 0.7519 - recall_4: 0.6853\n",
            "Epoch 33/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4474 - accuracy: 0.8070 - precision_4: 0.7462 - recall_4: 0.6783\n",
            "Epoch 34/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.8176 - precision_4: 0.7635 - recall_4: 0.6923\n",
            "Epoch 35/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4323 - accuracy: 0.8078 - precision_4: 0.7506 - recall_4: 0.6737\n",
            "Epoch 36/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.8143 - precision_4: 0.7666 - recall_4: 0.6737\n",
            "Epoch 37/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4468 - accuracy: 0.8005 - precision_4: 0.7266 - recall_4: 0.6876\n",
            "Epoch 38/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.8062 - precision_4: 0.7393 - recall_4: 0.6876\n",
            "Epoch 39/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4370 - accuracy: 0.8086 - precision_4: 0.7500 - recall_4: 0.6783\n",
            "Epoch 40/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8241 - precision_4: 0.7696 - recall_4: 0.7086\n",
            "Epoch 41/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4230 - accuracy: 0.8200 - precision_4: 0.7574 - recall_4: 0.7133\n",
            "Epoch 42/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8184 - precision_4: 0.7588 - recall_4: 0.7040\n",
            "Epoch 43/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8119 - precision_4: 0.7578 - recall_4: 0.6783\n",
            "Epoch 44/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8119 - precision_4: 0.7538 - recall_4: 0.6853\n",
            "Epoch 45/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.8265 - precision_4: 0.7755 - recall_4: 0.7086\n",
            "Epoch 46/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.8168 - precision_4: 0.7670 - recall_4: 0.6830\n",
            "Epoch 47/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8208 - precision_4: 0.7772 - recall_4: 0.6830\n",
            "Epoch 48/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8135 - precision_4: 0.7513 - recall_4: 0.6970\n",
            "Epoch 49/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8111 - precision_4: 0.7599 - recall_4: 0.6713\n",
            "Epoch 50/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8200 - precision_4: 0.7613 - recall_4: 0.7063\n",
            "Epoch 51/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4155 - accuracy: 0.8192 - precision_4: 0.7647 - recall_4: 0.6970\n",
            "Epoch 52/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8241 - precision_4: 0.7724 - recall_4: 0.7040\n",
            "Epoch 53/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8274 - precision_4: 0.7848 - recall_4: 0.6970\n",
            "Epoch 54/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8176 - precision_4: 0.7649 - recall_4: 0.6900\n",
            "Epoch 55/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8363 - precision_4: 0.7953 - recall_4: 0.7156\n",
            "Epoch 56/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8208 - precision_4: 0.7646 - recall_4: 0.7040\n",
            "Epoch 57/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4053 - accuracy: 0.8208 - precision_4: 0.7673 - recall_4: 0.6993\n",
            "Epoch 58/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8379 - precision_4: 0.8059 - recall_4: 0.7063\n",
            "Epoch 59/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.8314 - precision_4: 0.7861 - recall_4: 0.7110\n",
            "Epoch 60/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8225 - precision_4: 0.7518 - recall_4: 0.7343\n",
            "Epoch 61/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8347 - precision_4: 0.7912 - recall_4: 0.7156\n",
            "Epoch 62/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.8257 - precision_4: 0.7708 - recall_4: 0.7133\n",
            "Epoch 63/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8241 - precision_4: 0.7724 - recall_4: 0.7040\n",
            "Epoch 64/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8282 - precision_4: 0.7839 - recall_4: 0.7016\n",
            "Epoch 65/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8371 - precision_4: 0.7855 - recall_4: 0.7343\n",
            "Epoch 66/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.8282 - precision_4: 0.7753 - recall_4: 0.7156\n",
            "Epoch 67/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8298 - precision_4: 0.7778 - recall_4: 0.7179\n",
            "Epoch 68/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8371 - precision_4: 0.7943 - recall_4: 0.7203\n",
            "Epoch 69/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8363 - precision_4: 0.7969 - recall_4: 0.7133\n",
            "Epoch 70/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8265 - precision_4: 0.7727 - recall_4: 0.7133\n",
            "Epoch 71/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8371 - precision_4: 0.7841 - recall_4: 0.7366\n",
            "Epoch 72/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.8282 - precision_4: 0.7795 - recall_4: 0.7086\n",
            "Epoch 73/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.8371 - precision_4: 0.7913 - recall_4: 0.7249\n",
            "Epoch 74/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8461 - precision_4: 0.8015 - recall_4: 0.7436\n",
            "Epoch 75/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3773 - accuracy: 0.8363 - precision_4: 0.7836 - recall_4: 0.7343\n",
            "Epoch 76/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8290 - precision_4: 0.7801 - recall_4: 0.7110\n",
            "Epoch 77/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8257 - precision_4: 0.7763 - recall_4: 0.7040\n",
            "Epoch 78/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8428 - precision_4: 0.7850 - recall_4: 0.7576\n",
            "Epoch 79/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8412 - precision_4: 0.7955 - recall_4: 0.7343\n",
            "Epoch 80/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8469 - precision_4: 0.8051 - recall_4: 0.7413\n",
            "Epoch 81/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8436 - precision_4: 0.8000 - recall_4: 0.7366\n",
            "Epoch 82/150\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 0.3746 - accuracy: 0.8420 - precision_4: 0.7990 - recall_4: 0.7319\n",
            "Epoch 83/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3717 - accuracy: 0.8371 - precision_4: 0.7827 - recall_4: 0.7389\n",
            "Epoch 84/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8428 - precision_4: 0.7980 - recall_4: 0.7366\n",
            "Epoch 85/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8428 - precision_4: 0.8089 - recall_4: 0.7203\n",
            "Epoch 86/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3612 - accuracy: 0.8461 - precision_4: 0.7970 - recall_4: 0.7506\n",
            "Epoch 87/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3594 - accuracy: 0.8436 - precision_4: 0.7985 - recall_4: 0.7389\n",
            "Epoch 88/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8396 - precision_4: 0.8021 - recall_4: 0.7179\n",
            "Epoch 89/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.8412 - precision_4: 0.7940 - recall_4: 0.7366\n",
            "Epoch 90/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.8420 - precision_4: 0.7990 - recall_4: 0.7319\n",
            "Epoch 91/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3563 - accuracy: 0.8461 - precision_4: 0.7941 - recall_4: 0.7552\n",
            "Epoch 92/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8355 - precision_4: 0.7802 - recall_4: 0.7366\n",
            "Epoch 93/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8518 - precision_4: 0.8159 - recall_4: 0.7436\n",
            "Epoch 94/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8542 - precision_4: 0.8049 - recall_4: 0.7692\n",
            "Epoch 95/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3559 - accuracy: 0.8404 - precision_4: 0.7949 - recall_4: 0.7319\n",
            "Epoch 96/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8502 - precision_4: 0.8117 - recall_4: 0.7436\n",
            "Epoch 97/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3642 - accuracy: 0.8461 - precision_4: 0.8046 - recall_4: 0.7389\n",
            "Epoch 98/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.8469 - precision_4: 0.8035 - recall_4: 0.7436\n",
            "Epoch 99/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8493 - precision_4: 0.8035 - recall_4: 0.7529\n",
            "Epoch 100/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3633 - accuracy: 0.8420 - precision_4: 0.7873 - recall_4: 0.7506\n",
            "Epoch 101/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.8550 - precision_4: 0.8243 - recall_4: 0.7436\n",
            "Epoch 102/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8412 - precision_4: 0.7970 - recall_4: 0.7319\n",
            "Epoch 103/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8396 - precision_4: 0.7929 - recall_4: 0.7319\n",
            "Epoch 104/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3478 - accuracy: 0.8493 - precision_4: 0.8081 - recall_4: 0.7459\n",
            "Epoch 105/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.8583 - precision_4: 0.8195 - recall_4: 0.7622\n",
            "Epoch 106/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8493 - precision_4: 0.8128 - recall_4: 0.7389\n",
            "Epoch 107/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3537 - accuracy: 0.8469 - precision_4: 0.7961 - recall_4: 0.7552\n",
            "Epoch 108/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.8453 - precision_4: 0.7880 - recall_4: 0.7622\n",
            "Epoch 109/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3571 - accuracy: 0.8518 - precision_4: 0.8111 - recall_4: 0.7506\n",
            "Epoch 110/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3577 - accuracy: 0.8436 - precision_4: 0.7970 - recall_4: 0.7413\n",
            "Epoch 111/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3504 - accuracy: 0.8607 - precision_4: 0.8177 - recall_4: 0.7739\n",
            "Epoch 112/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8453 - precision_4: 0.8041 - recall_4: 0.7366\n",
            "Epoch 113/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8518 - precision_4: 0.8159 - recall_4: 0.7436\n",
            "Epoch 114/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8485 - precision_4: 0.8123 - recall_4: 0.7366\n",
            "Epoch 115/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8518 - precision_4: 0.7990 - recall_4: 0.7692\n",
            "Epoch 116/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8550 - precision_4: 0.8260 - recall_4: 0.7413\n",
            "Epoch 117/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8453 - precision_4: 0.8056 - recall_4: 0.7343\n",
            "Epoch 118/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.8477 - precision_4: 0.8010 - recall_4: 0.7506\n",
            "Epoch 119/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8502 - precision_4: 0.8070 - recall_4: 0.7506\n",
            "Epoch 120/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8534 - precision_4: 0.8217 - recall_4: 0.7413\n",
            "Epoch 121/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8599 - precision_4: 0.8253 - recall_4: 0.7599\n",
            "Epoch 122/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8526 - precision_4: 0.8246 - recall_4: 0.7343\n",
            "Epoch 123/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8493 - precision_4: 0.8035 - recall_4: 0.7529\n",
            "Epoch 124/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3411 - accuracy: 0.8461 - precision_4: 0.8077 - recall_4: 0.7343\n",
            "Epoch 125/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3381 - accuracy: 0.8583 - precision_4: 0.8244 - recall_4: 0.7552\n",
            "Epoch 126/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8477 - precision_4: 0.8071 - recall_4: 0.7413\n",
            "Epoch 127/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3394 - accuracy: 0.8559 - precision_4: 0.8150 - recall_4: 0.7599\n",
            "Epoch 128/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8526 - precision_4: 0.8163 - recall_4: 0.7459\n",
            "Epoch 129/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8477 - precision_4: 0.8010 - recall_4: 0.7506\n",
            "Epoch 130/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.8567 - precision_4: 0.8252 - recall_4: 0.7483\n",
            "Epoch 131/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3432 - accuracy: 0.8461 - precision_4: 0.8077 - recall_4: 0.7343\n",
            "Epoch 132/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8526 - precision_4: 0.8147 - recall_4: 0.7483\n",
            "Epoch 133/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3459 - accuracy: 0.8420 - precision_4: 0.7916 - recall_4: 0.7436\n",
            "Epoch 134/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3410 - accuracy: 0.8583 - precision_4: 0.8295 - recall_4: 0.7483\n",
            "Epoch 135/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8469 - precision_4: 0.8035 - recall_4: 0.7436\n",
            "Epoch 136/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3466 - accuracy: 0.8510 - precision_4: 0.8122 - recall_4: 0.7459\n",
            "Epoch 137/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3509 - accuracy: 0.8469 - precision_4: 0.8051 - recall_4: 0.7413\n",
            "Epoch 138/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8502 - precision_4: 0.8133 - recall_4: 0.7413\n",
            "Epoch 139/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8567 - precision_4: 0.8139 - recall_4: 0.7646\n",
            "Epoch 140/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8550 - precision_4: 0.8084 - recall_4: 0.7669\n",
            "Epoch 141/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8534 - precision_4: 0.8074 - recall_4: 0.7622\n",
            "Epoch 142/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8371 - precision_4: 0.7884 - recall_4: 0.7296\n",
            "Epoch 143/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8591 - precision_4: 0.8249 - recall_4: 0.7576\n",
            "Epoch 144/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3363 - accuracy: 0.8640 - precision_4: 0.8195 - recall_4: 0.7832\n",
            "Epoch 145/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8493 - precision_4: 0.8144 - recall_4: 0.7366\n",
            "Epoch 146/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8583 - precision_4: 0.8244 - recall_4: 0.7552\n",
            "Epoch 147/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8510 - precision_4: 0.8045 - recall_4: 0.7576\n",
            "Epoch 148/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3282 - accuracy: 0.8599 - precision_4: 0.8270 - recall_4: 0.7576\n",
            "Epoch 149/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8583 - precision_4: 0.8133 - recall_4: 0.7716\n",
            "Epoch 150/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8510 - precision_4: 0.8170 - recall_4: 0.7389\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9708 - precision_4: 0.9211 - recall_4: 1.0000\n",
            "Accuracy: 97.08%\n",
            "Precision: 92.11%\n",
            "Recall: 100.00%\n"
          ]
        }
      ],
      "source": [
        "test_size = 0.2\n",
        "train_data, test_data = train_test_split(eval_augmented_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "# last column\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_data)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "\n"
      ],
      "id": "woXDJmn21s5Y"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XQiPojfz22mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5317c0-ffd1-4f16-dadd-937c6cfeaeb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "77/77 [==============================] - 1s 3ms/step - loss: 8.2853 - accuracy: 0.6042 - precision_5: 0.4100 - recall_5: 0.3060\n",
            "Epoch 2/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 2.0348 - accuracy: 0.5729 - precision_5: 0.4074 - recall_5: 0.4925\n",
            "Epoch 3/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.5763 - accuracy: 0.6016 - precision_5: 0.4336 - recall_5: 0.4627\n",
            "Epoch 4/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.2854 - accuracy: 0.6042 - precision_5: 0.4333 - recall_5: 0.4366\n",
            "Epoch 5/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.2251 - accuracy: 0.6094 - precision_5: 0.4444 - recall_5: 0.4776\n",
            "Epoch 6/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.9480 - accuracy: 0.6393 - precision_5: 0.4831 - recall_5: 0.4813\n",
            "Epoch 7/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8679 - accuracy: 0.6406 - precision_5: 0.4845 - recall_5: 0.4664\n",
            "Epoch 8/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8404 - accuracy: 0.6341 - precision_5: 0.4762 - recall_5: 0.4851\n",
            "Epoch 9/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7521 - accuracy: 0.6549 - precision_5: 0.5058 - recall_5: 0.4888\n",
            "Epoch 10/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.6523 - precision_5: 0.5022 - recall_5: 0.4291\n",
            "Epoch 11/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6972 - accuracy: 0.6680 - precision_5: 0.5286 - recall_5: 0.4478\n",
            "Epoch 12/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7350 - accuracy: 0.6341 - precision_5: 0.4733 - recall_5: 0.4291\n",
            "Epoch 13/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.6589 - precision_5: 0.5135 - recall_5: 0.4254\n",
            "Epoch 14/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.6771 - precision_5: 0.5420 - recall_5: 0.4813\n",
            "Epoch 15/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6456 - accuracy: 0.6797 - precision_5: 0.5500 - recall_5: 0.4515\n",
            "Epoch 16/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6351 - accuracy: 0.6901 - precision_5: 0.5652 - recall_5: 0.4851\n",
            "Epoch 17/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6261 - accuracy: 0.6927 - precision_5: 0.5696 - recall_5: 0.4888\n",
            "Epoch 18/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6814 - accuracy: 0.6628 - precision_5: 0.5197 - recall_5: 0.4440\n",
            "Epoch 19/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.6745 - precision_5: 0.5425 - recall_5: 0.4291\n",
            "Epoch 20/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.6862 - precision_5: 0.5611 - recall_5: 0.4627\n",
            "Epoch 21/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6082 - accuracy: 0.7044 - precision_5: 0.5945 - recall_5: 0.4813\n",
            "Epoch 22/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.7018 - precision_5: 0.5924 - recall_5: 0.4664\n",
            "Epoch 23/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6545 - accuracy: 0.6901 - precision_5: 0.5708 - recall_5: 0.4515\n",
            "Epoch 24/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.7148 - precision_5: 0.6079 - recall_5: 0.5149\n",
            "Epoch 25/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6222 - accuracy: 0.6914 - precision_5: 0.5677 - recall_5: 0.4851\n",
            "Epoch 26/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6159 - accuracy: 0.6810 - precision_5: 0.5507 - recall_5: 0.4664\n",
            "Epoch 27/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.6966 - precision_5: 0.5745 - recall_5: 0.5037\n",
            "Epoch 28/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.6836 - precision_5: 0.5546 - recall_5: 0.4739\n",
            "Epoch 29/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6597 - accuracy: 0.6706 - precision_5: 0.5311 - recall_5: 0.4776\n",
            "Epoch 30/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5840 - accuracy: 0.6979 - precision_5: 0.5804 - recall_5: 0.4851\n",
            "Epoch 31/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5808 - accuracy: 0.7031 - precision_5: 0.5962 - recall_5: 0.4627\n",
            "Epoch 32/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6901 - precision_5: 0.5600 - recall_5: 0.5224\n",
            "Epoch 33/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5952 - accuracy: 0.6849 - precision_5: 0.5613 - recall_5: 0.4440\n",
            "Epoch 34/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6063 - accuracy: 0.6992 - precision_5: 0.5869 - recall_5: 0.4664\n",
            "Epoch 35/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6228 - accuracy: 0.6823 - precision_5: 0.5469 - recall_5: 0.5224\n",
            "Epoch 36/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7044 - precision_5: 0.5895 - recall_5: 0.5037\n",
            "Epoch 37/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.7148 - precision_5: 0.6184 - recall_5: 0.4776\n",
            "Epoch 38/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5693 - accuracy: 0.7044 - precision_5: 0.5903 - recall_5: 0.5000\n",
            "Epoch 39/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5672 - accuracy: 0.7240 - precision_5: 0.6273 - recall_5: 0.5149\n",
            "Epoch 40/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5937 - accuracy: 0.7227 - precision_5: 0.6201 - recall_5: 0.5299\n",
            "Epoch 41/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5806 - accuracy: 0.7083 - precision_5: 0.5932 - recall_5: 0.5224\n",
            "Epoch 42/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5885 - accuracy: 0.6992 - precision_5: 0.5787 - recall_5: 0.5075\n",
            "Epoch 43/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5598 - accuracy: 0.7318 - precision_5: 0.6476 - recall_5: 0.5075\n",
            "Epoch 44/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5669 - accuracy: 0.7135 - precision_5: 0.6165 - recall_5: 0.4739\n",
            "Epoch 45/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5573 - accuracy: 0.7201 - precision_5: 0.6233 - recall_5: 0.5000\n",
            "Epoch 46/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5577 - accuracy: 0.7044 - precision_5: 0.5887 - recall_5: 0.5075\n",
            "Epoch 47/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5660 - accuracy: 0.7148 - precision_5: 0.6099 - recall_5: 0.5075\n",
            "Epoch 48/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5769 - accuracy: 0.7253 - precision_5: 0.6351 - recall_5: 0.5000\n",
            "Epoch 49/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.7201 - precision_5: 0.6268 - recall_5: 0.4888\n",
            "Epoch 50/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5924 - accuracy: 0.7057 - precision_5: 0.5946 - recall_5: 0.4925\n",
            "Epoch 51/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5406 - accuracy: 0.7174 - precision_5: 0.6175 - recall_5: 0.5000\n",
            "Epoch 52/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5474 - accuracy: 0.7057 - precision_5: 0.6019 - recall_5: 0.4627\n",
            "Epoch 53/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5559 - accuracy: 0.7201 - precision_5: 0.6178 - recall_5: 0.5187\n",
            "Epoch 54/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5701 - accuracy: 0.7005 - precision_5: 0.5888 - recall_5: 0.4701\n",
            "Epoch 55/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6547 - accuracy: 0.6862 - precision_5: 0.5574 - recall_5: 0.4888\n",
            "Epoch 56/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5540 - accuracy: 0.7161 - precision_5: 0.6059 - recall_5: 0.5336\n",
            "Epoch 57/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7292 - precision_5: 0.6376 - recall_5: 0.5187\n",
            "Epoch 58/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7305 - precision_5: 0.6380 - recall_5: 0.5261\n",
            "Epoch 59/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.7122 - precision_5: 0.6044 - recall_5: 0.5075\n",
            "Epoch 60/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.7174 - precision_5: 0.6143 - recall_5: 0.5112\n",
            "Epoch 61/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7344 - precision_5: 0.6429 - recall_5: 0.5373\n",
            "Epoch 62/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5677 - accuracy: 0.7057 - precision_5: 0.5963 - recall_5: 0.4851\n",
            "Epoch 63/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.7253 - precision_5: 0.6256 - recall_5: 0.5299\n",
            "Epoch 64/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7370 - precision_5: 0.6514 - recall_5: 0.5299\n",
            "Epoch 65/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5465 - accuracy: 0.7318 - precision_5: 0.6396 - recall_5: 0.5299\n",
            "Epoch 66/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7344 - precision_5: 0.6538 - recall_5: 0.5075\n",
            "Epoch 67/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7448 - precision_5: 0.6607 - recall_5: 0.5522\n",
            "Epoch 68/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7135 - precision_5: 0.6081 - recall_5: 0.5037\n",
            "Epoch 69/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5442 - accuracy: 0.7279 - precision_5: 0.6439 - recall_5: 0.4925\n",
            "Epoch 70/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5660 - accuracy: 0.7253 - precision_5: 0.6213 - recall_5: 0.5448\n",
            "Epoch 71/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5762 - accuracy: 0.7253 - precision_5: 0.6213 - recall_5: 0.5448\n",
            "Epoch 72/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.7201 - precision_5: 0.6210 - recall_5: 0.5075\n",
            "Epoch 73/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7292 - precision_5: 0.6429 - recall_5: 0.5037\n",
            "Epoch 74/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7331 - precision_5: 0.6413 - recall_5: 0.5336\n",
            "Epoch 75/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7083 - precision_5: 0.5924 - recall_5: 0.5261\n",
            "Epoch 76/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5528 - accuracy: 0.7240 - precision_5: 0.6284 - recall_5: 0.5112\n",
            "Epoch 77/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7409 - precision_5: 0.6561 - recall_5: 0.5410\n",
            "Epoch 78/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7318 - precision_5: 0.6336 - recall_5: 0.5485\n",
            "Epoch 79/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5710 - accuracy: 0.7122 - precision_5: 0.5992 - recall_5: 0.5299\n",
            "Epoch 80/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7240 - precision_5: 0.6148 - recall_5: 0.5597\n",
            "Epoch 81/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.7253 - precision_5: 0.6267 - recall_5: 0.5261\n",
            "Epoch 82/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7214 - precision_5: 0.6250 - recall_5: 0.5037\n",
            "Epoch 83/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5720 - accuracy: 0.7227 - precision_5: 0.6190 - recall_5: 0.5336\n",
            "Epoch 84/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5254 - accuracy: 0.7357 - precision_5: 0.6457 - recall_5: 0.5373\n",
            "Epoch 85/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7253 - precision_5: 0.6223 - recall_5: 0.5410\n",
            "Epoch 86/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7318 - precision_5: 0.6550 - recall_5: 0.4888\n",
            "Epoch 87/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5927 - accuracy: 0.6979 - precision_5: 0.5849 - recall_5: 0.4627\n",
            "Epoch 88/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7370 - precision_5: 0.6528 - recall_5: 0.5261\n",
            "Epoch 89/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.7253 - precision_5: 0.6173 - recall_5: 0.5597\n",
            "Epoch 90/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5589 - accuracy: 0.7448 - precision_5: 0.6714 - recall_5: 0.5261\n",
            "Epoch 91/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.7422 - precision_5: 0.6667 - recall_5: 0.5224\n",
            "Epoch 92/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5359 - accuracy: 0.7487 - precision_5: 0.6728 - recall_5: 0.5448\n",
            "Epoch 93/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5574 - accuracy: 0.7318 - precision_5: 0.6384 - recall_5: 0.5336\n",
            "Epoch 94/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7396 - precision_5: 0.6560 - recall_5: 0.5336\n",
            "Epoch 95/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7474 - precision_5: 0.6637 - recall_5: 0.5597\n",
            "Epoch 96/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7318 - precision_5: 0.6409 - recall_5: 0.5261\n",
            "Epoch 97/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7422 - precision_5: 0.6591 - recall_5: 0.5410\n",
            "Epoch 98/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.7435 - precision_5: 0.6461 - recall_5: 0.5858\n",
            "Epoch 99/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5394 - accuracy: 0.7253 - precision_5: 0.6377 - recall_5: 0.4925\n",
            "Epoch 100/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7422 - precision_5: 0.6471 - recall_5: 0.5746\n",
            "Epoch 101/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7383 - precision_5: 0.6516 - recall_5: 0.5373\n",
            "Epoch 102/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7279 - precision_5: 0.6300 - recall_5: 0.5336\n",
            "Epoch 103/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5649 - accuracy: 0.7396 - precision_5: 0.6604 - recall_5: 0.5224\n",
            "Epoch 104/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7409 - precision_5: 0.6533 - recall_5: 0.5485\n",
            "Epoch 105/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7214 - precision_5: 0.6250 - recall_5: 0.5037\n",
            "Epoch 106/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.7435 - precision_5: 0.6802 - recall_5: 0.5000\n",
            "Epoch 107/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.7500 - precision_5: 0.6792 - recall_5: 0.5373\n",
            "Epoch 108/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7565 - precision_5: 0.6833 - recall_5: 0.5634\n",
            "Epoch 109/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5385 - accuracy: 0.7344 - precision_5: 0.6455 - recall_5: 0.5299\n",
            "Epoch 110/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7253 - precision_5: 0.6267 - recall_5: 0.5261\n",
            "Epoch 111/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.7448 - precision_5: 0.6593 - recall_5: 0.5560\n",
            "Epoch 112/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5057 - accuracy: 0.7370 - precision_5: 0.6587 - recall_5: 0.5112\n",
            "Epoch 113/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5273 - accuracy: 0.7292 - precision_5: 0.6429 - recall_5: 0.5037\n",
            "Epoch 114/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.7513 - precision_5: 0.6667 - recall_5: 0.5746\n",
            "Epoch 115/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5138 - accuracy: 0.7370 - precision_5: 0.6460 - recall_5: 0.5448\n",
            "Epoch 116/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5380 - accuracy: 0.7214 - precision_5: 0.6227 - recall_5: 0.5112\n",
            "Epoch 117/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7461 - precision_5: 0.6667 - recall_5: 0.5448\n",
            "Epoch 118/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7461 - precision_5: 0.6853 - recall_5: 0.5037\n",
            "Epoch 119/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5194 - accuracy: 0.7383 - precision_5: 0.6379 - recall_5: 0.5784\n",
            "Epoch 120/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4985 - accuracy: 0.7461 - precision_5: 0.6714 - recall_5: 0.5336\n",
            "Epoch 121/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4963 - accuracy: 0.7513 - precision_5: 0.6758 - recall_5: 0.5522\n",
            "Epoch 122/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5171 - accuracy: 0.7487 - precision_5: 0.6761 - recall_5: 0.5373\n",
            "Epoch 123/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4965 - accuracy: 0.7591 - precision_5: 0.7005 - recall_5: 0.5410\n",
            "Epoch 124/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5002 - accuracy: 0.7565 - precision_5: 0.6833 - recall_5: 0.5634\n",
            "Epoch 125/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4970 - accuracy: 0.7643 - precision_5: 0.7062 - recall_5: 0.5560\n",
            "Epoch 126/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5452 - accuracy: 0.7331 - precision_5: 0.6376 - recall_5: 0.5448\n",
            "Epoch 127/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.5159 - accuracy: 0.7461 - precision_5: 0.6667 - recall_5: 0.5448\n",
            "Epoch 128/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5058 - accuracy: 0.7591 - precision_5: 0.6797 - recall_5: 0.5858\n",
            "Epoch 129/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5142 - accuracy: 0.7344 - precision_5: 0.6404 - recall_5: 0.5448\n",
            "Epoch 130/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5096 - accuracy: 0.7409 - precision_5: 0.6635 - recall_5: 0.5224\n",
            "Epoch 131/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7370 - precision_5: 0.6486 - recall_5: 0.5373\n",
            "Epoch 132/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7357 - precision_5: 0.6383 - recall_5: 0.5597\n",
            "Epoch 133/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7578 - precision_5: 0.6916 - recall_5: 0.5522\n",
            "Epoch 134/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5251 - accuracy: 0.7500 - precision_5: 0.6610 - recall_5: 0.5821\n",
            "Epoch 135/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7591 - precision_5: 0.6861 - recall_5: 0.5709\n",
            "Epoch 136/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.7240 - precision_5: 0.6217 - recall_5: 0.5336\n",
            "Epoch 137/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.7188 - precision_5: 0.6226 - recall_5: 0.4925\n",
            "Epoch 138/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.7708 - precision_5: 0.7130 - recall_5: 0.5746\n",
            "Epoch 139/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7409 - precision_5: 0.6605 - recall_5: 0.5299\n",
            "Epoch 140/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7474 - precision_5: 0.6729 - recall_5: 0.5373\n",
            "Epoch 141/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7604 - precision_5: 0.6826 - recall_5: 0.5858\n",
            "Epoch 142/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7526 - precision_5: 0.6773 - recall_5: 0.5560\n",
            "Epoch 143/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7656 - precision_5: 0.7000 - recall_5: 0.5746\n",
            "Epoch 144/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7630 - precision_5: 0.6903 - recall_5: 0.5821\n",
            "Epoch 145/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7487 - precision_5: 0.6761 - recall_5: 0.5373\n",
            "Epoch 146/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.7539 - precision_5: 0.6756 - recall_5: 0.5672\n",
            "Epoch 147/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.7747 - precision_5: 0.7273 - recall_5: 0.5672\n",
            "Epoch 148/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7578 - precision_5: 0.6798 - recall_5: 0.5784\n",
            "Epoch 149/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.7500 - precision_5: 0.6810 - recall_5: 0.5336\n",
            "Epoch 150/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.7617 - precision_5: 0.7014 - recall_5: 0.5522\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.9102 - precision_5: 1.0000 - recall_5: 0.7406\n",
            "Accuracy: 91.02%\n",
            "Precision: 100.00%\n",
            "Recall: 74.06%\n"
          ]
        }
      ],
      "source": [
        "test_size = 0.5\n",
        "train_data, test_data = train_test_split(eval_augmented_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n"
      ],
      "id": "XQiPojfz22mt"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}