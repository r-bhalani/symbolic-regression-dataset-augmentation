{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fd33643"
      },
      "source": [
        "# An Adaptive Synthesis Approach for Dataset Augmentation"
      ],
      "id": "0fd33643"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2649232b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ba3c41-790a-44a4-a5c2-49da0bdaf630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gplearn in /usr/local/lib/python3.9/dist-packages (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "# Ruchi Bhalani, rb44675\n",
        "!pip install gplearn"
      ],
      "id": "2649232b"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8ZMR0CqzTxX",
        "outputId": "76e61bfe-3b6f-4663-8724-715164551f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "A8ZMR0CqzTxX"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ab1fe226"
      },
      "outputs": [],
      "source": [
        "# headers\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from collections import OrderedDict"
      ],
      "id": "ab1fe226"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "cc9c4481"
      },
      "outputs": [],
      "source": [
        "# function to calculate adjusted r2\n",
        "def get_adj_r2(r2, n, p):\n",
        "    return (1-(1-r2)*((n-1)/(n-p-1)))"
      ],
      "id": "cc9c4481"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7c7806"
      },
      "source": [
        "# Assignment 2: Regression and KNN classifier"
      ],
      "id": "7a7c7806"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bb47879"
      },
      "source": [
        "**Data Prep**\n"
      ],
      "id": "8bb47879"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "589c05de",
        "outputId": "112e841e-6349-4cec-a109-446d11b3873c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    int64  \n",
            " 2   BloodPressure             768 non-null    int64  \n",
            " 3   SkinThickness             768 non-null    int64  \n",
            " 4   Insulin                   768 non-null    int64  \n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n",
            "None\n",
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "763           10      101             76             48      180  32.9   \n",
            "764            2      122             70             27        0  36.8   \n",
            "765            5      121             72             23      112  26.2   \n",
            "766            1      126             60              0        0  30.1   \n",
            "767            1       93             70             31        0  30.4   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "763                     0.171   63        0  \n",
            "764                     0.340   27        0  \n",
            "765                     0.245   30        0  \n",
            "766                     0.349   47        1  \n",
            "767                     0.315   23        0  \n",
            "\n",
            "[768 rows x 9 columns]\n",
            "[]\n",
            "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
            "FINAL STEP: proper nouns -- []\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15ab0199-ad83-4ece-aef2-1eef98c38539\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15ab0199-ad83-4ece-aef2-1eef98c38539')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15ab0199-ad83-4ece-aef2-1eef98c38539 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15ab0199-ad83-4ece-aef2-1eef98c38539');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# NOTE: THESE VALUES ARE MEANT TO BE CUSTOMIZABLE BY THE USER\n",
        "# read in data\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/medical-charges.txt\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/breast_cancer.csv\"\n",
        "USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/diabetes.csv\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/disease.csv\"\n",
        "EXPECTED_DISTINCT_PERCENTAGE = 0.85\n",
        "\n",
        "# How many entries do we want to augment this dataset by?\n",
        "# TODO: Let the user pass this in as a percentage as well\n",
        "NEW_EXAMPLES = 10\n",
        "\n",
        "# data = pd.read_csv(USER_INPUT_DATASET, header = 0)\n",
        "df = pd.read_csv(USER_INPUT_DATASET, header=\"infer\")\n",
        "print(df.info())\n",
        "print(df)\n",
        "\n",
        "\n",
        "# DATA PREPROCESSING\n",
        "df = df.dropna(axis=1, how='all')\n",
        "# df = df.dropna()\n",
        "df = df.fillna(0)\n",
        "\n",
        "\n",
        "# Separate them into numerical, categorical, and proper noun columns (regex generation)\n",
        "numerical_cols = list()\n",
        "categorical_cols = list()\n",
        "proper_noun_cols = list()\n",
        "for (index, colname) in enumerate(df):\n",
        "    if colname in df.select_dtypes(include='object').columns:\n",
        "      unique_vals = df[colname].unique()\n",
        "      if len(unique_vals) >= EXPECTED_DISTINCT_PERCENTAGE * df.shape[0]:\n",
        "        proper_noun_cols.append(colname)\n",
        "      else:\n",
        "        categorical_cols.append(colname)\n",
        "    else:\n",
        "      numerical_cols.append(colname)\n",
        "\n",
        "\n",
        "print(categorical_cols)\n",
        "print(numerical_cols)\n",
        "print(\"FINAL STEP: proper nouns --\", proper_noun_cols)\n",
        "\n",
        "df.head()\n"
      ],
      "id": "589c05de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3043bb91"
      },
      "source": [
        "There are several categorical columns. We need to transform these to be able to do regression. "
      ],
      "id": "3043bb91"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0484464d",
        "outputId": "4faee9eb-0fd7-4198-f7db-c59a0f2003ee",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-896c3f0b-c583-4189-8d97-1beea326faa8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-896c3f0b-c583-4189-8d97-1beea326faa8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-896c3f0b-c583-4189-8d97-1beea326faa8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-896c3f0b-c583-4189-8d97-1beea326faa8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# we'll deal with the proper noun columns at the end\n",
        "regression_df = df.drop(columns=proper_noun_cols)\n",
        "regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "regression_df.head()"
      ],
      "id": "0484464d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38a13dc1"
      },
      "source": [
        "An interesting thing to check with regression problems is whether any of the individual features correlate very strongly with the label."
      ],
      "id": "38a13dc1"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad660fd",
        "outputId": "e251761c-3637-46a7-bd24-cdf0d6d786fa",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.129459       0.141282      -0.081672   \n",
            "Glucose                      0.129459  1.000000       0.152590       0.057328   \n",
            "BloodPressure                0.141282  0.152590       1.000000       0.207371   \n",
            "SkinThickness               -0.081672  0.057328       0.207371       1.000000   \n",
            "Insulin                     -0.073535  0.331357       0.088933       0.436783   \n",
            "BMI                          0.017683  0.221071       0.281805       0.392573   \n",
            "DiabetesPedigreeFunction    -0.033523  0.137337       0.041265       0.183928   \n",
            "Age                          0.544341  0.263514       0.239528      -0.113970   \n",
            "Outcome                      0.221898  0.466581       0.065068       0.074752   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.073535  0.017683                 -0.033523   \n",
            "Glucose                   0.331357  0.221071                  0.137337   \n",
            "BloodPressure             0.088933  0.281805                  0.041265   \n",
            "SkinThickness             0.436783  0.392573                  0.183928   \n",
            "Insulin                   1.000000  0.197859                  0.185071   \n",
            "BMI                       0.197859  1.000000                  0.140647   \n",
            "DiabetesPedigreeFunction  0.185071  0.140647                  1.000000   \n",
            "Age                      -0.042163  0.036242                  0.033561   \n",
            "Outcome                   0.130548  0.292695                  0.173844   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.544341  0.221898  \n",
            "Glucose                   0.263514  0.466581  \n",
            "BloodPressure             0.239528  0.065068  \n",
            "SkinThickness            -0.113970  0.074752  \n",
            "Insulin                  -0.042163  0.130548  \n",
            "BMI                       0.036242  0.292695  \n",
            "DiabetesPedigreeFunction  0.033561  0.173844  \n",
            "Age                       1.000000  0.238356  \n",
            "Outcome                   0.238356  1.000000   \n",
            "\n",
            "\n",
            "=== SIGNIFICANTLY CORRELATED COLUMNS ===\n",
            "[]\n",
            "OrderedDict()\n"
          ]
        }
      ],
      "source": [
        "print(regression_df.corr(), \"\\n\\n\")\n",
        "\n",
        "# Indexing with numbers on a numpy matrix will probably be faster\n",
        "\n",
        "print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "rows, cols = regression_df.shape\n",
        "corr = regression_df.corr().values\n",
        "fields = list(regression_df.columns)\n",
        "correlated_columns = list()\n",
        "nodes = []\n",
        "correlations = OrderedDict()\n",
        "\n",
        "# data structure that organizes correlations\n",
        "correlations_per_column = list()\n",
        "for i in range(cols):\n",
        "    for j in range(i+1, cols):\n",
        "      corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "      if corr[i,j] > 0.6:\n",
        "          print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "          correlated_columns.append([fields[i], fields[j], corr[i, j]])\n",
        "          correlations[corr[i, j]] = [fields[i], fields[j]]\n",
        "          if fields[i] not in nodes:\n",
        "            nodes.append(fields[i])\n",
        "          if fields[j] not in nodes:\n",
        "            nodes.append(fields[j])\n",
        "\n",
        "print(nodes)\n",
        "print(correlations)"
      ],
      "id": "6ad660fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l74DGBikGfow"
      },
      "source": [
        "We can visualize these attributes and their correlations as a weighted graph."
      ],
      "id": "l74DGBikGfow"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ggXpUuAfWqG2",
        "outputId": "193ec849-4561-4327-870d-be373282a64a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "for key,value in correlations.items():\n",
        "  G.add_edge(value[0], value[1], weight=key)\n",
        "\n",
        "\n",
        "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0.5]\n",
        "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.5]\n",
        "\n",
        "pos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n",
        "\n",
        "# nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=700)\n",
        "\n",
        "# edges\n",
        "nx.draw_networkx_edges(G, pos, edgelist=elarge, width=6)\n",
        "nx.draw_networkx_edges(\n",
        "    G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=\"b\", style=\"dashed\"\n",
        ")\n",
        "\n",
        "# node labels\n",
        "nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
        "# edge weight labels\n",
        "edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.margins(0.15)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ggXpUuAfWqG2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBaK36IxauAO"
      },
      "source": [
        "Now, let's organize them by pair and start generating values."
      ],
      "id": "lBaK36IxauAO"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkLcoIVuar-l",
        "outputId": "780639cf-55e2-4fec-d672-868be44e7414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict()\n"
          ]
        }
      ],
      "source": [
        "# weight all the correlations so that they're iterated through correctly\n",
        "weighted_correlations = OrderedDict()\n",
        "for key,value in correlations.items():\n",
        "  mult_factor = 1\n",
        "  if value[0] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  if value[1] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  weighted_correlations[mult_factor * key] = value\n",
        "\n",
        "weighted_correlations = OrderedDict(sorted(weighted_correlations.items(), reverse=True))\n",
        "print(weighted_correlations)\n"
      ],
      "id": "FkLcoIVuar-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numerical Data Generation: Symbolic Regression\n",
        "Now, let's augment the dataset with numerical data.\n",
        "\n",
        "The code is implementing a technique for data augmentation using symbolic regression, kernel density estimation, and K-nearest neighbor regression. The goal is to generate new samples of data that are similar to the original dataset, but not identical, to increase the size and diversity of the dataset for training machine learning models. The code takes in a pandas DataFrame df and the number of new samples to generate n_samples as inputs.\n",
        "\n",
        "The code first scales each column of the input dataset to the range [0, 1], fits a symbolic regressor to the scaled data, and generates new values using the regressor. It then applies kernel density estimation and K-nearest neighbor regression to the original data and generates new values using these methods. Finally, it performs a grid search to find the best kernel and bandwidth for the kernel density estimator, and generates new values using the best estimator.\n",
        "\n",
        "The new values generated by each method are clipped to the range of the original column data, and the new values for all columns are combined to create a new DataFrame new_df. The new_df is then concatenated with the original DataFrame df to create an augmented dataset, which is returned as output.\n",
        "\n"
      ],
      "metadata": {
        "id": "dJCaBCe3meJ0"
      },
      "id": "dJCaBCe3meJ0"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YqEGUD4oDrP",
        "outputId": "798eb776-0a4f-4a21-df98-02043f7643e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}\n",
            "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}\n",
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0    48.81          10071.7        7       0.00305286       0.00301773     15.02m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-212.16250948          -inf          -inf -133.46744026          -inf\n",
            " -281.12881055          -inf          -inf -234.94439958          -inf\n",
            " -335.32241593          -inf          -inf -287.66128371          -inf\n",
            " -365.75219178          -inf          -inf -317.71711134          -inf\n",
            " -375.91694359          -inf          -inf -336.11539337          -inf\n",
            " -378.37136908          -inf          -inf -348.22868624          -inf\n",
            " -379.38704625          -inf          -inf -356.76618794          -inf\n",
            " -380.37205545          -inf          -inf -363.15711045          -inf\n",
            " -381.48333254          -inf          -inf -368.19488025          -inf\n",
            " -382.69898589          -inf          -inf -372.34241894          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          335.372        7       0.00819325         0.008165     18.22m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-2205.95343925           -inf           -inf  -559.25572027\n",
            "           -inf -1052.41953892           -inf           -inf\n",
            "  -629.35077437           -inf  -880.01955654           -inf\n",
            "           -inf  -670.25407298           -inf  -829.48189416\n",
            "           -inf           -inf  -693.18518004           -inf\n",
            "  -800.65028245           -inf           -inf  -706.60941256\n",
            "           -inf  -781.69988178           -inf           -inf\n",
            "  -714.96223286           -inf  -769.74763495           -inf\n",
            "           -inf  -720.45759718           -inf  -762.09498762\n",
            "           -inf           -inf  -724.24488325           -inf\n",
            "  -756.97375804           -inf           -inf  -726.95636458\n",
            "           -inf  -753.39007971           -inf           -inf\n",
            "  -728.95984443           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          282.023        7       0.00763926       0.00761355      5.94m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-2445.03428884           -inf           -inf  -358.32805942\n",
            "           -inf  -946.47998163           -inf           -inf\n",
            "  -420.32378069           -inf  -711.26873186           -inf\n",
            "           -inf  -465.88752342           -inf  -649.1577841\n",
            "           -inf           -inf  -498.9893832            -inf\n",
            "  -631.66409828           -inf           -inf  -523.42788794\n",
            "           -inf  -629.32370997           -inf           -inf\n",
            "  -541.62281829           -inf  -631.0017599            -inf\n",
            "           -inf  -555.3095333            -inf  -631.98932878\n",
            "           -inf           -inf  -565.74609072           -inf\n",
            "  -631.3097818            -inf           -inf  -573.8286537\n",
            "           -inf  -629.57564525           -inf           -inf\n",
            "  -580.18945499           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          987.505        7       0.00278235       0.00292524      7.28m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-13495.19767892            -inf            -inf   -344.50510743\n",
            "            -inf  -3694.16418482            -inf            -inf\n",
            "   -402.35377832            -inf  -1920.79316416            -inf\n",
            "            -inf   -442.07866858            -inf  -1313.77990581\n",
            "            -inf            -inf   -467.64354722            -inf\n",
            "  -1033.1128268             -inf            -inf   -485.03174867\n",
            "            -inf   -880.98162067            -inf            -inf\n",
            "   -497.67865917            -inf   -790.76151053            -inf\n",
            "            -inf   -507.41697048            -inf   -733.6970289\n",
            "            -inf            -inf   -515.25722072            -inf\n",
            "   -695.77605486            -inf            -inf   -521.78785443\n",
            "            -inf   -669.61889804            -inf            -inf\n",
            "   -527.37133899            -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          6071.69        7       0.00125741       0.00139989      7.59m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-309589.17899775             -inf             -inf   -1566.17194355\n",
            "             -inf  -77720.40537206             -inf             -inf\n",
            "   -1016.10823309             -inf  -34824.02519881             -inf\n",
            "             -inf    -857.45327759             -inf  -19830.41047071\n",
            "             -inf             -inf    -788.7057134              -inf\n",
            "  -12900.74227397             -inf             -inf    -753.19806481\n",
            "             -inf   -9143.29642094             -inf             -inf\n",
            "    -733.11756746             -inf   -6882.66247141             -inf\n",
            "             -inf    -721.2209064              -inf   -5419.04564976\n",
            "             -inf             -inf    -714.06064143             -inf\n",
            "   -4418.30252909             -inf             -inf    -709.81166052\n",
            "             -inf   -3704.59787663             -inf             -inf\n",
            "    -707.43120698             -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          520.195        7       0.00641139       0.00657749      6.15m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-1262.16758172           -inf           -inf  -531.60986257\n",
            "           -inf  -701.68271431           -inf           -inf\n",
            "  -524.6201116            -inf  -598.08157738           -inf\n",
            "           -inf  -522.08982663           -inf  -562.26954159\n",
            "           -inf           -inf  -520.80582564           -inf\n",
            "  -545.83285564           -inf           -inf  -520.07313566\n",
            "           -inf  -536.96645631           -inf           -inf\n",
            "  -519.6355193            -inf  -531.70618285           -inf\n",
            "           -inf  -519.37394094           -inf  -528.39056976\n",
            "           -inf           -inf  -519.22595499           -inf\n",
            "  -526.2051062            -inf           -inf  -519.15644156\n",
            "           -inf  -524.71041988           -inf           -inf\n",
            "  -519.14467698           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81            33425        7       0.00224604       0.00245896      7.28m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ -10.97266936          -inf          -inf  -14.47760995          -inf\n",
            "  -28.45302491  -15.9240256   -10.01976593  -33.2008054    -8.55711879\n",
            "  -45.99796192  -29.39019944  -18.86317953  -51.81594996  -15.64128288\n",
            "  -63.69587154  -41.63165588  -27.9746282   -69.66101286  -23.31159305\n",
            "  -81.1537541   -52.94731945  -36.556336    -86.40319724  -30.69022234\n",
            "  -97.94722307  -65.19719414  -45.00726511 -101.9322133   -37.92936599\n",
            " -113.86677799  -78.505134    -53.76679594 -116.28702527  -45.33331512\n",
            " -128.84257618  -92.03010983  -62.82540399 -129.56786715  -52.93043257\n",
            " -142.87546522 -105.13576948  -72.06519569 -141.88902599  -60.66184121\n",
            " -156.00702594 -117.41326858  -81.21217257 -153.3596588   -68.34686653]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          29984.2        7       0.00275687       0.00269347      7.54m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-1612.51579683           -inf           -inf  -329.81974483\n",
            "           -inf  -758.98201829           -inf           -inf\n",
            "  -418.1532406            -inf  -642.26084751           -inf\n",
            "           -inf  -465.94522643           -inf  -612.16400627\n",
            "           -inf           -inf  -492.95834461           -inf\n",
            "  -593.48701589           -inf           -inf  -509.11341421\n",
            "           -inf  -579.95901377           -inf           -inf\n",
            "  -519.42832445           -inf  -571.17871268           -inf\n",
            "           -inf  -526.43809699           -inf  -565.62962012\n",
            "           -inf           -inf  -531.47316777           -inf\n",
            "  -562.06768498           -inf           -inf  -535.26798929\n",
            "           -inf  -559.74470965           -inf           -inf\n",
            "  -538.24897689           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          60.5031        7                0                0      6.03m\n",
            "(768, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KernelDensity, KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from gplearn.genetic import SymbolicRegressor\n",
        "\n",
        "def augment_dataset(df, n_samples):\n",
        "    # Get column names and data types\n",
        "    col_names = df.columns.tolist()\n",
        "    dtypes = df.dtypes.tolist()\n",
        "\n",
        "    # Create a list to store new data samples\n",
        "    new_data = []\n",
        "\n",
        "    # Loop through each column and generate new values\n",
        "    for col_name, dtype in zip(col_names, dtypes):\n",
        "        # Get the data from the column\n",
        "        col_data = df[col_name].values\n",
        "\n",
        "        # Scale the column data to the range [0, 1]\n",
        "        scaler = MinMaxScaler()\n",
        "        col_data_scaled = scaler.fit_transform(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Fit a symbolic regressor to the column data\n",
        "        est_gp = SymbolicRegressor(population_size=5000, tournament_size=50,\n",
        "                                    generations=50, stopping_criteria=0.01,\n",
        "                                    p_crossover=0.7, p_subtree_mutation=0.1,\n",
        "                                    p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
        "                                    max_samples=0.9, verbose=1,\n",
        "                                    parsimony_coefficient=0.001, random_state=0, warm_start=True)\n",
        "        est_gp.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the symbolic regressor\n",
        "        new_col_data = est_gp.predict(np.random.rand(n_samples, 1))\n",
        "\n",
        "        # Reshape the new column data to be a 2D array\n",
        "        new_col_data = new_col_data.reshape(-1, 1)\n",
        "\n",
        "        # Scale the new column data using the scaler fitted to the original column data\n",
        "        new_col_data_scaled = scaler.transform(new_col_data)\n",
        "\n",
        "        # Inverse transform the scaled new column data to get the original scale\n",
        "        new_col_data = scaler.inverse_transform(new_col_data_scaled)\n",
        "\n",
        "        # Fit a kernel density estimator to the original column data\n",
        "        kde = KernelDensity(kernel='gaussian', bandwidth=0.1)\n",
        "        kde.fit(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Generate new values using the kernel density estimator\n",
        "        kde_samples = kde.sample(n_samples)\n",
        "        kde_samples = np.squeeze(kde_samples)\n",
        "\n",
        "        # Fit a KNN regressor to the original column data\n",
        "        knn = KNeighborsRegressor(n_neighbors=5)\n",
        "        knn.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the KNN regressor\n",
        "        knn_samples = knn.predict(np.random.rand(n_samples, 1))\n",
        "        knn_samples = np.squeeze(knn_samples)\n",
        "\n",
        "        # Use a grid search to find the best kernel and bandwidth for the kernel density estimator\n",
        "        params = {'kernel': ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear'],\n",
        "                  'bandwidth': np.linspace(0.1, 1.0, 10)}\n",
        "        grid = GridSearchCV(KernelDensity(), params, cv=5)\n",
        "        print(\"DATA SHAPE HERE: \", print(col_data.reshape(-1, 1).shape))\n",
        "        grid.fit(col_data.reshape(-1, 1))\n",
        "        # grid.fit(col_data)\n",
        "        kde_best = grid.best_estimator_\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        if kde_best.kernel in [\"gaussian\", \"tophat\"]:\n",
        "            kde_best_samples = kde_best.sample(n_samples)\n",
        "        else:\n",
        "            # Evaluate log density for a grid of points\n",
        "            x_grid = np.linspace(col_data.min(), col_data.max(), 1000).reshape(-1, 1)\n",
        "            log_dens = kde_best.score_samples(x_grid)\n",
        "\n",
        "            # Sample from the grid according to the log densities\n",
        "            probs = np.exp(log_dens - log_dens.max())\n",
        "            probs /= probs.sum()\n",
        "            kde_best_samples = np.random.choice(x_grid.flatten(), size=n_samples, p=probs)\n",
        "            \n",
        "        kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        # kde_best_samples = kde_best.sample(n_samples)\n",
        "        # kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Clip the new values to the range of the original column data\n",
        "        kde_best_samples = np.clip(kde_best_samples, np.min(col_data), np.max(col_data))\n",
        "\n",
        "        # Add new values to the list of new data\n",
        "        new_data.append(kde_best_samples.flatten())\n",
        "\n",
        "    # Transpose the list of new data to match the shape of the original dataset\n",
        "    new_data = np.array(new_data).T\n",
        "\n",
        "    # Convert the new data to a dataframe and append it to the original dataset\n",
        "    new_df = pd.DataFrame(new_data, columns=col_names)\n",
        "    augmented_df = pd.concat([df, new_df], ignore_index=True)\n",
        "\n",
        "    # return augmented_df\n",
        "    return new_df\n",
        "\n",
        "print(np.seterr())\n",
        "np.seterr(invalid='warn')\n",
        "np.seterr(under='warn')\n",
        "print(np.seterr())\n",
        "new_df = augment_dataset(regression_df, df.shape[0])"
      ],
      "id": "4YqEGUD4oDrP"
    },
    {
      "cell_type": "code",
      "source": [
        "# def reverse_one_hot_encode(df):\n",
        "#     # Get a list of all columns with '_'\n",
        "#     oh_cols = [col for col in df.columns if '_' in col]\n",
        "\n",
        "#     # Get the original column names and values for each one-hot encoded column\n",
        "#     col_vals = {}\n",
        "#     for col in oh_cols:\n",
        "#         col_name = col.split('_')[0]\n",
        "#         col_val = col.split('_')[1]\n",
        "#         if col_name not in col_vals:\n",
        "#             col_vals[col_name] = {}\n",
        "#         col_vals[col_name][col_val] = col\n",
        "\n",
        "#     # Create a new dataframe to store the reverse one-hot encoded values\n",
        "#     new_df = pd.DataFrame(columns=col_vals.keys())\n",
        "\n",
        "#     # Iterate through each row of the original dataframe\n",
        "#     for index, row in df.iterrows():\n",
        "#         # Initialize a dictionary to store the values for this row\n",
        "#         new_row = {}\n",
        "\n",
        "#         # Iterate through each original column\n",
        "#         for col_name in df.columns:\n",
        "#             # If this is a one-hot encoded column, find the closest value to 1 and use that\n",
        "#             if col_name in oh_cols:\n",
        "#                 col_val = col_name.split('_')[1]\n",
        "#                 col_key = col_vals[col_name.split('_')[0]][col_val]\n",
        "                \n",
        "#                 # Get the numeric values for the one-hot encoded column\n",
        "#                 numeric_values = pd.to_numeric(row[col_key], errors='coerce')\n",
        "#                 # Set values < 0.5 to 0 and values >= 0.5 to the numeric value\n",
        "#                 numeric_values = np.where(numeric_values >= 0.5, numeric_values, 0)\n",
        "#                 # Set the value closest to 1 to 1, and the rest to 0\n",
        "#                 numeric_values = np.where(numeric_values == np.max(numeric_values), 1, 0)\n",
        "#                 # Combine the column name and the value (if it's 1) and add it to the new_row dictionary\n",
        "#                 new_row[col_name.split('_')[0]] = col_val if np.sum(numeric_values) > 0 else np.nan\n",
        "#             # If this is not a one-hot encoded column, use the original value\n",
        "#             else:\n",
        "#                 new_row[col_name] = row[col_name]\n",
        "\n",
        "#         # Append the values for this row to the new dataframe\n",
        "#         new_df = new_df.append(new_row, ignore_index=True)\n",
        "\n",
        "#     # Return the new dataframe\n",
        "#     return new_df"
      ],
      "metadata": {
        "id": "er5lGyrRnbUt"
      },
      "id": "er5lGyrRnbUt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "x0qeIOSFMs8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75d0b34-308e-4d2b-8ca8-21bfadf59c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Old:  768\n",
            "New:  768\n",
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  0.000000 -0.824551      -0.635139      -0.857652   \n",
            "Glucose                     -0.824551  0.000000      -0.721007      -0.859591   \n",
            "BloodPressure               -0.635139 -0.721007       0.000000      -0.591221   \n",
            "SkinThickness               -0.857652 -0.859591      -0.591221       0.000000   \n",
            "Insulin                     -0.882690 -0.550096      -0.584159      -0.409961   \n",
            "BMI                         -0.913420 -0.768766      -0.612830      -0.529938   \n",
            "DiabetesPedigreeFunction    -0.941543 -0.787318      -0.712160      -0.708398   \n",
            "Age                         -0.450684 -0.679051      -0.503688      -0.800700   \n",
            "Outcome                     -0.652560 -0.356666      -0.555795      -0.727136   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.882690 -0.913420                 -0.941543   \n",
            "Glucose                  -0.550096 -0.768766                 -0.787318   \n",
            "BloodPressure            -0.584159 -0.612830                 -0.712160   \n",
            "SkinThickness            -0.409961 -0.529938                 -0.708398   \n",
            "Insulin                   0.000000 -0.666502                 -0.802731   \n",
            "BMI                      -0.666502  0.000000                 -0.775112   \n",
            "DiabetesPedigreeFunction -0.802731 -0.775112                  0.000000   \n",
            "Age                      -0.926053 -0.877163                 -0.946705   \n",
            "Outcome                  -0.663926 -0.481190                 -0.635608   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies              -0.450684 -0.652560  \n",
            "Glucose                  -0.679051 -0.356666  \n",
            "BloodPressure            -0.503688 -0.555795  \n",
            "SkinThickness            -0.800700 -0.727136  \n",
            "Insulin                  -0.926053 -0.663926  \n",
            "BMI                      -0.877163 -0.481190  \n",
            "DiabetesPedigreeFunction -0.946705 -0.635608  \n",
            "Age                       0.000000 -0.636698  \n",
            "Outcome                  -0.636698  0.000000  \n",
            "     Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
            "0       0.119119   81.074074      48.116116       0.000000    0.000000   \n",
            "1       4.067067  123.901902      74.006006      27.945946   88.072072   \n",
            "2       1.973974  102.985986      65.945946      16.648649    0.846847   \n",
            "3      10.941942  184.060060      92.080080      45.189189  326.882883   \n",
            "4       1.038038   96.014014      61.915916       0.099099    0.000000   \n",
            "..           ...         ...            ...            ...         ...   \n",
            "763     9.138138  170.912913      88.172172      41.126126  248.972973   \n",
            "764     2.229229  107.965966      68.632633      20.018018    4.234234   \n",
            "765     9.002002  166.132132      87.927928      40.927928  229.495495   \n",
            "766     9.036036  167.128128      88.050050      41.027027  232.036036   \n",
            "767     4.730731  125.893894      74.982983      29.036036   94.846847   \n",
            "\n",
            "           BMI  DiabetesPedigreeFunction        Age   Outcome  \n",
            "0    22.030831                  0.136609  21.720721  0.004004  \n",
            "1    33.650751                  0.474194  32.951952  0.070070  \n",
            "2    28.881882                  0.293680  25.804805  0.030030  \n",
            "3    45.270671                  1.182186  59.138138  0.993994  \n",
            "4    26.329530                  0.230382  23.882883  0.017017  \n",
            "..         ...                       ...        ...       ...  \n",
            "763  42.516817                  0.952440  52.951952  0.986987  \n",
            "764  30.359560                  0.338222  27.126126  0.039039  \n",
            "765  41.777978                  0.907898  51.270270  0.984985  \n",
            "766  41.979479                  0.919620  51.930931  0.985986  \n",
            "767  34.120921                  0.497638  34.033033  0.077077  \n",
            "\n",
            "[768 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# reprocess the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# just brings columns back to normal and replaces values with NaN\n",
        "def reverse_one_hot_encode(df):\n",
        "    oh_cols = [col for col in df.columns if \"_\" in col and col.split(\"_\")[0] in categorical_cols]\n",
        "    print(oh_cols)\n",
        "    new_df = df.drop(columns=oh_cols).copy()\n",
        "    for col in oh_cols:\n",
        "        col_name, val = col.split(\"_\")\n",
        "        mask = df[col] == 1\n",
        "        new_df[col_name] = np.nan\n",
        "        new_df.loc[mask, col_name] = val\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_significantly_correlated_colmns(df):\n",
        "  print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "  rows, cols = df.shape\n",
        "  corr = df.corr().values\n",
        "  fields = list(df.columns)\n",
        "  for i in range(cols):\n",
        "      for j in range(i+1, cols-1):\n",
        "        corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "        if corr[i,j] > 0.75:\n",
        "            print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "\n",
        "new_df = reverse_one_hot_encode(new_df)\n",
        "corr1 = regression_df.corr()\n",
        "corr2 = new_df.corr()\n",
        "# print a matrix to see the difference in correlations\n",
        "diff = np.abs(corr1) - np.abs(corr2)\n",
        "\n",
        "print(\"Old: \", len(regression_df))\n",
        "print(\"New: \", len(new_df))\n",
        "print(diff)\n",
        "\n",
        "# print(print_significantly_correlated_colmns(regression_df))\n",
        "# print(print_significantly_correlated_colmns(new_df))\n",
        "new_df.head()\n",
        "print(new_df)"
      ],
      "id": "x0qeIOSFMs8x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIh2Sn5ohWnz"
      },
      "source": [
        "## Categorical Variable Generation: Hill Climbing Algorithm\n",
        "In this implementation, we use a hill-climbing algorithm to search for the best categorical variable distribution that matches the original distribution and is correlated with the numerical variables in the generated data. The algorithm starts with the original distribution and iteratively perturbs it by swapping two values, then computes the correlation with the numerical variables and updates the distribution if the correlation improves. The algorithm repeats this process for a fixed number of iterations (in this case, 10), then moves on to the next categorical variable.\n",
        "\n",
        "This is an example of an explicit search or optimization technique for program synthesis, as we are searching for the best program (i.e., categorical variable distribution) that satisfies certain constraints (i.e., matching the original distribution and being correlated with the numerical variables)."
      ],
      "id": "cIh2Sn5ohWnz"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Hp-B6m8qSRXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47306dd1-8c64-4ded-df3a-28c4b87f4b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.129459       0.141282      -0.081672   \n",
            "Glucose                      0.129459  1.000000       0.152590       0.057328   \n",
            "BloodPressure                0.141282  0.152590       1.000000       0.207371   \n",
            "SkinThickness               -0.081672  0.057328       0.207371       1.000000   \n",
            "Insulin                     -0.073535  0.331357       0.088933       0.436783   \n",
            "BMI                          0.017683  0.221071       0.281805       0.392573   \n",
            "DiabetesPedigreeFunction    -0.033523  0.137337       0.041265       0.183928   \n",
            "Age                          0.544341  0.263514       0.239528      -0.113970   \n",
            "Outcome                      0.221898  0.466581       0.065068       0.074752   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.073535  0.017683                 -0.033523   \n",
            "Glucose                   0.331357  0.221071                  0.137337   \n",
            "BloodPressure             0.088933  0.281805                  0.041265   \n",
            "SkinThickness             0.436783  0.392573                  0.183928   \n",
            "Insulin                   1.000000  0.197859                  0.185071   \n",
            "BMI                       0.197859  1.000000                  0.140647   \n",
            "DiabetesPedigreeFunction  0.185071  0.140647                  1.000000   \n",
            "Age                      -0.042163  0.036242                  0.033561   \n",
            "Outcome                   0.130548  0.292695                  0.173844   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.544341  0.221898  \n",
            "Glucose                   0.263514  0.466581  \n",
            "BloodPressure             0.239528  0.065068  \n",
            "SkinThickness            -0.113970  0.074752  \n",
            "Insulin                  -0.042163  0.130548  \n",
            "BMI                       0.036242  0.292695  \n",
            "DiabetesPedigreeFunction  0.033561  0.173844  \n",
            "Age                       1.000000  0.238356  \n",
            "Outcome                   0.238356  1.000000  \n",
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.407731       0.354059       0.248327   \n",
            "Glucose                      0.407731  1.000000       0.403984       0.344517   \n",
            "BloodPressure                0.354059  0.403984       1.000000       0.403289   \n",
            "SkinThickness                0.248327  0.344517       0.403289       1.000000   \n",
            "Insulin                      0.271020  0.520677       0.289113       0.572506   \n",
            "BMI                          0.316300  0.482088       0.488181       0.563837   \n",
            "DiabetesPedigreeFunction     0.300471  0.407428       0.283742       0.416063   \n",
            "Age                          0.692784  0.495683       0.410407       0.221713   \n",
            "Outcome                      0.427271  0.582300       0.245082       0.301332   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies               0.271020  0.316300                  0.300471   \n",
            "Glucose                   0.520677  0.482088                  0.407428   \n",
            "BloodPressure             0.289113  0.488181                  0.283742   \n",
            "SkinThickness             0.572506  0.563837                  0.416063   \n",
            "Insulin                   1.000000  0.420642                  0.457183   \n",
            "BMI                       0.420642  1.000000                  0.397927   \n",
            "DiabetesPedigreeFunction  0.457183  0.397927                  1.000000   \n",
            "Age                       0.298877  0.326380                  0.350375   \n",
            "Outcome                   0.344417  0.444561                  0.376873   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.692784  0.427271  \n",
            "Glucose                   0.495683  0.582300  \n",
            "BloodPressure             0.410407  0.245082  \n",
            "SkinThickness             0.221713  0.301332  \n",
            "Insulin                   0.298877  0.344417  \n",
            "BMI                       0.326380  0.444561  \n",
            "DiabetesPedigreeFunction  0.350375  0.376873  \n",
            "Age                       1.000000  0.440934  \n",
            "Outcome                   0.440934  1.000000  \n",
            "1.0     282\n",
            "2.0     220\n",
            "0.0     178\n",
            "3.0     159\n",
            "4.0     134\n",
            "5.0     119\n",
            "6.0     104\n",
            "8.0      85\n",
            "7.0      75\n",
            "9.0      62\n",
            "10.0     49\n",
            "11.0     24\n",
            "13.0     22\n",
            "12.0     16\n",
            "15.0      3\n",
            "14.0      3\n",
            "17.0      1\n",
            "Name: Pregnancies, dtype: int64\n",
            "100.0    40\n",
            "99.0     37\n",
            "106.0    30\n",
            "95.0     30\n",
            "112.0    29\n",
            "         ..\n",
            "190.0     2\n",
            "178.0     1\n",
            "160.0     1\n",
            "67.0      1\n",
            "169.0     1\n",
            "Name: Glucose, Length: 136, dtype: int64\n",
            "70.0     118\n",
            "74.0     103\n",
            "78.0      91\n",
            "64.0      91\n",
            "72.0      89\n",
            "68.0      88\n",
            "80.0      81\n",
            "60.0      75\n",
            "0.0       74\n",
            "62.0      73\n",
            "76.0      73\n",
            "66.0      60\n",
            "88.0      52\n",
            "58.0      46\n",
            "82.0      46\n",
            "86.0      46\n",
            "84.0      45\n",
            "90.0      40\n",
            "50.0      25\n",
            "92.0      22\n",
            "54.0      22\n",
            "52.0      22\n",
            "56.0      19\n",
            "85.0      15\n",
            "75.0      14\n",
            "94.0      12\n",
            "48.0      11\n",
            "65.0      11\n",
            "96.0       8\n",
            "44.0       7\n",
            "110.0      5\n",
            "106.0      5\n",
            "100.0      5\n",
            "104.0      5\n",
            "98.0       5\n",
            "108.0      5\n",
            "30.0       4\n",
            "46.0       3\n",
            "102.0      3\n",
            "114.0      3\n",
            "40.0       2\n",
            "55.0       2\n",
            "24.0       2\n",
            "38.0       2\n",
            "79.0       2\n",
            "122.0      1\n",
            "95.0       1\n",
            "61.0       1\n",
            "69.0       1\n",
            "Name: BloodPressure, dtype: int64\n",
            "0.0     426\n",
            "32.0     64\n",
            "23.0     52\n",
            "30.0     48\n",
            "31.0     47\n",
            "27.0     44\n",
            "33.0     43\n",
            "18.0     41\n",
            "39.0     40\n",
            "28.0     39\n",
            "40.0     36\n",
            "25.0     35\n",
            "19.0     34\n",
            "29.0     34\n",
            "41.0     33\n",
            "37.0     32\n",
            "22.0     32\n",
            "26.0     31\n",
            "20.0     28\n",
            "17.0     27\n",
            "24.0     26\n",
            "13.0     25\n",
            "35.0     24\n",
            "15.0     24\n",
            "36.0     23\n",
            "21.0     22\n",
            "42.0     19\n",
            "45.0     18\n",
            "46.0     17\n",
            "16.0     17\n",
            "12.0     17\n",
            "34.0     16\n",
            "38.0     13\n",
            "14.0     13\n",
            "44.0     11\n",
            "43.0     10\n",
            "10.0     10\n",
            "11.0     10\n",
            "50.0      8\n",
            "7.0       8\n",
            "47.0      7\n",
            "48.0      7\n",
            "54.0      6\n",
            "49.0      5\n",
            "52.0      3\n",
            "63.0      3\n",
            "60.0      2\n",
            "51.0      2\n",
            "8.0       2\n",
            "56.0      1\n",
            "99.0      1\n",
            "Name: SkinThickness, dtype: int64\n",
            "0.0      564\n",
            "1.0       79\n",
            "2.0       33\n",
            "3.0       18\n",
            "105.0     15\n",
            "        ... \n",
            "510.0      1\n",
            "270.0      1\n",
            "178.0      1\n",
            "392.0      1\n",
            "232.0      1\n",
            "Name: Insulin, Length: 277, dtype: int64\n",
            "22.0    140\n",
            "21.0    115\n",
            "25.0    102\n",
            "24.0     98\n",
            "23.0     77\n",
            "28.0     66\n",
            "27.0     66\n",
            "26.0     65\n",
            "29.0     63\n",
            "30.0     47\n",
            "37.0     46\n",
            "41.0     45\n",
            "31.0     43\n",
            "32.0     34\n",
            "33.0     34\n",
            "38.0     32\n",
            "45.0     31\n",
            "42.0     29\n",
            "36.0     28\n",
            "46.0     27\n",
            "40.0     27\n",
            "34.0     24\n",
            "39.0     24\n",
            "43.0     22\n",
            "51.0     20\n",
            "35.0     20\n",
            "58.0     18\n",
            "50.0     17\n",
            "47.0     16\n",
            "52.0     15\n",
            "44.0     13\n",
            "53.0     11\n",
            "49.0     10\n",
            "54.0     10\n",
            "66.0      9\n",
            "62.0      9\n",
            "48.0      9\n",
            "57.0      9\n",
            "60.0      8\n",
            "55.0      7\n",
            "59.0      7\n",
            "63.0      7\n",
            "56.0      6\n",
            "61.0      5\n",
            "65.0      5\n",
            "67.0      5\n",
            "68.0      4\n",
            "69.0      3\n",
            "72.0      3\n",
            "64.0      2\n",
            "70.0      2\n",
            "81.0      1\n",
            "Name: Age, dtype: int64\n",
            "0.0    1002\n",
            "1.0     534\n",
            "Name: Outcome, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0          6.0    148.0           72.0           35.0      0.0  33.6   \n",
              "1          1.0     85.0           66.0           29.0      0.0  26.6   \n",
              "2          8.0    183.0           64.0            0.0      0.0  23.3   \n",
              "3          1.0     89.0           66.0           23.0     94.0  28.1   \n",
              "4          0.0    137.0           40.0           35.0    168.0  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction   Age  Outcome  \n",
              "0                     0.627  50.0      1.0  \n",
              "1                     0.351  31.0      0.0  \n",
              "2                     0.672  32.0      1.0  \n",
              "3                     0.167  21.0      0.0  \n",
              "4                     2.288  33.0      1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-342a6b1a-801e-4327-b03f-6065b3af2b82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-342a6b1a-801e-4327-b03f-6065b3af2b82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-342a6b1a-801e-4327-b03f-6065b3af2b82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-342a6b1a-801e-4327-b03f-6065b3af2b82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import random\n",
        "import copy\n",
        "\n",
        "def generate_categorical_variables(original_data, generated_data):\n",
        "    # Extract the original categorical variables\n",
        "    original_categorical_data = original_data.select_dtypes(include=['object'])\n",
        "    \n",
        "    # Extract the generated numerical variables\n",
        "    generated_numerical_data = generated_data.select_dtypes(include=['int64', 'float64'])\n",
        "    \n",
        "    # Generate new categorical variables for the generated data using hill-climbing algorithm\n",
        "    new_categorical_data = copy.deepcopy(original_categorical_data)\n",
        "    for col in new_categorical_data.columns:\n",
        "        current_distribution = original_categorical_data[col].value_counts(normalize=True)\n",
        "        for _ in range(10):\n",
        "            # Perturb the distribution by swapping two values\n",
        "            i, j = random.sample(range(len(current_distribution)), 2)\n",
        "            new_distribution = current_distribution.copy()\n",
        "            new_distribution[i], new_distribution[j] = new_distribution[j], new_distribution[i]\n",
        "            new_distribution /= new_distribution.sum()\n",
        "\n",
        "            # Compute the correlation with the numerical variables\n",
        "            new_correlation = abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))\n",
        "            current_correlation=abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))*-1\n",
        "            # Update the categorical variable if the correlation improves\n",
        "            if new_correlation.sum() > current_correlation.sum():\n",
        "                current_correlation = new_correlation\n",
        "                new_categorical_data[col] = np.random.choice(new_distribution.index, size=len(new_categorical_data), p=new_distribution.values)\n",
        "    \n",
        "    # Replace the NaNs in the generated data with the new categorical data\n",
        "    generated_data.update(new_categorical_data)\n",
        "    \n",
        "    return generated_data\n",
        "\n",
        "\n",
        "final_df = generate_categorical_variables(df, new_df)\n",
        "# final_df.head()\n",
        "\n",
        "final_df = pd.concat([df, final_df], axis=0)\n",
        "final_df_normalized = pd.get_dummies(final_df, columns=categorical_cols)\n",
        "complete_encoded = pd.concat([final_df_normalized, regression_df], axis=0)\n",
        "print(regression_df.corr())\n",
        "print(complete_encoded.corr())\n",
        "\n",
        "# make sure that all columns are rounded appropriately\n",
        "for col in final_df.columns:\n",
        "  if col in numerical_cols:\n",
        "    # print(col)\n",
        "    # all(print(x) for x in col)\n",
        "    if all((x*1.0).is_integer() for x in df[col]):\n",
        "      final_df[col] = np.round(final_df[col], 0)\n",
        "      # print(final_df[col])\n",
        "      print(final_df[col].value_counts())\n",
        "\n",
        "# print(final_df)\n",
        "final_df.head()\n"
      ],
      "id": "Hp-B6m8qSRXx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVkqlquMVDX"
      },
      "source": [
        "Let's view both distributions to evaluate the quality of these generated values."
      ],
      "id": "LkVkqlquMVDX"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "0o2tCRcLOTao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "6a2337fc-bd4c-4454-f42b-3e727b85d277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The distributions are not similar.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def compare_distributions(list1, list2):\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n",
        "\n",
        "    # Plot histograms\n",
        "    ax1.hist(list1, density=True, alpha=0.5)\n",
        "    ax2.hist(list2, density=True, alpha=0.5)\n",
        "\n",
        "    # Fit normal distributions\n",
        "    mu1, std1 = norm.fit(list1)\n",
        "    mu2, std2 = norm.fit(list2)\n",
        "\n",
        "    # Plot normal distribution curves\n",
        "    x1 = np.linspace(min(list1), max(list1), 100)\n",
        "    ax1.plot(x1, norm.pdf(x1, mu1, std1))\n",
        "    ax1.set_title('List 1')\n",
        "    ax2.set_title('List 2')\n",
        "    x2 = np.linspace(min(list2), max(list2), 100)\n",
        "    ax2.plot(x2, norm.pdf(x2, mu2, std2))\n",
        "\n",
        "    # Check if distributions are similar\n",
        "    if np.allclose(mu1, mu2) and np.allclose(std1, std2):\n",
        "        print('The distributions are similar.')\n",
        "    else:\n",
        "        print('The distributions are not similar.')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "compare_distributions(list(df[numerical_cols[3]]), list(new_df[numerical_cols[3]]))"
      ],
      "id": "0o2tCRcLOTao"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8av1-IvPy-t"
      },
      "source": [
        "It seems like the criteria for determining if the distributions are similar is too strict, especially given that we are working off a small dataset."
      ],
      "id": "_8av1-IvPy-t"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "jtmfU3j2P9sA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "29035752-3cc3-4dc4-fb2d-22e94e4563a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL DIVERGENCE:  0.06242586714580429\n",
            "The distributions are similar\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_distributions_tolerance(list1, list2, tolerance=0.07):\n",
        "    \"\"\"Compares the distributions of two lists and plots them side by side.\n",
        "\n",
        "    Args:\n",
        "        list1 (list): The first list to compare.\n",
        "        list2 (list): The second list to compare.\n",
        "        tolerance (float): The tolerance for accepting similarity between the distributions.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the distributions are similar, False otherwise.\n",
        "    \"\"\"\n",
        "    # Calculate the histograms for the two lists\n",
        "    hist1, bins1 = np.histogram(list1, density=True)\n",
        "    hist2, bins2 = np.histogram(list2, density=True)\n",
        "\n",
        "    # Normalize the histograms to have unit area\n",
        "    hist1 = hist1 / np.sum(hist1)\n",
        "    hist2 = hist2 / np.sum(hist2)\n",
        "\n",
        "    # Calculate the KL divergence between the two histograms\n",
        "    kl_div = stats.entropy(hist2, hist1)\n",
        "\n",
        "    # Plot the histograms side by side\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    ax[0].bar(bins1[:-1], hist1, width=np.diff(bins1), align='edge')\n",
        "    ax[1].bar(bins2[:-1], hist2, width=np.diff(bins2), align='edge')\n",
        "    ax[0].set_title('Original Dataset')\n",
        "    ax[1].set_title('Augmented Dataset')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Check if the KL divergence is within the tolerance\n",
        "    print(\"KL DIVERGENCE: \", kl_div)\n",
        "    similar = True if kl_div <= tolerance else False\n",
        "\n",
        "    if similar:\n",
        "      print(\"The distributions are similar\")\n",
        "    else:\n",
        "      print(\"The distrubutions are NOT similar\")\n",
        "    return similar\n",
        "\n",
        "\n",
        "compare_distributions_tolerance(list(df[numerical_cols[2]]), list(new_df[numerical_cols[2]]))\n",
        "# new_df.to_csv('augmented_dataset.csv', index=False)"
      ],
      "id": "jtmfU3j2P9sA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PUT IT ALL TOGETHER"
      ],
      "metadata": {
        "id": "W3ptvFuiWa7k"
      },
      "id": "W3ptvFuiWa7k"
    },
    {
      "cell_type": "code",
      "source": [
        "# def prepare_for_regression(df, categorical_cols, proper_noun_cols=None):\n",
        "#   regression_df = df.drop(columns=proper_noun_cols)\n",
        "#   regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "#   return regression_df\n",
        "\n",
        "# def separate_columns(df):\n",
        "#   numerical_cols = list()\n",
        "#   categorical_cols = list()\n",
        "#   proper_noun_cols = list()\n",
        "#   for (index, colname) in enumerate(df):\n",
        "#       if colname in df.select_dtypes(include='object').columns:\n",
        "#         unique_vals = df[colname].unique()\n",
        "#         if len(unique_vals) >= 0.85 * df.shape[0]:\n",
        "#           proper_noun_cols.append(colname)\n",
        "#         else:\n",
        "#           categorical_cols.append(colname)\n",
        "#       else:\n",
        "#         numerical_cols.append(colname)\n",
        "#   return numerical_cols, categorical_cols, proper_noun_cols\n",
        "\n",
        "# def run_data_augmenter(dataset, num_samples=None, percentage_augmentation=None):\n",
        "#     df = pd.read_csv(dataset, header=\"infer\")\n",
        "#     print(df.info())\n",
        "#     print(df)\n",
        "\n",
        "\n",
        "#     # DATA PREPROCESSING\n",
        "#     df = df.dropna(axis=1, how='all')\n",
        "#     # df = df.dropna()\n",
        "#     df = df.fillna(0)\n",
        "#     new_data = 0\n",
        "#     if num_samples != None:\n",
        "#       new_data = num_samples\n",
        "#     else:\n",
        "#       new_data = int(df.shape[0] * percentage_augmentation)\n",
        "    \n",
        "#     numerical_cols, categorical_cols, _ = separate_columns(df)\n",
        "#     regression_df = prepare_for_regression(df, categorical_cols)\n",
        "    \n",
        "#     np.seterr(invalid='warn') \n",
        "#     np.seterr(under='warn')\n",
        "    \n",
        "#     new_df = augment_dataset(regression_df, new_data)\n",
        "#     new_df = reverse_one_hot_encode(new_df, categorical_cols)\n",
        "\n",
        "    \n",
        "#     num_cat_generated = generate_categorical_variables(df, new_df)\n",
        "#     # final_df.head()\n",
        "\n",
        "#     final_df = pd.concat([df, num_cat_generated], axis=0)\n",
        "\n",
        "#     # make sure that all columns are rounded appropriately\n",
        "#     for col in final_df.columns:\n",
        "#       if col in numerical_cols:\n",
        "#         # print(col)\n",
        "#         # all(print(x) for x in col)\n",
        "#         if all((x*1.0).is_integer() for x in df[col]):\n",
        "#           final_df[col] = np.round(final_df[col], 0)\n",
        "#           # print(final_df[col])\n",
        "#           print(final_df[col].value_counts())\n",
        "    \n",
        "#     return final_df"
      ],
      "metadata": {
        "id": "3wxGi_QLUdV6"
      },
      "id": "3wxGi_QLUdV6",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkDfuWoz7FJ"
      },
      "source": [
        "##Downstream ML task\n",
        "\n",
        "To demonstrate the usefulness of this dataset augmentation, we're going to be predicting the diabetes diagnoses of patients."
      ],
      "id": "okkDfuWoz7FJ"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "8j2Gng6G0RcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d29ad99-6412-4796-c4fc-4f8553219a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "609            1      111             62             13      182  24.0   \n",
            "610            3      106             54             21      158  30.9   \n",
            "611            3      174             58             22      194  32.9   \n",
            "612            7      168             88             42      321  38.2   \n",
            "613            6      105             80             28        0  32.5   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "609                     0.138   23        0  \n",
            "610                     0.292   24        0  \n",
            "611                     0.593   36        1  \n",
            "612                     0.787   40        1  \n",
            "613                     0.878   26        0  \n",
            "\n",
            "[614 rows x 9 columns]\n",
            "Epoch 1/150\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 12.9732 - accuracy: 0.3583 - precision_3: 0.3157 - recall_3: 0.7277\n",
            "Epoch 2/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 3.4489 - accuracy: 0.4837 - precision_3: 0.2570 - recall_3: 0.2582\n",
            "Epoch 3/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 2.7109 - accuracy: 0.5033 - precision_3: 0.3034 - recall_3: 0.3333\n",
            "Epoch 4/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 2.1148 - accuracy: 0.5261 - precision_3: 0.3227 - recall_3: 0.3333\n",
            "Epoch 5/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.5575 - accuracy: 0.5749 - precision_3: 0.3824 - recall_3: 0.3662\n",
            "Epoch 6/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.2904 - accuracy: 0.5782 - precision_3: 0.3905 - recall_3: 0.3850\n",
            "Epoch 7/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1622 - accuracy: 0.6075 - precision_3: 0.4307 - recall_3: 0.4085\n",
            "Epoch 8/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.1136 - accuracy: 0.6189 - precision_3: 0.4426 - recall_3: 0.3803\n",
            "Epoch 9/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9635 - accuracy: 0.6173 - precision_3: 0.4461 - recall_3: 0.4272\n",
            "Epoch 10/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9063 - accuracy: 0.6303 - precision_3: 0.4628 - recall_3: 0.4085\n",
            "Epoch 11/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9320 - accuracy: 0.6221 - precision_3: 0.4492 - recall_3: 0.3944\n",
            "Epoch 12/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7846 - accuracy: 0.6401 - precision_3: 0.4792 - recall_3: 0.4319\n",
            "Epoch 13/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7808 - accuracy: 0.6384 - precision_3: 0.4772 - recall_3: 0.4413\n",
            "Epoch 14/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8132 - accuracy: 0.6287 - precision_3: 0.4603 - recall_3: 0.4085\n",
            "Epoch 15/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7398 - accuracy: 0.6612 - precision_3: 0.5145 - recall_3: 0.4178\n",
            "Epoch 16/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7009 - accuracy: 0.6531 - precision_3: 0.5000 - recall_3: 0.4131\n",
            "Epoch 17/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6758 - accuracy: 0.6580 - precision_3: 0.5083 - recall_3: 0.4319\n",
            "Epoch 18/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6753 - accuracy: 0.6645 - precision_3: 0.5189 - recall_3: 0.4507\n",
            "Epoch 19/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6650 - accuracy: 0.6515 - precision_3: 0.4971 - recall_3: 0.3991\n",
            "Epoch 20/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.6547 - precision_3: 0.5031 - recall_3: 0.3850\n",
            "Epoch 21/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.6775 - precision_3: 0.5429 - recall_3: 0.4460\n",
            "Epoch 22/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7346 - accuracy: 0.6678 - precision_3: 0.5249 - recall_3: 0.4460\n",
            "Epoch 23/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6324 - accuracy: 0.6971 - precision_3: 0.5860 - recall_3: 0.4319\n",
            "Epoch 24/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.6629 - precision_3: 0.5161 - recall_3: 0.4507\n",
            "Epoch 25/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6533 - accuracy: 0.6889 - precision_3: 0.5705 - recall_3: 0.4178\n",
            "Epoch 26/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6415 - accuracy: 0.6596 - precision_3: 0.5123 - recall_3: 0.3897\n",
            "Epoch 27/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6217 - accuracy: 0.6987 - precision_3: 0.5833 - recall_3: 0.4601\n",
            "Epoch 28/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6300 - accuracy: 0.6808 - precision_3: 0.5548 - recall_3: 0.4038\n",
            "Epoch 29/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6136 - accuracy: 0.6873 - precision_3: 0.5686 - recall_3: 0.4085\n",
            "Epoch 30/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.6808 - precision_3: 0.5480 - recall_3: 0.4554\n",
            "Epoch 31/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5928 - accuracy: 0.6971 - precision_3: 0.5818 - recall_3: 0.4507\n",
            "Epoch 32/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6338 - accuracy: 0.6759 - precision_3: 0.5427 - recall_3: 0.4178\n",
            "Epoch 33/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.6873 - precision_3: 0.5614 - recall_3: 0.4507\n",
            "Epoch 34/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6019 - accuracy: 0.6938 - precision_3: 0.5817 - recall_3: 0.4178\n",
            "Epoch 35/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.6824 - precision_3: 0.5577 - recall_3: 0.4085\n",
            "Epoch 36/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6641 - accuracy: 0.6873 - precision_3: 0.5629 - recall_3: 0.4413\n",
            "Epoch 37/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.7052 - precision_3: 0.6039 - recall_3: 0.4366\n",
            "Epoch 38/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.7166 - precision_3: 0.6242 - recall_3: 0.4601\n",
            "Epoch 39/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6566 - accuracy: 0.7036 - precision_3: 0.5987 - recall_3: 0.4413\n",
            "Epoch 40/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7199 - precision_3: 0.6258 - recall_3: 0.4789\n",
            "Epoch 41/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5926 - accuracy: 0.6987 - precision_3: 0.5795 - recall_3: 0.4789\n",
            "Epoch 42/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.7036 - precision_3: 0.6099 - recall_3: 0.4038\n",
            "Epoch 43/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7231 - precision_3: 0.6483 - recall_3: 0.4413\n",
            "Epoch 44/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5817 - accuracy: 0.7052 - precision_3: 0.6081 - recall_3: 0.4225\n",
            "Epoch 45/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.7068 - precision_3: 0.6122 - recall_3: 0.4225\n",
            "Epoch 46/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5868 - accuracy: 0.7296 - precision_3: 0.6407 - recall_3: 0.5023\n",
            "Epoch 47/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6889 - precision_3: 0.5604 - recall_3: 0.4789\n",
            "Epoch 48/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5645 - accuracy: 0.7166 - precision_3: 0.6242 - recall_3: 0.4601\n",
            "Epoch 49/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5589 - accuracy: 0.7296 - precision_3: 0.6556 - recall_3: 0.4648\n",
            "Epoch 50/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.7231 - precision_3: 0.6443 - recall_3: 0.4507\n",
            "Epoch 51/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.7150 - precision_3: 0.6159 - recall_3: 0.4742\n",
            "Epoch 52/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.7182 - precision_3: 0.6235 - recall_3: 0.4742\n",
            "Epoch 53/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5638 - accuracy: 0.7280 - precision_3: 0.6620 - recall_3: 0.4413\n",
            "Epoch 54/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7296 - precision_3: 0.6460 - recall_3: 0.4883\n",
            "Epoch 55/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5624 - accuracy: 0.7313 - precision_3: 0.6412 - recall_3: 0.5117\n",
            "Epoch 56/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.7313 - precision_3: 0.6446 - recall_3: 0.5023\n",
            "Epoch 57/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5955 - accuracy: 0.7150 - precision_3: 0.6218 - recall_3: 0.4554\n",
            "Epoch 58/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5519 - accuracy: 0.7280 - precision_3: 0.6369 - recall_3: 0.5023\n",
            "Epoch 59/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5432 - accuracy: 0.7280 - precision_3: 0.6369 - recall_3: 0.5023\n",
            "Epoch 60/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5561 - accuracy: 0.7150 - precision_3: 0.6145 - recall_3: 0.4789\n",
            "Epoch 61/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5695 - accuracy: 0.7166 - precision_3: 0.6226 - recall_3: 0.4648\n",
            "Epoch 62/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.7378 - precision_3: 0.6688 - recall_3: 0.4836\n",
            "Epoch 63/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.7248 - precision_3: 0.6294 - recall_3: 0.5023\n",
            "Epoch 64/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.7150 - precision_3: 0.6234 - recall_3: 0.4507\n",
            "Epoch 65/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5686 - accuracy: 0.7215 - precision_3: 0.6250 - recall_3: 0.4930\n",
            "Epoch 66/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6069 - accuracy: 0.7020 - precision_3: 0.6000 - recall_3: 0.4225\n",
            "Epoch 67/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7394 - precision_3: 0.6532 - recall_3: 0.5305\n",
            "Epoch 68/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7166 - precision_3: 0.6291 - recall_3: 0.4460\n",
            "Epoch 69/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7296 - precision_3: 0.6497 - recall_3: 0.4789\n",
            "Epoch 70/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5395 - accuracy: 0.7410 - precision_3: 0.6667 - recall_3: 0.5070\n",
            "Epoch 71/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5194 - accuracy: 0.7280 - precision_3: 0.6554 - recall_3: 0.4554\n",
            "Epoch 72/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5414 - accuracy: 0.7459 - precision_3: 0.6629 - recall_3: 0.5446\n",
            "Epoch 73/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5706 - accuracy: 0.7313 - precision_3: 0.6579 - recall_3: 0.4695\n",
            "Epoch 74/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5545 - accuracy: 0.7296 - precision_3: 0.6497 - recall_3: 0.4789\n",
            "Epoch 75/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5244 - accuracy: 0.7394 - precision_3: 0.6667 - recall_3: 0.4977\n",
            "Epoch 76/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5614 - accuracy: 0.7345 - precision_3: 0.6562 - recall_3: 0.4930\n",
            "Epoch 77/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.7492 - precision_3: 0.6725 - recall_3: 0.5399\n",
            "Epoch 78/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5396 - accuracy: 0.7410 - precision_3: 0.6552 - recall_3: 0.5352\n",
            "Epoch 79/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5284 - accuracy: 0.7329 - precision_3: 0.6561 - recall_3: 0.4836\n",
            "Epoch 80/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5452 - accuracy: 0.7296 - precision_3: 0.6442 - recall_3: 0.4930\n",
            "Epoch 81/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5459 - accuracy: 0.7345 - precision_3: 0.6582 - recall_3: 0.4883\n",
            "Epoch 82/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5541 - accuracy: 0.7329 - precision_3: 0.6433 - recall_3: 0.5164\n",
            "Epoch 83/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5106 - accuracy: 0.7476 - precision_3: 0.6883 - recall_3: 0.4977\n",
            "Epoch 84/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5445 - accuracy: 0.7378 - precision_3: 0.6646 - recall_3: 0.4930\n",
            "Epoch 85/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5494 - accuracy: 0.7166 - precision_3: 0.6154 - recall_3: 0.4883\n",
            "Epoch 86/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5828 - accuracy: 0.7150 - precision_3: 0.6011 - recall_3: 0.5305\n",
            "Epoch 87/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6488 - accuracy: 0.7052 - precision_3: 0.6143 - recall_3: 0.4038\n",
            "Epoch 88/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7476 - precision_3: 0.6726 - recall_3: 0.5305\n",
            "Epoch 89/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.7166 - precision_3: 0.6102 - recall_3: 0.5070\n",
            "Epoch 90/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7362 - precision_3: 0.6735 - recall_3: 0.4648\n",
            "Epoch 91/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5441 - accuracy: 0.7410 - precision_3: 0.6776 - recall_3: 0.4836\n",
            "Epoch 92/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7280 - precision_3: 0.6353 - recall_3: 0.5070\n",
            "Epoch 93/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.7296 - precision_3: 0.6442 - recall_3: 0.4930\n",
            "Epoch 94/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7278 - accuracy: 0.6954 - precision_3: 0.5833 - recall_3: 0.4272\n",
            "Epoch 95/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7590 - precision_3: 0.7019 - recall_3: 0.5305\n",
            "Epoch 96/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5237 - accuracy: 0.7394 - precision_3: 0.6779 - recall_3: 0.4742\n",
            "Epoch 97/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5072 - accuracy: 0.7622 - precision_3: 0.7006 - recall_3: 0.5493\n",
            "Epoch 98/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.7296 - precision_3: 0.6599 - recall_3: 0.4554\n",
            "Epoch 99/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7362 - precision_3: 0.6584 - recall_3: 0.4977\n",
            "Epoch 100/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7524 - precision_3: 0.7075 - recall_3: 0.4883\n",
            "Epoch 101/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5552 - accuracy: 0.7427 - precision_3: 0.6647 - recall_3: 0.5211\n",
            "Epoch 102/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.7362 - precision_3: 0.6667 - recall_3: 0.4789\n",
            "Epoch 103/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.7671 - precision_3: 0.7108 - recall_3: 0.5540\n",
            "Epoch 104/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.7329 - precision_3: 0.6815 - recall_3: 0.4319\n",
            "Epoch 105/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7296 - precision_3: 0.6313 - recall_3: 0.5305\n",
            "Epoch 106/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5463 - accuracy: 0.7329 - precision_3: 0.6601 - recall_3: 0.4742\n",
            "Epoch 107/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7378 - precision_3: 0.6529 - recall_3: 0.5211\n",
            "Epoch 108/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5327 - accuracy: 0.7443 - precision_3: 0.6842 - recall_3: 0.4883\n",
            "Epoch 109/150\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5319 - accuracy: 0.7362 - precision_3: 0.6624 - recall_3: 0.4883\n",
            "Epoch 110/150\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5386 - accuracy: 0.7427 - precision_3: 0.6730 - recall_3: 0.5023\n",
            "Epoch 111/150\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5319 - accuracy: 0.7541 - precision_3: 0.6914 - recall_3: 0.5258\n",
            "Epoch 112/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6125 - accuracy: 0.7541 - precision_3: 0.6824 - recall_3: 0.5446\n",
            "Epoch 113/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7622 - precision_3: 0.7219 - recall_3: 0.5117\n",
            "Epoch 114/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7492 - precision_3: 0.6954 - recall_3: 0.4930\n",
            "Epoch 115/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7394 - precision_3: 0.6710 - recall_3: 0.4883\n",
            "Epoch 116/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7476 - precision_3: 0.6883 - recall_3: 0.4977\n",
            "Epoch 117/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7606 - precision_3: 0.7037 - recall_3: 0.5352\n",
            "Epoch 118/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7524 - precision_3: 0.6685 - recall_3: 0.5681\n",
            "Epoch 119/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7573 - precision_3: 0.7133 - recall_3: 0.5023\n",
            "Epoch 120/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7492 - precision_3: 0.6725 - recall_3: 0.5399\n",
            "Epoch 121/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.7215 - precision_3: 0.6296 - recall_3: 0.4789\n",
            "Epoch 122/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7622 - precision_3: 0.7161 - recall_3: 0.5211\n",
            "Epoch 123/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5169 - accuracy: 0.7378 - precision_3: 0.6831 - recall_3: 0.4554\n",
            "Epoch 124/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5653 - accuracy: 0.7345 - precision_3: 0.6344 - recall_3: 0.5540\n",
            "Epoch 125/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7443 - precision_3: 0.7029 - recall_3: 0.4554\n",
            "Epoch 126/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7345 - precision_3: 0.6524 - recall_3: 0.5023\n",
            "Epoch 127/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7557 - precision_3: 0.6957 - recall_3: 0.5258\n",
            "Epoch 128/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5561 - accuracy: 0.7231 - precision_3: 0.6303 - recall_3: 0.4883\n",
            "Epoch 129/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7427 - precision_3: 0.6752 - recall_3: 0.4977\n",
            "Epoch 130/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7622 - precision_3: 0.7055 - recall_3: 0.5399\n",
            "Epoch 131/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7573 - precision_3: 0.7133 - recall_3: 0.5023\n",
            "Epoch 132/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7541 - precision_3: 0.7039 - recall_3: 0.5023\n",
            "Epoch 133/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5018 - accuracy: 0.7410 - precision_3: 0.6627 - recall_3: 0.5164\n",
            "Epoch 134/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.7329 - precision_3: 0.6581 - recall_3: 0.4789\n",
            "Epoch 135/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.7541 - precision_3: 0.6742 - recall_3: 0.5634\n",
            "Epoch 136/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7459 - precision_3: 0.6815 - recall_3: 0.5023\n",
            "Epoch 137/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7801 - precision_3: 0.7566 - recall_3: 0.5399\n",
            "Epoch 138/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7427 - precision_3: 0.6608 - recall_3: 0.5305\n",
            "Epoch 139/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7443 - precision_3: 0.6750 - recall_3: 0.5070\n",
            "Epoch 140/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7638 - precision_3: 0.7208 - recall_3: 0.5211\n",
            "Epoch 141/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.7524 - precision_3: 0.6894 - recall_3: 0.5211\n",
            "Epoch 142/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.7752 - precision_3: 0.7451 - recall_3: 0.5352\n",
            "Epoch 143/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5018 - accuracy: 0.7427 - precision_3: 0.6871 - recall_3: 0.4742\n",
            "Epoch 144/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.7590 - precision_3: 0.7181 - recall_3: 0.5023\n",
            "Epoch 145/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.7573 - precision_3: 0.7000 - recall_3: 0.5258\n",
            "Epoch 146/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7492 - precision_3: 0.6810 - recall_3: 0.5211\n",
            "Epoch 147/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5218 - accuracy: 0.7362 - precision_3: 0.6584 - recall_3: 0.4977\n",
            "Epoch 148/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5641 - accuracy: 0.7248 - precision_3: 0.6294 - recall_3: 0.5023\n",
            "Epoch 149/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5864 - accuracy: 0.7264 - precision_3: 0.6552 - recall_3: 0.4460\n",
            "Epoch 150/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.5145 - accuracy: 0.7573 - precision_3: 0.6758 - recall_3: 0.5775\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6240 - accuracy: 0.6688 - precision_3: 0.5526 - recall_3: 0.3818\n",
            "Accuracy: 66.88%\n",
            "Precision: 55.26%\n",
            "Recall: 38.18%\n"
          ]
        }
      ],
      "source": [
        "eval_original_df = df\n",
        "eval_augmented_df = final_df\n",
        "\n",
        "if 'id' in eval_original_df.columns:\n",
        "  eval_original_df = df.drop(columns=['id'])\n",
        "  eval_augmented_df = final_df.drop(columns=['id'])\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras.metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "# data = pd.read_csv('path/to/dataset.csv')\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "test_size = 0.2\n",
        "train_data, test_data = train_test_split(eval_original_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_data)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "\n"
      ],
      "id": "8j2Gng6G0RcV"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "woXDJmn21s5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e7fa39-86c3-414c-d933-e8a38ec018e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin        BMI  \\\n",
            "0            6.0    148.0           72.0           35.0      0.0  33.600000   \n",
            "1            1.0     85.0           66.0           29.0      0.0  26.600000   \n",
            "2            8.0    183.0           64.0            0.0      0.0  23.300000   \n",
            "3            1.0     89.0           66.0           23.0     94.0  28.100000   \n",
            "4            0.0    137.0           40.0           35.0    168.0  43.100000   \n",
            "..           ...      ...            ...            ...      ...        ...   \n",
            "455          2.0    107.0           68.0           19.0      3.0  30.090891   \n",
            "456         10.0    180.0           90.0           44.0    294.0  44.128829   \n",
            "457          2.0    100.0           64.0           13.0      1.0  27.874374   \n",
            "458          3.0    116.0           72.0           25.0     64.0  32.307407   \n",
            "459          8.0    161.0           86.0           40.0    207.0  40.568969   \n",
            "\n",
            "     DiabetesPedigreeFunction   Age  Outcome  \n",
            "0                    0.627000  50.0      1.0  \n",
            "1                    0.351000  31.0      0.0  \n",
            "2                    0.672000  32.0      1.0  \n",
            "3                    0.167000  21.0      0.0  \n",
            "4                    2.288000  33.0      1.0  \n",
            "..                        ...   ...      ...  \n",
            "455                  0.328845  27.0      0.0  \n",
            "456                  1.088412  57.0      1.0  \n",
            "457                  0.267892  25.0      0.0  \n",
            "458                  0.406208  30.0      0.0  \n",
            "459                  0.846945  49.0      1.0  \n",
            "\n",
            "[1228 rows x 9 columns]\n",
            "Epoch 1/150\n",
            "123/123 [==============================] - 1s 2ms/step - loss: 2.5314 - accuracy: 0.6963 - precision_4: 0.5757 - recall_4: 0.4965\n",
            "Epoch 2/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.6626 - accuracy: 0.7085 - precision_4: 0.5868 - recall_4: 0.5594\n",
            "Epoch 3/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.6003 - accuracy: 0.7296 - precision_4: 0.6228 - recall_4: 0.5734\n",
            "Epoch 4/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.7402 - precision_4: 0.6463 - recall_4: 0.5664\n",
            "Epoch 5/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7590 - precision_4: 0.6727 - recall_4: 0.6037\n",
            "Epoch 6/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7459 - precision_4: 0.6496 - recall_4: 0.5921\n",
            "Epoch 7/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7638 - precision_4: 0.6787 - recall_4: 0.6154\n",
            "Epoch 8/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7484 - precision_4: 0.6485 - recall_4: 0.6107\n",
            "Epoch 9/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7598 - precision_4: 0.6718 - recall_4: 0.6107\n",
            "Epoch 10/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4989 - accuracy: 0.7647 - precision_4: 0.6759 - recall_4: 0.6270\n",
            "Epoch 11/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7573 - precision_4: 0.6642 - recall_4: 0.6177\n",
            "Epoch 12/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.7834 - precision_4: 0.7128 - recall_4: 0.6364\n",
            "Epoch 13/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.7858 - precision_4: 0.7005 - recall_4: 0.6760\n",
            "Epoch 14/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7769 - precision_4: 0.6972 - recall_4: 0.6387\n",
            "Epoch 15/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.7793 - precision_4: 0.6965 - recall_4: 0.6527\n",
            "Epoch 16/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.7923 - precision_4: 0.7266 - recall_4: 0.6503\n",
            "Epoch 17/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7834 - precision_4: 0.7032 - recall_4: 0.6573\n",
            "Epoch 18/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.8062 - precision_4: 0.7406 - recall_4: 0.6853\n",
            "Epoch 19/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7907 - precision_4: 0.7299 - recall_4: 0.6364\n",
            "Epoch 20/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.7866 - precision_4: 0.7147 - recall_4: 0.6480\n",
            "Epoch 21/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7858 - precision_4: 0.7107 - recall_4: 0.6527\n",
            "Epoch 22/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.7875 - precision_4: 0.7090 - recall_4: 0.6643\n",
            "Epoch 23/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7964 - precision_4: 0.7266 - recall_4: 0.6690\n",
            "Epoch 24/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.7956 - precision_4: 0.7247 - recall_4: 0.6690\n",
            "Epoch 25/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4506 - accuracy: 0.8168 - precision_4: 0.7488 - recall_4: 0.7156\n",
            "Epoch 26/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.7989 - precision_4: 0.7264 - recall_4: 0.6807\n",
            "Epoch 27/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.8070 - precision_4: 0.7474 - recall_4: 0.6760\n",
            "Epoch 28/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4565 - accuracy: 0.7940 - precision_4: 0.7211 - recall_4: 0.6690\n",
            "Epoch 29/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4424 - accuracy: 0.8094 - precision_4: 0.7396 - recall_4: 0.7016\n",
            "Epoch 30/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.8029 - precision_4: 0.7429 - recall_4: 0.6667\n",
            "Epoch 31/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.8086 - precision_4: 0.7474 - recall_4: 0.6830\n",
            "Epoch 32/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8111 - precision_4: 0.7519 - recall_4: 0.6853\n",
            "Epoch 33/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4474 - accuracy: 0.8070 - precision_4: 0.7462 - recall_4: 0.6783\n",
            "Epoch 34/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.8176 - precision_4: 0.7635 - recall_4: 0.6923\n",
            "Epoch 35/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4323 - accuracy: 0.8078 - precision_4: 0.7506 - recall_4: 0.6737\n",
            "Epoch 36/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.8143 - precision_4: 0.7666 - recall_4: 0.6737\n",
            "Epoch 37/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4468 - accuracy: 0.8005 - precision_4: 0.7266 - recall_4: 0.6876\n",
            "Epoch 38/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.8062 - precision_4: 0.7393 - recall_4: 0.6876\n",
            "Epoch 39/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4370 - accuracy: 0.8086 - precision_4: 0.7500 - recall_4: 0.6783\n",
            "Epoch 40/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8241 - precision_4: 0.7696 - recall_4: 0.7086\n",
            "Epoch 41/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4230 - accuracy: 0.8200 - precision_4: 0.7574 - recall_4: 0.7133\n",
            "Epoch 42/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8184 - precision_4: 0.7588 - recall_4: 0.7040\n",
            "Epoch 43/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8119 - precision_4: 0.7578 - recall_4: 0.6783\n",
            "Epoch 44/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8119 - precision_4: 0.7538 - recall_4: 0.6853\n",
            "Epoch 45/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.8265 - precision_4: 0.7755 - recall_4: 0.7086\n",
            "Epoch 46/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.8168 - precision_4: 0.7670 - recall_4: 0.6830\n",
            "Epoch 47/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8208 - precision_4: 0.7772 - recall_4: 0.6830\n",
            "Epoch 48/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8135 - precision_4: 0.7513 - recall_4: 0.6970\n",
            "Epoch 49/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8111 - precision_4: 0.7599 - recall_4: 0.6713\n",
            "Epoch 50/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8200 - precision_4: 0.7613 - recall_4: 0.7063\n",
            "Epoch 51/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4155 - accuracy: 0.8192 - precision_4: 0.7647 - recall_4: 0.6970\n",
            "Epoch 52/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8241 - precision_4: 0.7724 - recall_4: 0.7040\n",
            "Epoch 53/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8274 - precision_4: 0.7848 - recall_4: 0.6970\n",
            "Epoch 54/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8176 - precision_4: 0.7649 - recall_4: 0.6900\n",
            "Epoch 55/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8363 - precision_4: 0.7953 - recall_4: 0.7156\n",
            "Epoch 56/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8208 - precision_4: 0.7646 - recall_4: 0.7040\n",
            "Epoch 57/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4053 - accuracy: 0.8208 - precision_4: 0.7673 - recall_4: 0.6993\n",
            "Epoch 58/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8379 - precision_4: 0.8059 - recall_4: 0.7063\n",
            "Epoch 59/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.8314 - precision_4: 0.7861 - recall_4: 0.7110\n",
            "Epoch 60/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8225 - precision_4: 0.7518 - recall_4: 0.7343\n",
            "Epoch 61/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8347 - precision_4: 0.7912 - recall_4: 0.7156\n",
            "Epoch 62/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.8257 - precision_4: 0.7708 - recall_4: 0.7133\n",
            "Epoch 63/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8241 - precision_4: 0.7724 - recall_4: 0.7040\n",
            "Epoch 64/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8282 - precision_4: 0.7839 - recall_4: 0.7016\n",
            "Epoch 65/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8371 - precision_4: 0.7855 - recall_4: 0.7343\n",
            "Epoch 66/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.8282 - precision_4: 0.7753 - recall_4: 0.7156\n",
            "Epoch 67/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8298 - precision_4: 0.7778 - recall_4: 0.7179\n",
            "Epoch 68/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8371 - precision_4: 0.7943 - recall_4: 0.7203\n",
            "Epoch 69/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8363 - precision_4: 0.7969 - recall_4: 0.7133\n",
            "Epoch 70/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8265 - precision_4: 0.7727 - recall_4: 0.7133\n",
            "Epoch 71/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8371 - precision_4: 0.7841 - recall_4: 0.7366\n",
            "Epoch 72/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.8282 - precision_4: 0.7795 - recall_4: 0.7086\n",
            "Epoch 73/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.8371 - precision_4: 0.7913 - recall_4: 0.7249\n",
            "Epoch 74/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8461 - precision_4: 0.8015 - recall_4: 0.7436\n",
            "Epoch 75/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3773 - accuracy: 0.8363 - precision_4: 0.7836 - recall_4: 0.7343\n",
            "Epoch 76/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8290 - precision_4: 0.7801 - recall_4: 0.7110\n",
            "Epoch 77/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8257 - precision_4: 0.7763 - recall_4: 0.7040\n",
            "Epoch 78/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8428 - precision_4: 0.7850 - recall_4: 0.7576\n",
            "Epoch 79/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8412 - precision_4: 0.7955 - recall_4: 0.7343\n",
            "Epoch 80/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8469 - precision_4: 0.8051 - recall_4: 0.7413\n",
            "Epoch 81/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8436 - precision_4: 0.8000 - recall_4: 0.7366\n",
            "Epoch 82/150\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 0.3746 - accuracy: 0.8420 - precision_4: 0.7990 - recall_4: 0.7319\n",
            "Epoch 83/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3717 - accuracy: 0.8371 - precision_4: 0.7827 - recall_4: 0.7389\n",
            "Epoch 84/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8428 - precision_4: 0.7980 - recall_4: 0.7366\n",
            "Epoch 85/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8428 - precision_4: 0.8089 - recall_4: 0.7203\n",
            "Epoch 86/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3612 - accuracy: 0.8461 - precision_4: 0.7970 - recall_4: 0.7506\n",
            "Epoch 87/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3594 - accuracy: 0.8436 - precision_4: 0.7985 - recall_4: 0.7389\n",
            "Epoch 88/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8396 - precision_4: 0.8021 - recall_4: 0.7179\n",
            "Epoch 89/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.8412 - precision_4: 0.7940 - recall_4: 0.7366\n",
            "Epoch 90/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.8420 - precision_4: 0.7990 - recall_4: 0.7319\n",
            "Epoch 91/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3563 - accuracy: 0.8461 - precision_4: 0.7941 - recall_4: 0.7552\n",
            "Epoch 92/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8355 - precision_4: 0.7802 - recall_4: 0.7366\n",
            "Epoch 93/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8518 - precision_4: 0.8159 - recall_4: 0.7436\n",
            "Epoch 94/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8542 - precision_4: 0.8049 - recall_4: 0.7692\n",
            "Epoch 95/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3559 - accuracy: 0.8404 - precision_4: 0.7949 - recall_4: 0.7319\n",
            "Epoch 96/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8502 - precision_4: 0.8117 - recall_4: 0.7436\n",
            "Epoch 97/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3642 - accuracy: 0.8461 - precision_4: 0.8046 - recall_4: 0.7389\n",
            "Epoch 98/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.8469 - precision_4: 0.8035 - recall_4: 0.7436\n",
            "Epoch 99/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8493 - precision_4: 0.8035 - recall_4: 0.7529\n",
            "Epoch 100/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3633 - accuracy: 0.8420 - precision_4: 0.7873 - recall_4: 0.7506\n",
            "Epoch 101/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.8550 - precision_4: 0.8243 - recall_4: 0.7436\n",
            "Epoch 102/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8412 - precision_4: 0.7970 - recall_4: 0.7319\n",
            "Epoch 103/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8396 - precision_4: 0.7929 - recall_4: 0.7319\n",
            "Epoch 104/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3478 - accuracy: 0.8493 - precision_4: 0.8081 - recall_4: 0.7459\n",
            "Epoch 105/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.8583 - precision_4: 0.8195 - recall_4: 0.7622\n",
            "Epoch 106/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8493 - precision_4: 0.8128 - recall_4: 0.7389\n",
            "Epoch 107/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3537 - accuracy: 0.8469 - precision_4: 0.7961 - recall_4: 0.7552\n",
            "Epoch 108/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.8453 - precision_4: 0.7880 - recall_4: 0.7622\n",
            "Epoch 109/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3571 - accuracy: 0.8518 - precision_4: 0.8111 - recall_4: 0.7506\n",
            "Epoch 110/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3577 - accuracy: 0.8436 - precision_4: 0.7970 - recall_4: 0.7413\n",
            "Epoch 111/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3504 - accuracy: 0.8607 - precision_4: 0.8177 - recall_4: 0.7739\n",
            "Epoch 112/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8453 - precision_4: 0.8041 - recall_4: 0.7366\n",
            "Epoch 113/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8518 - precision_4: 0.8159 - recall_4: 0.7436\n",
            "Epoch 114/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8485 - precision_4: 0.8123 - recall_4: 0.7366\n",
            "Epoch 115/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8518 - precision_4: 0.7990 - recall_4: 0.7692\n",
            "Epoch 116/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8550 - precision_4: 0.8260 - recall_4: 0.7413\n",
            "Epoch 117/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8453 - precision_4: 0.8056 - recall_4: 0.7343\n",
            "Epoch 118/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.8477 - precision_4: 0.8010 - recall_4: 0.7506\n",
            "Epoch 119/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8502 - precision_4: 0.8070 - recall_4: 0.7506\n",
            "Epoch 120/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8534 - precision_4: 0.8217 - recall_4: 0.7413\n",
            "Epoch 121/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8599 - precision_4: 0.8253 - recall_4: 0.7599\n",
            "Epoch 122/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8526 - precision_4: 0.8246 - recall_4: 0.7343\n",
            "Epoch 123/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8493 - precision_4: 0.8035 - recall_4: 0.7529\n",
            "Epoch 124/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3411 - accuracy: 0.8461 - precision_4: 0.8077 - recall_4: 0.7343\n",
            "Epoch 125/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3381 - accuracy: 0.8583 - precision_4: 0.8244 - recall_4: 0.7552\n",
            "Epoch 126/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8477 - precision_4: 0.8071 - recall_4: 0.7413\n",
            "Epoch 127/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3394 - accuracy: 0.8559 - precision_4: 0.8150 - recall_4: 0.7599\n",
            "Epoch 128/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8526 - precision_4: 0.8163 - recall_4: 0.7459\n",
            "Epoch 129/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8477 - precision_4: 0.8010 - recall_4: 0.7506\n",
            "Epoch 130/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.8567 - precision_4: 0.8252 - recall_4: 0.7483\n",
            "Epoch 131/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3432 - accuracy: 0.8461 - precision_4: 0.8077 - recall_4: 0.7343\n",
            "Epoch 132/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8526 - precision_4: 0.8147 - recall_4: 0.7483\n",
            "Epoch 133/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3459 - accuracy: 0.8420 - precision_4: 0.7916 - recall_4: 0.7436\n",
            "Epoch 134/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3410 - accuracy: 0.8583 - precision_4: 0.8295 - recall_4: 0.7483\n",
            "Epoch 135/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8469 - precision_4: 0.8035 - recall_4: 0.7436\n",
            "Epoch 136/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3466 - accuracy: 0.8510 - precision_4: 0.8122 - recall_4: 0.7459\n",
            "Epoch 137/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3509 - accuracy: 0.8469 - precision_4: 0.8051 - recall_4: 0.7413\n",
            "Epoch 138/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8502 - precision_4: 0.8133 - recall_4: 0.7413\n",
            "Epoch 139/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8567 - precision_4: 0.8139 - recall_4: 0.7646\n",
            "Epoch 140/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8550 - precision_4: 0.8084 - recall_4: 0.7669\n",
            "Epoch 141/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8534 - precision_4: 0.8074 - recall_4: 0.7622\n",
            "Epoch 142/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8371 - precision_4: 0.7884 - recall_4: 0.7296\n",
            "Epoch 143/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8591 - precision_4: 0.8249 - recall_4: 0.7576\n",
            "Epoch 144/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3363 - accuracy: 0.8640 - precision_4: 0.8195 - recall_4: 0.7832\n",
            "Epoch 145/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8493 - precision_4: 0.8144 - recall_4: 0.7366\n",
            "Epoch 146/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8583 - precision_4: 0.8244 - recall_4: 0.7552\n",
            "Epoch 147/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8510 - precision_4: 0.8045 - recall_4: 0.7576\n",
            "Epoch 148/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3282 - accuracy: 0.8599 - precision_4: 0.8270 - recall_4: 0.7576\n",
            "Epoch 149/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8583 - precision_4: 0.8133 - recall_4: 0.7716\n",
            "Epoch 150/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8510 - precision_4: 0.8170 - recall_4: 0.7389\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9708 - precision_4: 0.9211 - recall_4: 1.0000\n",
            "Accuracy: 97.08%\n",
            "Precision: 92.11%\n",
            "Recall: 100.00%\n"
          ]
        }
      ],
      "source": [
        "test_size = 0.2\n",
        "train_data, test_data = train_test_split(eval_augmented_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "# last column\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_data)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "\n"
      ],
      "id": "woXDJmn21s5Y"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XQiPojfz22mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5317c0-ffd1-4f16-dadd-937c6cfeaeb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "77/77 [==============================] - 1s 3ms/step - loss: 8.2853 - accuracy: 0.6042 - precision_5: 0.4100 - recall_5: 0.3060\n",
            "Epoch 2/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 2.0348 - accuracy: 0.5729 - precision_5: 0.4074 - recall_5: 0.4925\n",
            "Epoch 3/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.5763 - accuracy: 0.6016 - precision_5: 0.4336 - recall_5: 0.4627\n",
            "Epoch 4/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.2854 - accuracy: 0.6042 - precision_5: 0.4333 - recall_5: 0.4366\n",
            "Epoch 5/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.2251 - accuracy: 0.6094 - precision_5: 0.4444 - recall_5: 0.4776\n",
            "Epoch 6/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.9480 - accuracy: 0.6393 - precision_5: 0.4831 - recall_5: 0.4813\n",
            "Epoch 7/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8679 - accuracy: 0.6406 - precision_5: 0.4845 - recall_5: 0.4664\n",
            "Epoch 8/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8404 - accuracy: 0.6341 - precision_5: 0.4762 - recall_5: 0.4851\n",
            "Epoch 9/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7521 - accuracy: 0.6549 - precision_5: 0.5058 - recall_5: 0.4888\n",
            "Epoch 10/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.6523 - precision_5: 0.5022 - recall_5: 0.4291\n",
            "Epoch 11/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6972 - accuracy: 0.6680 - precision_5: 0.5286 - recall_5: 0.4478\n",
            "Epoch 12/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7350 - accuracy: 0.6341 - precision_5: 0.4733 - recall_5: 0.4291\n",
            "Epoch 13/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.6589 - precision_5: 0.5135 - recall_5: 0.4254\n",
            "Epoch 14/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.6771 - precision_5: 0.5420 - recall_5: 0.4813\n",
            "Epoch 15/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6456 - accuracy: 0.6797 - precision_5: 0.5500 - recall_5: 0.4515\n",
            "Epoch 16/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6351 - accuracy: 0.6901 - precision_5: 0.5652 - recall_5: 0.4851\n",
            "Epoch 17/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6261 - accuracy: 0.6927 - precision_5: 0.5696 - recall_5: 0.4888\n",
            "Epoch 18/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6814 - accuracy: 0.6628 - precision_5: 0.5197 - recall_5: 0.4440\n",
            "Epoch 19/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.6745 - precision_5: 0.5425 - recall_5: 0.4291\n",
            "Epoch 20/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.6862 - precision_5: 0.5611 - recall_5: 0.4627\n",
            "Epoch 21/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6082 - accuracy: 0.7044 - precision_5: 0.5945 - recall_5: 0.4813\n",
            "Epoch 22/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.7018 - precision_5: 0.5924 - recall_5: 0.4664\n",
            "Epoch 23/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6545 - accuracy: 0.6901 - precision_5: 0.5708 - recall_5: 0.4515\n",
            "Epoch 24/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.7148 - precision_5: 0.6079 - recall_5: 0.5149\n",
            "Epoch 25/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6222 - accuracy: 0.6914 - precision_5: 0.5677 - recall_5: 0.4851\n",
            "Epoch 26/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6159 - accuracy: 0.6810 - precision_5: 0.5507 - recall_5: 0.4664\n",
            "Epoch 27/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.6966 - precision_5: 0.5745 - recall_5: 0.5037\n",
            "Epoch 28/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.6836 - precision_5: 0.5546 - recall_5: 0.4739\n",
            "Epoch 29/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6597 - accuracy: 0.6706 - precision_5: 0.5311 - recall_5: 0.4776\n",
            "Epoch 30/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5840 - accuracy: 0.6979 - precision_5: 0.5804 - recall_5: 0.4851\n",
            "Epoch 31/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5808 - accuracy: 0.7031 - precision_5: 0.5962 - recall_5: 0.4627\n",
            "Epoch 32/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6901 - precision_5: 0.5600 - recall_5: 0.5224\n",
            "Epoch 33/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5952 - accuracy: 0.6849 - precision_5: 0.5613 - recall_5: 0.4440\n",
            "Epoch 34/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6063 - accuracy: 0.6992 - precision_5: 0.5869 - recall_5: 0.4664\n",
            "Epoch 35/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6228 - accuracy: 0.6823 - precision_5: 0.5469 - recall_5: 0.5224\n",
            "Epoch 36/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.7044 - precision_5: 0.5895 - recall_5: 0.5037\n",
            "Epoch 37/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.7148 - precision_5: 0.6184 - recall_5: 0.4776\n",
            "Epoch 38/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5693 - accuracy: 0.7044 - precision_5: 0.5903 - recall_5: 0.5000\n",
            "Epoch 39/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5672 - accuracy: 0.7240 - precision_5: 0.6273 - recall_5: 0.5149\n",
            "Epoch 40/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5937 - accuracy: 0.7227 - precision_5: 0.6201 - recall_5: 0.5299\n",
            "Epoch 41/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5806 - accuracy: 0.7083 - precision_5: 0.5932 - recall_5: 0.5224\n",
            "Epoch 42/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5885 - accuracy: 0.6992 - precision_5: 0.5787 - recall_5: 0.5075\n",
            "Epoch 43/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5598 - accuracy: 0.7318 - precision_5: 0.6476 - recall_5: 0.5075\n",
            "Epoch 44/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5669 - accuracy: 0.7135 - precision_5: 0.6165 - recall_5: 0.4739\n",
            "Epoch 45/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5573 - accuracy: 0.7201 - precision_5: 0.6233 - recall_5: 0.5000\n",
            "Epoch 46/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5577 - accuracy: 0.7044 - precision_5: 0.5887 - recall_5: 0.5075\n",
            "Epoch 47/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5660 - accuracy: 0.7148 - precision_5: 0.6099 - recall_5: 0.5075\n",
            "Epoch 48/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5769 - accuracy: 0.7253 - precision_5: 0.6351 - recall_5: 0.5000\n",
            "Epoch 49/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.7201 - precision_5: 0.6268 - recall_5: 0.4888\n",
            "Epoch 50/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5924 - accuracy: 0.7057 - precision_5: 0.5946 - recall_5: 0.4925\n",
            "Epoch 51/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5406 - accuracy: 0.7174 - precision_5: 0.6175 - recall_5: 0.5000\n",
            "Epoch 52/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5474 - accuracy: 0.7057 - precision_5: 0.6019 - recall_5: 0.4627\n",
            "Epoch 53/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5559 - accuracy: 0.7201 - precision_5: 0.6178 - recall_5: 0.5187\n",
            "Epoch 54/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5701 - accuracy: 0.7005 - precision_5: 0.5888 - recall_5: 0.4701\n",
            "Epoch 55/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6547 - accuracy: 0.6862 - precision_5: 0.5574 - recall_5: 0.4888\n",
            "Epoch 56/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5540 - accuracy: 0.7161 - precision_5: 0.6059 - recall_5: 0.5336\n",
            "Epoch 57/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7292 - precision_5: 0.6376 - recall_5: 0.5187\n",
            "Epoch 58/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7305 - precision_5: 0.6380 - recall_5: 0.5261\n",
            "Epoch 59/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.7122 - precision_5: 0.6044 - recall_5: 0.5075\n",
            "Epoch 60/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.7174 - precision_5: 0.6143 - recall_5: 0.5112\n",
            "Epoch 61/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7344 - precision_5: 0.6429 - recall_5: 0.5373\n",
            "Epoch 62/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5677 - accuracy: 0.7057 - precision_5: 0.5963 - recall_5: 0.4851\n",
            "Epoch 63/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.7253 - precision_5: 0.6256 - recall_5: 0.5299\n",
            "Epoch 64/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7370 - precision_5: 0.6514 - recall_5: 0.5299\n",
            "Epoch 65/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5465 - accuracy: 0.7318 - precision_5: 0.6396 - recall_5: 0.5299\n",
            "Epoch 66/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7344 - precision_5: 0.6538 - recall_5: 0.5075\n",
            "Epoch 67/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7448 - precision_5: 0.6607 - recall_5: 0.5522\n",
            "Epoch 68/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7135 - precision_5: 0.6081 - recall_5: 0.5037\n",
            "Epoch 69/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5442 - accuracy: 0.7279 - precision_5: 0.6439 - recall_5: 0.4925\n",
            "Epoch 70/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5660 - accuracy: 0.7253 - precision_5: 0.6213 - recall_5: 0.5448\n",
            "Epoch 71/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5762 - accuracy: 0.7253 - precision_5: 0.6213 - recall_5: 0.5448\n",
            "Epoch 72/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.7201 - precision_5: 0.6210 - recall_5: 0.5075\n",
            "Epoch 73/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7292 - precision_5: 0.6429 - recall_5: 0.5037\n",
            "Epoch 74/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7331 - precision_5: 0.6413 - recall_5: 0.5336\n",
            "Epoch 75/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7083 - precision_5: 0.5924 - recall_5: 0.5261\n",
            "Epoch 76/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5528 - accuracy: 0.7240 - precision_5: 0.6284 - recall_5: 0.5112\n",
            "Epoch 77/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7409 - precision_5: 0.6561 - recall_5: 0.5410\n",
            "Epoch 78/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7318 - precision_5: 0.6336 - recall_5: 0.5485\n",
            "Epoch 79/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5710 - accuracy: 0.7122 - precision_5: 0.5992 - recall_5: 0.5299\n",
            "Epoch 80/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7240 - precision_5: 0.6148 - recall_5: 0.5597\n",
            "Epoch 81/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.7253 - precision_5: 0.6267 - recall_5: 0.5261\n",
            "Epoch 82/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7214 - precision_5: 0.6250 - recall_5: 0.5037\n",
            "Epoch 83/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5720 - accuracy: 0.7227 - precision_5: 0.6190 - recall_5: 0.5336\n",
            "Epoch 84/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5254 - accuracy: 0.7357 - precision_5: 0.6457 - recall_5: 0.5373\n",
            "Epoch 85/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7253 - precision_5: 0.6223 - recall_5: 0.5410\n",
            "Epoch 86/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7318 - precision_5: 0.6550 - recall_5: 0.4888\n",
            "Epoch 87/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5927 - accuracy: 0.6979 - precision_5: 0.5849 - recall_5: 0.4627\n",
            "Epoch 88/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7370 - precision_5: 0.6528 - recall_5: 0.5261\n",
            "Epoch 89/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.7253 - precision_5: 0.6173 - recall_5: 0.5597\n",
            "Epoch 90/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5589 - accuracy: 0.7448 - precision_5: 0.6714 - recall_5: 0.5261\n",
            "Epoch 91/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.7422 - precision_5: 0.6667 - recall_5: 0.5224\n",
            "Epoch 92/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5359 - accuracy: 0.7487 - precision_5: 0.6728 - recall_5: 0.5448\n",
            "Epoch 93/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5574 - accuracy: 0.7318 - precision_5: 0.6384 - recall_5: 0.5336\n",
            "Epoch 94/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7396 - precision_5: 0.6560 - recall_5: 0.5336\n",
            "Epoch 95/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7474 - precision_5: 0.6637 - recall_5: 0.5597\n",
            "Epoch 96/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7318 - precision_5: 0.6409 - recall_5: 0.5261\n",
            "Epoch 97/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7422 - precision_5: 0.6591 - recall_5: 0.5410\n",
            "Epoch 98/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.7435 - precision_5: 0.6461 - recall_5: 0.5858\n",
            "Epoch 99/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5394 - accuracy: 0.7253 - precision_5: 0.6377 - recall_5: 0.4925\n",
            "Epoch 100/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7422 - precision_5: 0.6471 - recall_5: 0.5746\n",
            "Epoch 101/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7383 - precision_5: 0.6516 - recall_5: 0.5373\n",
            "Epoch 102/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7279 - precision_5: 0.6300 - recall_5: 0.5336\n",
            "Epoch 103/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5649 - accuracy: 0.7396 - precision_5: 0.6604 - recall_5: 0.5224\n",
            "Epoch 104/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7409 - precision_5: 0.6533 - recall_5: 0.5485\n",
            "Epoch 105/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7214 - precision_5: 0.6250 - recall_5: 0.5037\n",
            "Epoch 106/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.7435 - precision_5: 0.6802 - recall_5: 0.5000\n",
            "Epoch 107/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.7500 - precision_5: 0.6792 - recall_5: 0.5373\n",
            "Epoch 108/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7565 - precision_5: 0.6833 - recall_5: 0.5634\n",
            "Epoch 109/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5385 - accuracy: 0.7344 - precision_5: 0.6455 - recall_5: 0.5299\n",
            "Epoch 110/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7253 - precision_5: 0.6267 - recall_5: 0.5261\n",
            "Epoch 111/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.7448 - precision_5: 0.6593 - recall_5: 0.5560\n",
            "Epoch 112/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5057 - accuracy: 0.7370 - precision_5: 0.6587 - recall_5: 0.5112\n",
            "Epoch 113/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5273 - accuracy: 0.7292 - precision_5: 0.6429 - recall_5: 0.5037\n",
            "Epoch 114/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.7513 - precision_5: 0.6667 - recall_5: 0.5746\n",
            "Epoch 115/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5138 - accuracy: 0.7370 - precision_5: 0.6460 - recall_5: 0.5448\n",
            "Epoch 116/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5380 - accuracy: 0.7214 - precision_5: 0.6227 - recall_5: 0.5112\n",
            "Epoch 117/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7461 - precision_5: 0.6667 - recall_5: 0.5448\n",
            "Epoch 118/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7461 - precision_5: 0.6853 - recall_5: 0.5037\n",
            "Epoch 119/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5194 - accuracy: 0.7383 - precision_5: 0.6379 - recall_5: 0.5784\n",
            "Epoch 120/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4985 - accuracy: 0.7461 - precision_5: 0.6714 - recall_5: 0.5336\n",
            "Epoch 121/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4963 - accuracy: 0.7513 - precision_5: 0.6758 - recall_5: 0.5522\n",
            "Epoch 122/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5171 - accuracy: 0.7487 - precision_5: 0.6761 - recall_5: 0.5373\n",
            "Epoch 123/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4965 - accuracy: 0.7591 - precision_5: 0.7005 - recall_5: 0.5410\n",
            "Epoch 124/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5002 - accuracy: 0.7565 - precision_5: 0.6833 - recall_5: 0.5634\n",
            "Epoch 125/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4970 - accuracy: 0.7643 - precision_5: 0.7062 - recall_5: 0.5560\n",
            "Epoch 126/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5452 - accuracy: 0.7331 - precision_5: 0.6376 - recall_5: 0.5448\n",
            "Epoch 127/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.5159 - accuracy: 0.7461 - precision_5: 0.6667 - recall_5: 0.5448\n",
            "Epoch 128/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5058 - accuracy: 0.7591 - precision_5: 0.6797 - recall_5: 0.5858\n",
            "Epoch 129/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5142 - accuracy: 0.7344 - precision_5: 0.6404 - recall_5: 0.5448\n",
            "Epoch 130/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5096 - accuracy: 0.7409 - precision_5: 0.6635 - recall_5: 0.5224\n",
            "Epoch 131/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7370 - precision_5: 0.6486 - recall_5: 0.5373\n",
            "Epoch 132/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7357 - precision_5: 0.6383 - recall_5: 0.5597\n",
            "Epoch 133/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7578 - precision_5: 0.6916 - recall_5: 0.5522\n",
            "Epoch 134/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5251 - accuracy: 0.7500 - precision_5: 0.6610 - recall_5: 0.5821\n",
            "Epoch 135/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7591 - precision_5: 0.6861 - recall_5: 0.5709\n",
            "Epoch 136/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.7240 - precision_5: 0.6217 - recall_5: 0.5336\n",
            "Epoch 137/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.7188 - precision_5: 0.6226 - recall_5: 0.4925\n",
            "Epoch 138/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.7708 - precision_5: 0.7130 - recall_5: 0.5746\n",
            "Epoch 139/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7409 - precision_5: 0.6605 - recall_5: 0.5299\n",
            "Epoch 140/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7474 - precision_5: 0.6729 - recall_5: 0.5373\n",
            "Epoch 141/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7604 - precision_5: 0.6826 - recall_5: 0.5858\n",
            "Epoch 142/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7526 - precision_5: 0.6773 - recall_5: 0.5560\n",
            "Epoch 143/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7656 - precision_5: 0.7000 - recall_5: 0.5746\n",
            "Epoch 144/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7630 - precision_5: 0.6903 - recall_5: 0.5821\n",
            "Epoch 145/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7487 - precision_5: 0.6761 - recall_5: 0.5373\n",
            "Epoch 146/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.7539 - precision_5: 0.6756 - recall_5: 0.5672\n",
            "Epoch 147/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.7747 - precision_5: 0.7273 - recall_5: 0.5672\n",
            "Epoch 148/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7578 - precision_5: 0.6798 - recall_5: 0.5784\n",
            "Epoch 149/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.7500 - precision_5: 0.6810 - recall_5: 0.5336\n",
            "Epoch 150/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.7617 - precision_5: 0.7014 - recall_5: 0.5522\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.9102 - precision_5: 1.0000 - recall_5: 0.7406\n",
            "Accuracy: 91.02%\n",
            "Precision: 100.00%\n",
            "Recall: 74.06%\n"
          ]
        }
      ],
      "source": [
        "test_size = 0.5\n",
        "train_data, test_data = train_test_split(eval_augmented_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n"
      ],
      "id": "XQiPojfz22mt"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}