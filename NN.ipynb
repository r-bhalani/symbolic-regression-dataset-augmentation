{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fd33643"
      },
      "source": [
        "# Program Synthesis for Dataset Augmentation with Symbolic Regression and Genetic Programming"
      ],
      "id": "0fd33643"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "2649232b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7f00df-b875-485f-b1b2-b67000cbc125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gplearn in /usr/local/lib/python3.9/dist-packages (0.4.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gplearn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Ruchi Bhalani, rb44675\n",
        "!pip install gplearn"
      ],
      "id": "2649232b"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8ZMR0CqzTxX",
        "outputId": "f49e1fe7-97ff-4f04-a02e-a543f49fa2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "A8ZMR0CqzTxX"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ab1fe226"
      },
      "outputs": [],
      "source": [
        "# headers\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from collections import OrderedDict\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "ab1fe226"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "cc9c4481"
      },
      "outputs": [],
      "source": [
        "# function to calculate adjusted r2\n",
        "def get_adj_r2(r2, n, p):\n",
        "    return (1-(1-r2)*((n-1)/(n-p-1)))"
      ],
      "id": "cc9c4481"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7c7806"
      },
      "source": [
        "# Assignment 2: Regression and KNN classifier"
      ],
      "id": "7a7c7806"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bb47879"
      },
      "source": [
        "**Data Prep**\n"
      ],
      "id": "8bb47879"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "589c05de",
        "outputId": "e829a770-d00f-473f-8bc1-934d3269dd80",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    int64  \n",
            " 2   BloodPressure             768 non-null    int64  \n",
            " 3   SkinThickness             768 non-null    int64  \n",
            " 4   Insulin                   768 non-null    int64  \n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n",
            "None\n",
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "763           10      101             76             48      180  32.9   \n",
            "764            2      122             70             27        0  36.8   \n",
            "765            5      121             72             23      112  26.2   \n",
            "766            1      126             60              0        0  30.1   \n",
            "767            1       93             70             31        0  30.4   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "763                     0.171   63        0  \n",
            "764                     0.340   27        0  \n",
            "765                     0.245   30        0  \n",
            "766                     0.349   47        1  \n",
            "767                     0.315   23        0  \n",
            "\n",
            "[768 rows x 9 columns]\n",
            "[]\n",
            "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
            "FINAL STEP: proper nouns -- []\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8349eeb-941c-463c-9660-eea69a931868\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8349eeb-941c-463c-9660-eea69a931868')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8349eeb-941c-463c-9660-eea69a931868 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8349eeb-941c-463c-9660-eea69a931868');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# NOTE: THESE VALUES ARE MEANT TO BE CUSTOMIZABLE BY THE USER\n",
        "# read in data\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/medical-charges.txt\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/breast_cancer.csv\"\n",
        "USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/diabetes.csv\"\n",
        "# USER_INPUT_DATASET = \"/content/drive/MyDrive/373P/FinalProjectCode/disease.csv\"\n",
        "EXPECTED_DISTINCT_PERCENTAGE = 0.85\n",
        "\n",
        "# How many entries do we want to augment this dataset by?\n",
        "# TODO: Let the user pass this in as a percentage as well\n",
        "NEW_EXAMPLES = 10\n",
        "\n",
        "# data = pd.read_csv(USER_INPUT_DATASET, header = 0)\n",
        "df = pd.read_csv(USER_INPUT_DATASET, header=\"infer\")\n",
        "print(df.info())\n",
        "print(df)\n",
        "\n",
        "\n",
        "# DATA PREPROCESSING\n",
        "df = df.dropna(axis=1, how='all')\n",
        "# df = df.dropna()\n",
        "df = df.fillna(0)\n",
        "\n",
        "\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, shuffle=False)\n",
        "df = train_data\n",
        "\n",
        "# Separate them into numerical, categorical, and proper noun columns (regex generation)\n",
        "numerical_cols = list()\n",
        "categorical_cols = list()\n",
        "proper_noun_cols = list()\n",
        "for (index, colname) in enumerate(df):\n",
        "    if colname in df.select_dtypes(include='object').columns:\n",
        "      unique_vals = df[colname].unique()\n",
        "      if len(unique_vals) >= EXPECTED_DISTINCT_PERCENTAGE * df.shape[0]:\n",
        "        proper_noun_cols.append(colname)\n",
        "      else:\n",
        "        categorical_cols.append(colname)\n",
        "    else:\n",
        "      numerical_cols.append(colname)\n",
        "\n",
        "\n",
        "print(categorical_cols)\n",
        "print(numerical_cols)\n",
        "print(\"FINAL STEP: proper nouns --\", proper_noun_cols)\n",
        "\n",
        "df.head()\n"
      ],
      "id": "589c05de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3043bb91"
      },
      "source": [
        "There are several categorical columns. We need to transform these to be able to do regression. "
      ],
      "id": "3043bb91"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0484464d",
        "outputId": "9903cec2-211d-49ca-865f-14b555c3b230",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2d73865-bdef-4cda-af96-2ad8360d254b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2d73865-bdef-4cda-af96-2ad8360d254b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f2d73865-bdef-4cda-af96-2ad8360d254b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f2d73865-bdef-4cda-af96-2ad8360d254b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# we'll deal with the proper noun columns at the end\n",
        "regression_df = df.drop(columns=proper_noun_cols)\n",
        "regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "regression_df.head()"
      ],
      "id": "0484464d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38a13dc1"
      },
      "source": [
        "An interesting thing to check with regression problems is whether any of the individual features correlate very strongly with the label."
      ],
      "id": "38a13dc1"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad660fd",
        "outputId": "958ed878-f945-4c66-d498-ce8dc6d70598",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.136707       0.112177      -0.083398   \n",
            "Glucose                      0.136707  1.000000       0.135931       0.060795   \n",
            "BloodPressure                0.112177  0.135931       1.000000       0.210994   \n",
            "SkinThickness               -0.083398  0.060795       0.210994       1.000000   \n",
            "Insulin                     -0.056020  0.346124       0.101901       0.431684   \n",
            "BMI                          0.036012  0.222744       0.275194       0.381554   \n",
            "DiabetesPedigreeFunction    -0.047127  0.149888       0.022229       0.183296   \n",
            "Age                          0.535479  0.273592       0.222201      -0.120908   \n",
            "Outcome                      0.211148  0.458698       0.059317       0.076651   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.056020  0.036012                 -0.047127   \n",
            "Glucose                   0.346124  0.222744                  0.149888   \n",
            "BloodPressure             0.101901  0.275194                  0.022229   \n",
            "SkinThickness             0.431684  0.381554                  0.183296   \n",
            "Insulin                   1.000000  0.191183                  0.222238   \n",
            "BMI                       0.191183  1.000000                  0.135152   \n",
            "DiabetesPedigreeFunction  0.222238  0.135152                  1.000000   \n",
            "Age                      -0.010908  0.049873                  0.034870   \n",
            "Outcome                   0.152785  0.318709                  0.189090   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.535479  0.211148  \n",
            "Glucose                   0.273592  0.458698  \n",
            "BloodPressure             0.222201  0.059317  \n",
            "SkinThickness            -0.120908  0.076651  \n",
            "Insulin                  -0.010908  0.152785  \n",
            "BMI                       0.049873  0.318709  \n",
            "DiabetesPedigreeFunction  0.034870  0.189090  \n",
            "Age                       1.000000  0.218166  \n",
            "Outcome                   0.218166  1.000000   \n",
            "\n",
            "\n",
            "=== SIGNIFICANTLY CORRELATED COLUMNS ===\n",
            "[]\n",
            "OrderedDict()\n"
          ]
        }
      ],
      "source": [
        "print(regression_df.corr(), \"\\n\\n\")\n",
        "\n",
        "# Indexing with numbers on a numpy matrix will probably be faster\n",
        "\n",
        "print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "rows, cols = regression_df.shape\n",
        "corr = regression_df.corr().values\n",
        "fields = list(regression_df.columns)\n",
        "correlated_columns = list()\n",
        "nodes = []\n",
        "correlations = OrderedDict()\n",
        "\n",
        "# data structure that organizes correlations\n",
        "correlations_per_column = list()\n",
        "for i in range(cols):\n",
        "    for j in range(i+1, cols):\n",
        "      corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "      if corr[i,j] > 0.6:\n",
        "          print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "          correlated_columns.append([fields[i], fields[j], corr[i, j]])\n",
        "          correlations[corr[i, j]] = [fields[i], fields[j]]\n",
        "          if fields[i] not in nodes:\n",
        "            nodes.append(fields[i])\n",
        "          if fields[j] not in nodes:\n",
        "            nodes.append(fields[j])\n",
        "\n",
        "print(nodes)\n",
        "print(correlations)"
      ],
      "id": "6ad660fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l74DGBikGfow"
      },
      "source": [
        "We can visualize these attributes and their correlations as a weighted graph."
      ],
      "id": "l74DGBikGfow"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ggXpUuAfWqG2",
        "outputId": "e417bc6a-401d-4417-9cda-df4eaccbe127"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "for key,value in correlations.items():\n",
        "  G.add_edge(value[0], value[1], weight=key)\n",
        "\n",
        "\n",
        "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0.5]\n",
        "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.5]\n",
        "\n",
        "pos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n",
        "\n",
        "# nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=700)\n",
        "\n",
        "# edges\n",
        "nx.draw_networkx_edges(G, pos, edgelist=elarge, width=6)\n",
        "nx.draw_networkx_edges(\n",
        "    G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=\"b\", style=\"dashed\"\n",
        ")\n",
        "\n",
        "# node labels\n",
        "nx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")\n",
        "# edge weight labels\n",
        "edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.margins(0.15)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ggXpUuAfWqG2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBaK36IxauAO"
      },
      "source": [
        "Now, let's organize them by pair and start generating values."
      ],
      "id": "lBaK36IxauAO"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkLcoIVuar-l",
        "outputId": "e26b0a16-d4f3-4d8c-f29c-ae5baea9f148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict()\n"
          ]
        }
      ],
      "source": [
        "# weight all the correlations so that they're iterated through correctly\n",
        "weighted_correlations = OrderedDict()\n",
        "for key,value in correlations.items():\n",
        "  mult_factor = 1\n",
        "  if value[0] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  if value[1] in numerical_cols:\n",
        "    mult_factor *= 10\n",
        "  weighted_correlations[mult_factor * key] = value\n",
        "\n",
        "weighted_correlations = OrderedDict(sorted(weighted_correlations.items(), reverse=True))\n",
        "print(weighted_correlations)\n"
      ],
      "id": "FkLcoIVuar-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numerical Data Generation: Symbolic Regression\n",
        "Now, let's augment the dataset with numerical data.\n",
        "\n",
        "The code is implementing a technique for data augmentation using symbolic regression, kernel density estimation, and K-nearest neighbor regression. The goal is to generate new samples of data that are similar to the original dataset, but not identical, to increase the size and diversity of the dataset for training machine learning models. The code takes in a pandas DataFrame df and the number of new samples to generate n_samples as inputs.\n",
        "\n",
        "The code first scales each column of the input dataset to the range [0, 1], fits a symbolic regressor to the scaled data, and generates new values using the regressor. It then applies kernel density estimation and K-nearest neighbor regression to the original data and generates new values using these methods. Finally, it performs a grid search to find the best kernel and bandwidth for the kernel density estimator, and generates new values using the best estimator.\n",
        "\n",
        "The new values generated by each method are clipped to the range of the original column data, and the new values for all columns are combined to create a new DataFrame new_df. The new_df is then concatenated with the original DataFrame df to create an augmented dataset, which is returned as output.\n",
        "\n"
      ],
      "metadata": {
        "id": "dJCaBCe3meJ0"
      },
      "id": "dJCaBCe3meJ0"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YqEGUD4oDrP",
        "outputId": "9c0942c9-5f80-4991-c5e6-de7e5e8f6ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}\n",
            "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}\n",
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0    48.81          9826.21        7       0.00305441       0.00266057     14.53m\n",
            "(614, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-180.35595604          -inf          -inf -108.61787859          -inf\n",
            " -227.97353849          -inf          -inf -189.1314716           -inf\n",
            " -269.90028508          -inf          -inf -231.02058119          -inf\n",
            " -293.6724088           -inf          -inf -254.86876972          -inf\n",
            " -301.4664062           -inf          -inf -269.44045919          -inf\n",
            " -303.23245023          -inf          -inf -279.02028535          -inf\n",
            " -303.93142753          -inf          -inf -285.76440289          -inf\n",
            " -304.64977229          -inf          -inf -290.80736463          -inf\n",
            " -305.49135824          -inf          -inf -294.77785479          -inf\n",
            " -306.42761029          -inf          -inf -298.04242473          -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          333.511        7       0.00821244       0.00810278     16.74m\n",
            "(614, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-2647.61834852           -inf           -inf  -483.70565037\n",
            "           -inf -1060.06317259           -inf           -inf\n",
            "  -519.93196513           -inf  -799.06152604           -inf\n",
            "           -inf  -546.08742387           -inf  -715.66553064\n",
            "           -inf           -inf  -561.2945519            -inf\n",
            "  -673.08457948           -inf           -inf  -570.30921521\n",
            "           -inf  -647.65919076           -inf           -inf\n",
            "  -575.96358349           -inf  -632.09245559           -inf\n",
            "           -inf  -579.71063664           -inf  -622.17304102\n",
            "           -inf           -inf  -582.31127863           -inf\n",
            "  -615.53672254           -inf           -inf  -584.18581624\n",
            "           -inf  -610.9018018            -inf           -inf\n",
            "  -585.57955312           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          277.954        7       0.00764004       0.00696911      7.63m\n",
            "(614, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-2303.76312771           -inf           -inf  -280.01522692\n",
            "           -inf  -843.86319959           -inf           -inf\n",
            "  -332.76104986           -inf  -607.33197298           -inf\n",
            "           -inf  -370.21825506           -inf  -540.66583616\n",
            "           -inf           -inf  -397.09812427           -inf\n",
            "  -518.5995448            -inf           -inf  -416.80222424\n",
            "           -inf  -512.16244827           -inf           -inf\n",
            "  -431.42230282           -inf  -510.67081891           -inf\n",
            "           -inf  -442.41081909           -inf  -509.62838139\n",
            "           -inf           -inf  -450.79521378           -inf\n",
            "  -507.88583732           -inf           -inf  -457.29678737\n",
            "           -inf  -505.69725762           -inf           -inf\n",
            "  -462.42085119           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          1042.86        7       0.00279196       0.00300038      7.84m\n",
            "(614, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-13460.4108301             -inf            -inf   -300.67500889\n",
            "            -inf  -3623.02859879            -inf            -inf\n",
            "   -335.52288917            -inf  -1834.59298649            -inf\n",
            "            -inf   -363.55527246            -inf  -1219.68031092\n",
            "            -inf            -inf   -382.1706831             -inf\n",
            "   -935.37456107            -inf            -inf   -394.97508097\n",
            "            -inf   -781.1968293             -inf            -inf\n",
            "   -404.33678683            -inf   -689.4259262             -inf\n",
            "            -inf   -411.56805113            -inf   -631.03985122\n",
            "            -inf            -inf   -417.40367907            -inf\n",
            "   -591.9536139             -inf            -inf   -422.27458377\n",
            "            -inf   -564.74988301            -inf            -inf\n",
            "   -426.44701786            -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          7063.92        7       0.00122896       0.00173112      6.58m\n",
            "(614, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-207929.55314254             -inf             -inf   -1233.77851106\n",
            "             -inf  -52239.6268235              -inf             -inf\n",
            "    -802.48160281             -inf  -23442.02646936             -inf\n",
            "             -inf    -678.49335707             -inf  -13379.15774491\n",
            "             -inf             -inf    -624.9838077              -inf\n",
            "   -8729.93713228             -inf             -inf    -597.48954562\n",
            "             -inf   -6209.97388456             -inf             -inf\n",
            "    -582.05508792             -inf   -4694.49959238             -inf\n",
            "             -inf    -573.01108482             -inf   -3713.78066839\n",
            "             -inf             -inf    -567.66000839             -inf\n",
            "   -3043.56468254             -inf             -inf    -564.57384125\n",
            "             -inf   -2565.86258079             -inf             -inf\n",
            "    -562.93671553             -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          506.837        7       0.00642331       0.00626685      7.71m\n",
            "(614, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-1236.09605549           -inf           -inf  -428.73817528\n",
            "           -inf  -617.76687648           -inf           -inf\n",
            "  -421.39371689           -inf  -504.02985506           -inf\n",
            "           -inf  -419.04622095           -inf  -464.5628859\n",
            "           -inf           -inf  -417.87273155           -inf\n",
            "  -446.33115286           -inf           -inf  -417.20434255\n",
            "           -inf  -436.45810347           -inf           -inf\n",
            "  -416.80651112           -inf  -430.58199609           -inf\n",
            "           -inf  -416.57035812           -inf  -426.85967412\n",
            "           -inf           -inf  -416.43894068           -inf\n",
            "  -424.38752618           -inf           -inf  -416.38012377\n",
            "           -inf  -422.68102545           -inf           -inf\n",
            "  -416.37457045           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          34675.3        7       0.00229195       0.00257905      8.11m\n",
            "(614, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ -11.08314823          -inf          -inf  -13.72547081          -inf\n",
            "  -24.20178122  -14.45396633  -10.32473838  -28.05584229   -9.33173306\n",
            "  -37.77610728  -24.7746607   -16.87051448  -42.63959123  -14.47302441\n",
            "  -51.71575092  -34.20757878  -23.83687602  -56.69870993  -20.29764696\n",
            "  -65.55576484  -42.71552485  -30.30590418  -69.92931143  -25.86982155\n",
            "  -78.88210236  -52.37461387  -36.7723061   -82.22921934  -31.40496083\n",
            "  -91.52066064  -62.98715259  -43.63763342  -93.61766679  -37.18208867\n",
            " -103.41957586  -73.92114352  -50.79623848 -104.16646936  -43.15887898\n",
            " -114.57872532  -84.44905323  -58.1604916  -113.9615327   -49.29320322\n",
            " -125.02867282  -94.35302923  -65.46743528 -123.08651426  -55.40931364]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          26426.5        7       0.00278024       0.00238833      6.33m\n",
            "(614, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [-1847.10415409           -inf           -inf  -275.36485585\n",
            "           -inf  -747.22163409           -inf           -inf\n",
            "  -340.70138786           -inf  -576.54381382           -inf\n",
            "           -inf  -376.8835413            -inf  -525.0179887\n",
            "           -inf           -inf  -397.24863826           -inf\n",
            "  -496.99930253           -inf           -inf  -409.33905072\n",
            "           -inf  -479.02100912           -inf           -inf\n",
            "  -417.01719922           -inf  -467.71230095           -inf\n",
            "           -inf  -422.21939774           -inf  -460.51990967\n",
            "           -inf           -inf  -425.95171967           -inf\n",
            "  -455.80508135           -inf           -inf  -428.76495184\n",
            "           -inf  -452.63130015           -inf           -inf\n",
            "  -430.97698217           -inf]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:961: RuntimeWarning: invalid value encountered in subtract\n",
            "  (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    |   Population Average    |             Best Individual              |\n",
            "---- ------------------------- ------------------------------------------ ----------\n",
            " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
            "   0    48.81          59.9142        7                0                0      7.68m\n",
            "(614, 1)\n",
            "DATA SHAPE HERE:  None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KernelDensity, KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from gplearn.genetic import SymbolicRegressor\n",
        "\n",
        "def augment_dataset(df, n_samples):\n",
        "    # Get column names and data types\n",
        "    col_names = df.columns.tolist()\n",
        "    dtypes = df.dtypes.tolist()\n",
        "\n",
        "    # Create a list to store new data samples\n",
        "    new_data = []\n",
        "\n",
        "    # Loop through each column and generate new values\n",
        "    for col_name, dtype in zip(col_names, dtypes):\n",
        "        # Get the data from the column\n",
        "        col_data = df[col_name].values\n",
        "\n",
        "        # Scale the column data to the range [0, 1]\n",
        "        scaler = MinMaxScaler()\n",
        "        col_data_scaled = scaler.fit_transform(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Fit a symbolic regressor to the column data\n",
        "        est_gp = SymbolicRegressor(population_size=5000, tournament_size=50,\n",
        "                                    generations=50, stopping_criteria=0.01,\n",
        "                                    p_crossover=0.7, p_subtree_mutation=0.1,\n",
        "                                    p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
        "                                    max_samples=0.9, verbose=1,\n",
        "                                    parsimony_coefficient=0.001, random_state=0, warm_start=True)\n",
        "        est_gp.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the symbolic regressor\n",
        "        new_col_data = est_gp.predict(np.random.rand(n_samples, 1))\n",
        "\n",
        "        # Reshape the new column data to be a 2D array\n",
        "        new_col_data = new_col_data.reshape(-1, 1)\n",
        "\n",
        "        # Scale the new column data using the scaler fitted to the original column data\n",
        "        new_col_data_scaled = scaler.transform(new_col_data)\n",
        "\n",
        "        # Inverse transform the scaled new column data to get the original scale\n",
        "        new_col_data = scaler.inverse_transform(new_col_data_scaled)\n",
        "\n",
        "        # Fit a kernel density estimator to the original column data\n",
        "        kde = KernelDensity(kernel='gaussian', bandwidth=0.1)\n",
        "        kde.fit(col_data.reshape(-1, 1))\n",
        "\n",
        "        # Generate new values using the kernel density estimator\n",
        "        kde_samples = kde.sample(n_samples)\n",
        "        kde_samples = np.squeeze(kde_samples)\n",
        "\n",
        "        # Fit a KNN regressor to the original column data\n",
        "        knn = KNeighborsRegressor(n_neighbors=5)\n",
        "        knn.fit(col_data_scaled, col_data_scaled)\n",
        "\n",
        "        # Generate new values using the KNN regressor\n",
        "        knn_samples = knn.predict(np.random.rand(n_samples, 1))\n",
        "        knn_samples = np.squeeze(knn_samples)\n",
        "\n",
        "        # Use a grid search to find the best kernel and bandwidth for the kernel density estimator\n",
        "        params = {'kernel': ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear'],\n",
        "                  'bandwidth': np.linspace(0.1, 1.0, 10)}\n",
        "        grid = GridSearchCV(KernelDensity(), params, cv=5)\n",
        "        print(\"DATA SHAPE HERE: \", print(col_data.reshape(-1, 1).shape))\n",
        "        grid.fit(col_data.reshape(-1, 1))\n",
        "        # grid.fit(col_data)\n",
        "        kde_best = grid.best_estimator_\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        if kde_best.kernel in [\"gaussian\", \"tophat\"]:\n",
        "            kde_best_samples = kde_best.sample(n_samples)\n",
        "        else:\n",
        "            # Evaluate log density for a grid of points\n",
        "            x_grid = np.linspace(col_data.min(), col_data.max(), 1000).reshape(-1, 1)\n",
        "            log_dens = kde_best.score_samples(x_grid)\n",
        "\n",
        "            # Sample from the grid according to the log densities\n",
        "            probs = np.exp(log_dens - log_dens.max())\n",
        "            probs /= probs.sum()\n",
        "            kde_best_samples = np.random.choice(x_grid.flatten(), size=n_samples, p=probs)\n",
        "            \n",
        "        kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Generate new values using the best kernel density estimator\n",
        "        # kde_best_samples = kde_best.sample(n_samples)\n",
        "        # kde_best_samples = np.squeeze(kde_best_samples)\n",
        "\n",
        "        # Clip the new values to the range of the original column data\n",
        "        kde_best_samples = np.clip(kde_best_samples, np.min(col_data), np.max(col_data))\n",
        "\n",
        "        # Add new values to the list of new data\n",
        "        new_data.append(kde_best_samples.flatten())\n",
        "\n",
        "    # Transpose the list of new data to match the shape of the original dataset\n",
        "    new_data = np.array(new_data).T\n",
        "\n",
        "    # Convert the new data to a dataframe and append it to the original dataset\n",
        "    new_df = pd.DataFrame(new_data, columns=col_names)\n",
        "    augmented_df = pd.concat([df, new_df], ignore_index=True)\n",
        "\n",
        "    # return augmented_df\n",
        "    return new_df\n",
        "\n",
        "print(np.seterr())\n",
        "np.seterr(invalid='warn')\n",
        "np.seterr(under='warn')\n",
        "print(np.seterr())\n",
        "new_df = augment_dataset(regression_df, df.shape[0])"
      ],
      "id": "4YqEGUD4oDrP"
    },
    {
      "cell_type": "code",
      "source": [
        "# def reverse_one_hot_encode(df):\n",
        "#     # Get a list of all columns with '_'\n",
        "#     oh_cols = [col for col in df.columns if '_' in col]\n",
        "\n",
        "#     # Get the original column names and values for each one-hot encoded column\n",
        "#     col_vals = {}\n",
        "#     for col in oh_cols:\n",
        "#         col_name = col.split('_')[0]\n",
        "#         col_val = col.split('_')[1]\n",
        "#         if col_name not in col_vals:\n",
        "#             col_vals[col_name] = {}\n",
        "#         col_vals[col_name][col_val] = col\n",
        "\n",
        "#     # Create a new dataframe to store the reverse one-hot encoded values\n",
        "#     new_df = pd.DataFrame(columns=col_vals.keys())\n",
        "\n",
        "#     # Iterate through each row of the original dataframe\n",
        "#     for index, row in df.iterrows():\n",
        "#         # Initialize a dictionary to store the values for this row\n",
        "#         new_row = {}\n",
        "\n",
        "#         # Iterate through each original column\n",
        "#         for col_name in df.columns:\n",
        "#             # If this is a one-hot encoded column, find the closest value to 1 and use that\n",
        "#             if col_name in oh_cols:\n",
        "#                 col_val = col_name.split('_')[1]\n",
        "#                 col_key = col_vals[col_name.split('_')[0]][col_val]\n",
        "                \n",
        "#                 # Get the numeric values for the one-hot encoded column\n",
        "#                 numeric_values = pd.to_numeric(row[col_key], errors='coerce')\n",
        "#                 # Set values < 0.5 to 0 and values >= 0.5 to the numeric value\n",
        "#                 numeric_values = np.where(numeric_values >= 0.5, numeric_values, 0)\n",
        "#                 # Set the value closest to 1 to 1, and the rest to 0\n",
        "#                 numeric_values = np.where(numeric_values == np.max(numeric_values), 1, 0)\n",
        "#                 # Combine the column name and the value (if it's 1) and add it to the new_row dictionary\n",
        "#                 new_row[col_name.split('_')[0]] = col_val if np.sum(numeric_values) > 0 else np.nan\n",
        "#             # If this is not a one-hot encoded column, use the original value\n",
        "#             else:\n",
        "#                 new_row[col_name] = row[col_name]\n",
        "\n",
        "#         # Append the values for this row to the new dataframe\n",
        "#         new_df = new_df.append(new_row, ignore_index=True)\n",
        "\n",
        "#     # Return the new dataframe\n",
        "#     return new_df"
      ],
      "metadata": {
        "id": "er5lGyrRnbUt"
      },
      "id": "er5lGyrRnbUt",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "x0qeIOSFMs8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1a38a2-7ff3-4684-8cf7-bdb64c4d1b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Old:  614\n",
            "New:  614\n",
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  0.000000 -0.810298      -0.674421      -0.858128   \n",
            "Glucose                     -0.810298  0.000000      -0.752904      -0.855096   \n",
            "BloodPressure               -0.674421 -0.752904       0.000000      -0.593067   \n",
            "SkinThickness               -0.858128 -0.855096      -0.593067       0.000000   \n",
            "Insulin                     -0.895706 -0.512708      -0.577891      -0.418462   \n",
            "BMI                         -0.899858 -0.760644      -0.624358      -0.547200   \n",
            "DiabetesPedigreeFunction    -0.926052 -0.759727      -0.740592      -0.716671   \n",
            "Age                         -0.457131 -0.660524      -0.530179      -0.791314   \n",
            "Outcome                     -0.657076 -0.351559      -0.560706      -0.698379   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.895706 -0.899858                 -0.926052   \n",
            "Glucose                  -0.512708 -0.760644                 -0.759727   \n",
            "BloodPressure            -0.577891 -0.624358                 -0.740592   \n",
            "SkinThickness            -0.418462 -0.547200                 -0.716671   \n",
            "Insulin                   0.000000 -0.673589                 -0.765396   \n",
            "BMI                      -0.673589  0.000000                 -0.783171   \n",
            "DiabetesPedigreeFunction -0.765396 -0.783171                  0.000000   \n",
            "Age                      -0.950938 -0.866297                 -0.940122   \n",
            "Outcome                  -0.635047 -0.447762                 -0.613357   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies              -0.457131 -0.657076  \n",
            "Glucose                  -0.660524 -0.351559  \n",
            "BloodPressure            -0.530179 -0.560706  \n",
            "SkinThickness            -0.791314 -0.698379  \n",
            "Insulin                  -0.950938 -0.635047  \n",
            "BMI                      -0.866297 -0.447762  \n",
            "DiabetesPedigreeFunction -0.940122 -0.613357  \n",
            "Age                       0.000000 -0.651776  \n",
            "Outcome                  -0.651776  0.000000  \n",
            "     Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
            "0       6.143143  142.108108      79.989990      33.000000  141.423423   \n",
            "1       3.981982  121.891892      72.418418      26.954955   75.369369   \n",
            "2       0.000000   56.882883       0.000000       0.000000    0.000000   \n",
            "3       4.220220  125.063063      74.006006      28.936937   88.918919   \n",
            "4       5.054054  131.207207      76.082082      30.918919  110.090090   \n",
            "..           ...         ...            ...            ...         ...   \n",
            "609     1.939940  103.063063      65.091091      14.963964    0.846847   \n",
            "610     9.019019  173.027027      88.050050      41.027027  249.819820   \n",
            "611     1.956957  103.063063      65.823824      15.063063    0.846847   \n",
            "612     1.905906  102.072072      64.358358      14.864865    0.846847   \n",
            "613     9.070070  174.018018      88.172172      41.819820  269.297297   \n",
            "\n",
            "           BMI  DiabetesPedigreeFunction        Age   Outcome  \n",
            "0    36.807608                  0.664086  40.939940  0.955956  \n",
            "1    32.979079                  0.457784  31.210210  0.062062  \n",
            "2    17.866466                  0.094410  21.000000  0.000000  \n",
            "3    33.785085                  0.502326  33.312312  0.074074  \n",
            "4    34.926927                  0.567968  36.915916  0.923924  \n",
            "..         ...                       ...        ...       ...  \n",
            "609  28.411712                  0.291335  25.084084  0.028028  \n",
            "610  42.382482                  0.943063  53.732733  0.986987  \n",
            "611  28.546046                  0.293680  25.144144  0.028028  \n",
            "612  28.210210                  0.286647  25.024024  0.027027  \n",
            "613  42.785485                  0.968851  54.153153  0.987988  \n",
            "\n",
            "[614 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# reprocess the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# just brings columns back to normal and replaces values with NaN\n",
        "def reverse_one_hot_encode(df):\n",
        "    oh_cols = [col for col in df.columns if \"_\" in col and col.split(\"_\")[0] in categorical_cols]\n",
        "    print(oh_cols)\n",
        "    new_df = df.drop(columns=oh_cols).copy()\n",
        "    for col in oh_cols:\n",
        "        col_name, val = col.split(\"_\")\n",
        "        mask = df[col] == 1\n",
        "        new_df[col_name] = np.nan\n",
        "        new_df.loc[mask, col_name] = val\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_significantly_correlated_colmns(df):\n",
        "  print(\"=== SIGNIFICANTLY CORRELATED COLUMNS ===\")\n",
        "  rows, cols = df.shape\n",
        "  corr = df.corr().values\n",
        "  fields = list(df.columns)\n",
        "  for i in range(cols):\n",
        "      for j in range(i+1, cols-1):\n",
        "        corr[i,j] = round(abs(corr[i,j]), 5)\n",
        "        if corr[i,j] > 0.75:\n",
        "            print(fields[i], ' ', fields[j], ' ', corr[i,j])\n",
        "\n",
        "new_df = reverse_one_hot_encode(new_df)\n",
        "corr1 = regression_df.corr()\n",
        "corr2 = new_df.corr()\n",
        "# print a matrix to see the difference in correlations\n",
        "diff = np.abs(corr1) - np.abs(corr2)\n",
        "\n",
        "print(\"Old: \", len(regression_df))\n",
        "print(\"New: \", len(new_df))\n",
        "print(diff)\n",
        "\n",
        "# print(print_significantly_correlated_colmns(regression_df))\n",
        "# print(print_significantly_correlated_colmns(new_df))\n",
        "new_df.head()\n",
        "print(new_df)"
      ],
      "id": "x0qeIOSFMs8x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIh2Sn5ohWnz"
      },
      "source": [
        "## Categorical Variable Generation: Hill Climbing Algorithm\n",
        "In this implementation, we use a hill-climbing algorithm to search for the best categorical variable distribution that matches the original distribution and is correlated with the numerical variables in the generated data. The algorithm starts with the original distribution and iteratively perturbs it by swapping two values, then computes the correlation with the numerical variables and updates the distribution if the correlation improves. The algorithm repeats this process for a fixed number of iterations (in this case, 10), then moves on to the next categorical variable.\n",
        "\n",
        "This is an example of an explicit search or optimization technique for program synthesis, as we are searching for the best program (i.e., categorical variable distribution) that satisfies certain constraints (i.e., matching the original distribution and being correlated with the numerical variables)."
      ],
      "id": "cIh2Sn5ohWnz"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Hp-B6m8qSRXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ea88a63-ee1a-4de9-a9f2-a956560467fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.136707       0.112177      -0.083398   \n",
            "Glucose                      0.136707  1.000000       0.135931       0.060795   \n",
            "BloodPressure                0.112177  0.135931       1.000000       0.210994   \n",
            "SkinThickness               -0.083398  0.060795       0.210994       1.000000   \n",
            "Insulin                     -0.056020  0.346124       0.101901       0.431684   \n",
            "BMI                          0.036012  0.222744       0.275194       0.381554   \n",
            "DiabetesPedigreeFunction    -0.047127  0.149888       0.022229       0.183296   \n",
            "Age                          0.535479  0.273592       0.222201      -0.120908   \n",
            "Outcome                      0.211148  0.458698       0.059317       0.076651   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies              -0.056020  0.036012                 -0.047127   \n",
            "Glucose                   0.346124  0.222744                  0.149888   \n",
            "BloodPressure             0.101901  0.275194                  0.022229   \n",
            "SkinThickness             0.431684  0.381554                  0.183296   \n",
            "Insulin                   1.000000  0.191183                  0.222238   \n",
            "BMI                       0.191183  1.000000                  0.135152   \n",
            "DiabetesPedigreeFunction  0.222238  0.135152                  1.000000   \n",
            "Age                      -0.010908  0.049873                  0.034870   \n",
            "Outcome                   0.152785  0.318709                  0.189090   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.535479  0.211148  \n",
            "Glucose                   0.273592  0.458698  \n",
            "BloodPressure             0.222201  0.059317  \n",
            "SkinThickness            -0.120908  0.076651  \n",
            "Insulin                  -0.010908  0.152785  \n",
            "BMI                       0.049873  0.318709  \n",
            "DiabetesPedigreeFunction  0.034870  0.189090  \n",
            "Age                       1.000000  0.218166  \n",
            "Outcome                   0.218166  1.000000  \n",
            "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
            "Pregnancies                  1.000000  0.396089       0.317243       0.235702   \n",
            "Glucose                      0.396089  1.000000       0.373916       0.337015   \n",
            "BloodPressure                0.317243  0.373916       1.000000       0.392931   \n",
            "SkinThickness                0.235702  0.337015       0.392931       1.000000   \n",
            "Insulin                      0.263085  0.514147       0.282211       0.565440   \n",
            "BMI                          0.313339  0.466988       0.465037       0.551533   \n",
            "DiabetesPedigreeFunction     0.270202  0.395935       0.249451       0.408127   \n",
            "Age                          0.677332  0.487471       0.384719       0.203058   \n",
            "Outcome                      0.408707  0.568769       0.225842       0.288539   \n",
            "\n",
            "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
            "Pregnancies               0.263085  0.313339                  0.270202   \n",
            "Glucose                   0.514147  0.466988                  0.395935   \n",
            "BloodPressure             0.282211  0.465037                  0.249451   \n",
            "SkinThickness             0.565440  0.551533                  0.408127   \n",
            "Insulin                   1.000000  0.404074                  0.466425   \n",
            "BMI                       0.404074  1.000000                  0.378962   \n",
            "DiabetesPedigreeFunction  0.466425  0.378962                  1.000000   \n",
            "Age                       0.299278  0.319482                  0.329969   \n",
            "Outcome                   0.348601  0.453750                  0.375609   \n",
            "\n",
            "                               Age   Outcome  \n",
            "Pregnancies               0.677332  0.408707  \n",
            "Glucose                   0.487471  0.568769  \n",
            "BloodPressure             0.384719  0.225842  \n",
            "SkinThickness             0.203058  0.288539  \n",
            "Insulin                   0.299278  0.348601  \n",
            "BMI                       0.319482  0.453750  \n",
            "DiabetesPedigreeFunction  0.329969  0.375609  \n",
            "Age                       1.000000  0.416349  \n",
            "Outcome                   0.416349  1.000000  \n",
            "1.0     230\n",
            "2.0     161\n",
            "0.0     146\n",
            "3.0     136\n",
            "4.0     113\n",
            "5.0      99\n",
            "7.0      82\n",
            "6.0      73\n",
            "8.0      63\n",
            "9.0      50\n",
            "10.0     31\n",
            "12.0     14\n",
            "13.0     12\n",
            "11.0     10\n",
            "15.0      3\n",
            "14.0      3\n",
            "17.0      2\n",
            "Name: Pregnancies, dtype: int64\n",
            "100.0    29\n",
            "105.0    27\n",
            "124.0    26\n",
            "109.0    26\n",
            "129.0    25\n",
            "         ..\n",
            "68.0      1\n",
            "172.0     1\n",
            "72.0      1\n",
            "182.0     1\n",
            "191.0     1\n",
            "Name: Glucose, Length: 130, dtype: int64\n",
            "70.0     92\n",
            "68.0     88\n",
            "64.0     81\n",
            "74.0     79\n",
            "72.0     72\n",
            "80.0     65\n",
            "62.0     60\n",
            "76.0     57\n",
            "66.0     57\n",
            "78.0     56\n",
            "0.0      53\n",
            "60.0     53\n",
            "82.0     45\n",
            "84.0     42\n",
            "90.0     42\n",
            "88.0     37\n",
            "56.0     27\n",
            "58.0     26\n",
            "86.0     25\n",
            "50.0     21\n",
            "75.0     17\n",
            "52.0     16\n",
            "54.0     15\n",
            "48.0     12\n",
            "65.0     10\n",
            "92.0     10\n",
            "85.0      9\n",
            "110.0     6\n",
            "98.0      5\n",
            "104.0     5\n",
            "44.0      5\n",
            "96.0      5\n",
            "55.0      4\n",
            "30.0      4\n",
            "94.0      3\n",
            "108.0     3\n",
            "100.0     3\n",
            "122.0     2\n",
            "40.0      2\n",
            "95.0      2\n",
            "24.0      2\n",
            "38.0      2\n",
            "1.0       1\n",
            "53.0      1\n",
            "71.0      1\n",
            "102.0     1\n",
            "77.0      1\n",
            "61.0      1\n",
            "46.0      1\n",
            "73.0      1\n",
            "Name: BloodPressure, dtype: int64\n",
            "0.0     330\n",
            "30.0     52\n",
            "32.0     47\n",
            "28.0     38\n",
            "23.0     37\n",
            "18.0     36\n",
            "33.0     35\n",
            "22.0     34\n",
            "19.0     32\n",
            "25.0     31\n",
            "37.0     30\n",
            "31.0     29\n",
            "41.0     29\n",
            "29.0     26\n",
            "27.0     26\n",
            "42.0     25\n",
            "35.0     25\n",
            "40.0     25\n",
            "15.0     24\n",
            "39.0     22\n",
            "13.0     20\n",
            "24.0     20\n",
            "20.0     20\n",
            "26.0     19\n",
            "17.0     19\n",
            "36.0     18\n",
            "34.0     17\n",
            "21.0     16\n",
            "14.0     16\n",
            "12.0     16\n",
            "38.0     14\n",
            "16.0     13\n",
            "43.0     11\n",
            "10.0      8\n",
            "11.0      7\n",
            "46.0      7\n",
            "45.0      6\n",
            "47.0      6\n",
            "52.0      5\n",
            "44.0      5\n",
            "8.0       5\n",
            "50.0      4\n",
            "49.0      4\n",
            "7.0       4\n",
            "63.0      3\n",
            "54.0      3\n",
            "48.0      3\n",
            "60.0      2\n",
            "99.0      2\n",
            "56.0      1\n",
            "51.0      1\n",
            "Name: SkinThickness, dtype: int64\n",
            "0.0      443\n",
            "1.0       63\n",
            "2.0       29\n",
            "3.0       18\n",
            "64.0      13\n",
            "        ... \n",
            "328.0      1\n",
            "310.0      1\n",
            "579.0      1\n",
            "275.0      1\n",
            "269.0      1\n",
            "Name: Insulin, Length: 245, dtype: int64\n",
            "22.0    113\n",
            "21.0     85\n",
            "25.0     83\n",
            "24.0     82\n",
            "26.0     61\n",
            "28.0     57\n",
            "23.0     57\n",
            "29.0     55\n",
            "27.0     44\n",
            "30.0     41\n",
            "41.0     36\n",
            "31.0     35\n",
            "36.0     34\n",
            "37.0     33\n",
            "33.0     33\n",
            "42.0     29\n",
            "32.0     26\n",
            "38.0     22\n",
            "46.0     21\n",
            "35.0     20\n",
            "40.0     20\n",
            "45.0     19\n",
            "39.0     19\n",
            "44.0     17\n",
            "43.0     15\n",
            "34.0     15\n",
            "58.0     14\n",
            "54.0     13\n",
            "51.0     13\n",
            "52.0     12\n",
            "55.0     11\n",
            "50.0      8\n",
            "57.0      8\n",
            "62.0      7\n",
            "48.0      6\n",
            "56.0      6\n",
            "60.0      6\n",
            "66.0      6\n",
            "53.0      6\n",
            "63.0      5\n",
            "49.0      5\n",
            "47.0      5\n",
            "65.0      4\n",
            "61.0      4\n",
            "59.0      4\n",
            "67.0      4\n",
            "72.0      3\n",
            "69.0      2\n",
            "81.0      2\n",
            "64.0      2\n",
            "Name: Age, dtype: int64\n",
            "0.0    821\n",
            "1.0    407\n",
            "Name: Outcome, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0          6.0    148.0           72.0           35.0      0.0  33.6   \n",
              "1          1.0     85.0           66.0           29.0      0.0  26.6   \n",
              "2          8.0    183.0           64.0            0.0      0.0  23.3   \n",
              "3          1.0     89.0           66.0           23.0     94.0  28.1   \n",
              "4          0.0    137.0           40.0           35.0    168.0  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction   Age  Outcome  \n",
              "0                     0.627  50.0      1.0  \n",
              "1                     0.351  31.0      0.0  \n",
              "2                     0.672  32.0      1.0  \n",
              "3                     0.167  21.0      0.0  \n",
              "4                     2.288  33.0      1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4054d112-e855-4f0a-848f-339661c4a789\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4054d112-e855-4f0a-848f-339661c4a789')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4054d112-e855-4f0a-848f-339661c4a789 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4054d112-e855-4f0a-848f-339661c4a789');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "import random\n",
        "import copy\n",
        "\n",
        "def generate_categorical_variables(original_data, generated_data):\n",
        "    # Extract the original categorical variables\n",
        "    original_categorical_data = original_data.select_dtypes(include=['object'])\n",
        "    \n",
        "    # Extract the generated numerical variables\n",
        "    generated_numerical_data = generated_data.select_dtypes(include=['int64', 'float64'])\n",
        "    \n",
        "    # Generate new categorical variables for the generated data using hill-climbing algorithm\n",
        "    new_categorical_data = copy.deepcopy(original_categorical_data)\n",
        "    for col in new_categorical_data.columns:\n",
        "        current_distribution = original_categorical_data[col].value_counts(normalize=True)\n",
        "        for _ in range(10):\n",
        "            # Perturb the distribution by swapping two values\n",
        "            i, j = random.sample(range(len(current_distribution)), 2)\n",
        "            new_distribution = current_distribution.copy()\n",
        "            new_distribution[i], new_distribution[j] = new_distribution[j], new_distribution[i]\n",
        "            new_distribution /= new_distribution.sum()\n",
        "\n",
        "            # Compute the correlation with the numerical variables\n",
        "            new_correlation = abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))\n",
        "            current_correlation=abs(generated_numerical_data.corrwith(new_categorical_data[col].replace(current_distribution), axis=0))*-1\n",
        "            # Update the categorical variable if the correlation improves\n",
        "            if new_correlation.sum() > current_correlation.sum():\n",
        "                current_correlation = new_correlation\n",
        "                new_categorical_data[col] = np.random.choice(new_distribution.index, size=len(new_categorical_data), p=new_distribution.values)\n",
        "    \n",
        "    # Replace the NaNs in the generated data with the new categorical data\n",
        "    generated_data.update(new_categorical_data)\n",
        "    \n",
        "    return generated_data\n",
        "\n",
        "\n",
        "final_df = generate_categorical_variables(df, new_df)\n",
        "# final_df.head()\n",
        "\n",
        "final_df = pd.concat([df, final_df], axis=0)\n",
        "final_df_normalized = pd.get_dummies(final_df, columns=categorical_cols)\n",
        "complete_encoded = pd.concat([final_df_normalized, regression_df], axis=0)\n",
        "print(regression_df.corr())\n",
        "print(complete_encoded.corr())\n",
        "\n",
        "# make sure that all columns are rounded appropriately\n",
        "for col in final_df.columns:\n",
        "  if col in numerical_cols:\n",
        "    # print(col)\n",
        "    # all(print(x) for x in col)\n",
        "    if all((x*1.0).is_integer() for x in df[col]):\n",
        "      final_df[col] = np.round(final_df[col], 0)\n",
        "      # print(final_df[col])\n",
        "      print(final_df[col].value_counts())\n",
        "\n",
        "# print(final_df)\n",
        "final_df.head()\n"
      ],
      "id": "Hp-B6m8qSRXx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkVkqlquMVDX"
      },
      "source": [
        "Let's view both distributions to evaluate the quality of these generated values."
      ],
      "id": "LkVkqlquMVDX"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "0o2tCRcLOTao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "eabcf0d3-0182-469c-e8b6-e5d71aefc664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The distributions are not similar.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def compare_distributions(list1, list2):\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n",
        "\n",
        "    # Plot histograms\n",
        "    ax1.hist(list1, density=True, alpha=0.5)\n",
        "    ax2.hist(list2, density=True, alpha=0.5)\n",
        "\n",
        "    # Fit normal distributions\n",
        "    mu1, std1 = norm.fit(list1)\n",
        "    mu2, std2 = norm.fit(list2)\n",
        "\n",
        "    # Plot normal distribution curves\n",
        "    x1 = np.linspace(min(list1), max(list1), 100)\n",
        "    ax1.plot(x1, norm.pdf(x1, mu1, std1))\n",
        "    ax1.set_title('Original Dataset')\n",
        "    ax2.set_title('Augmented Dataset')\n",
        "    x2 = np.linspace(min(list2), max(list2), 100)\n",
        "    ax2.plot(x2, norm.pdf(x2, mu2, std2))\n",
        "\n",
        "    # Check if distributions are similar\n",
        "    if np.allclose(mu1, mu2) and np.allclose(std1, std2):\n",
        "        print('The distributions are similar.')\n",
        "    else:\n",
        "        print('The distributions are not similar.')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "compare_distributions(list(df[numerical_cols[2]]), list(new_df[numerical_cols[2]]))"
      ],
      "id": "0o2tCRcLOTao"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8av1-IvPy-t"
      },
      "source": [
        "It seems like the criteria for determining if the distributions are similar is too strict, especially given that we are working off a small dataset."
      ],
      "id": "_8av1-IvPy-t"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "jtmfU3j2P9sA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "b42f6fc8-cf2b-48a0-f5b1-d2e7915527ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL DIVERGENCE:  0.0033091043528511487\n",
            "The distributions are similar\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_distributions_tolerance(list1, list2, tolerance=0.07):\n",
        "    \"\"\"Compares the distributions of two lists and plots them side by side.\n",
        "\n",
        "    Args:\n",
        "        list1 (list): The first list to compare.\n",
        "        list2 (list): The second list to compare.\n",
        "        tolerance (float): The tolerance for accepting similarity between the distributions.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the distributions are similar, False otherwise.\n",
        "    \"\"\"\n",
        "    # Calculate the histograms for the two lists\n",
        "    hist1, bins1 = np.histogram(list1, density=True)\n",
        "    hist2, bins2 = np.histogram(list2, density=True)\n",
        "\n",
        "    # Normalize the histograms to have unit area\n",
        "    hist1 = hist1 / np.sum(hist1)\n",
        "    hist2 = hist2 / np.sum(hist2)\n",
        "\n",
        "    # Calculate the KL divergence between the two histograms\n",
        "    kl_div = stats.entropy(hist2, hist1)\n",
        "\n",
        "    # Plot the histograms side by side\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    ax[0].bar(bins1[:-1], hist1, width=np.diff(bins1), align='edge')\n",
        "    ax[1].bar(bins2[:-1], hist2, width=np.diff(bins2), align='edge')\n",
        "    ax[0].set_title('Original Dataset')\n",
        "    ax[1].set_title('Augmented Dataset')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Check if the KL divergence is within the tolerance\n",
        "    print(\"KL DIVERGENCE: \", kl_div)\n",
        "    similar = True if kl_div <= tolerance else False\n",
        "\n",
        "    if similar:\n",
        "      print(\"The distributions are similar\")\n",
        "    else:\n",
        "      print(\"The distrubutions are NOT similar\")\n",
        "    return similar\n",
        "\n",
        "\n",
        "compare_distributions_tolerance(list(df[numerical_cols[2]]), list(new_df[numerical_cols[2]]))\n",
        "# new_df.to_csv('augmented_dataset.csv', index=False)"
      ],
      "id": "jtmfU3j2P9sA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PUT IT ALL TOGETHER"
      ],
      "metadata": {
        "id": "W3ptvFuiWa7k"
      },
      "id": "W3ptvFuiWa7k"
    },
    {
      "cell_type": "code",
      "source": [
        "# def prepare_for_regression(df, categorical_cols, proper_noun_cols=None):\n",
        "#   regression_df = df.drop(columns=proper_noun_cols)\n",
        "#   regression_df = pd.get_dummies(regression_df, columns=categorical_cols)\n",
        "#   return regression_df\n",
        "\n",
        "# def separate_columns(df):\n",
        "#   numerical_cols = list()\n",
        "#   categorical_cols = list()\n",
        "#   proper_noun_cols = list()\n",
        "#   for (index, colname) in enumerate(df):\n",
        "#       if colname in df.select_dtypes(include='object').columns:\n",
        "#         unique_vals = df[colname].unique()\n",
        "#         if len(unique_vals) >= 0.85 * df.shape[0]:\n",
        "#           proper_noun_cols.append(colname)\n",
        "#         else:\n",
        "#           categorical_cols.append(colname)\n",
        "#       else:\n",
        "#         numerical_cols.append(colname)\n",
        "#   return numerical_cols, categorical_cols, proper_noun_cols\n",
        "\n",
        "# def run_data_augmenter(dataset, num_samples=None, percentage_augmentation=None):\n",
        "#     df = pd.read_csv(dataset, header=\"infer\")\n",
        "#     print(df.info())\n",
        "#     print(df)\n",
        "\n",
        "\n",
        "#     # DATA PREPROCESSING\n",
        "#     df = df.dropna(axis=1, how='all')\n",
        "#     # df = df.dropna()\n",
        "#     df = df.fillna(0)\n",
        "#     new_data = 0\n",
        "#     if num_samples != None:\n",
        "#       new_data = num_samples\n",
        "#     else:\n",
        "#       new_data = int(df.shape[0] * percentage_augmentation)\n",
        "    \n",
        "#     numerical_cols, categorical_cols, _ = separate_columns(df)\n",
        "#     regression_df = prepare_for_regression(df, categorical_cols)\n",
        "    \n",
        "#     np.seterr(invalid='warn') \n",
        "#     np.seterr(under='warn')\n",
        "    \n",
        "#     new_df = augment_dataset(regression_df, new_data)\n",
        "#     new_df = reverse_one_hot_encode(new_df, categorical_cols)\n",
        "\n",
        "    \n",
        "#     num_cat_generated = generate_categorical_variables(df, new_df)\n",
        "#     # final_df.head()\n",
        "\n",
        "#     final_df = pd.concat([df, num_cat_generated], axis=0)\n",
        "\n",
        "#     # make sure that all columns are rounded appropriately\n",
        "#     for col in final_df.columns:\n",
        "#       if col in numerical_cols:\n",
        "#         # print(col)\n",
        "#         # all(print(x) for x in col)\n",
        "#         if all((x*1.0).is_integer() for x in df[col]):\n",
        "#           final_df[col] = np.round(final_df[col], 0)\n",
        "#           # print(final_df[col])\n",
        "#           print(final_df[col].value_counts())\n",
        "    \n",
        "#     return final_df"
      ],
      "metadata": {
        "id": "3wxGi_QLUdV6"
      },
      "id": "3wxGi_QLUdV6",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkDfuWoz7FJ"
      },
      "source": [
        "##Downstream ML task\n",
        "\n",
        "To demonstrate the usefulness of this dataset augmentation, we're going to be predicting the diabetes diagnoses of patients."
      ],
      "id": "okkDfuWoz7FJ"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "8j2Gng6G0RcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d0f807-dfa8-484f-e4e9-fdb3581efdd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "609            1      111             62             13      182  24.0   \n",
            "610            3      106             54             21      158  30.9   \n",
            "611            3      174             58             22      194  32.9   \n",
            "612            7      168             88             42      321  38.2   \n",
            "613            6      105             80             28        0  32.5   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "609                     0.138   23        0  \n",
            "610                     0.292   24        0  \n",
            "611                     0.593   36        1  \n",
            "612                     0.787   40        1  \n",
            "613                     0.878   26        0  \n",
            "\n",
            "[614 rows x 9 columns]\n",
            "0      1\n",
            "1      0\n",
            "2      1\n",
            "3      0\n",
            "4      1\n",
            "      ..\n",
            "609    0\n",
            "610    0\n",
            "611    1\n",
            "612    1\n",
            "613    0\n",
            "Name: Outcome, Length: 614, dtype: int64\n",
            "Epoch 1/150\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 2.3046 - accuracy: 0.4870 - precision_6: 0.3023 - recall_6: 0.3662\n",
            "Epoch 2/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3325 - accuracy: 0.5163 - precision_6: 0.3359 - recall_6: 0.4038\n",
            "Epoch 3/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9342 - accuracy: 0.5505 - precision_6: 0.3575 - recall_6: 0.3709\n",
            "Epoch 4/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8026 - accuracy: 0.5798 - precision_6: 0.3797 - recall_6: 0.3333\n",
            "Epoch 5/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7775 - accuracy: 0.6075 - precision_6: 0.4186 - recall_6: 0.3380\n",
            "Epoch 6/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7409 - accuracy: 0.6091 - precision_6: 0.4322 - recall_6: 0.4038\n",
            "Epoch 7/150\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.7116 - accuracy: 0.6384 - precision_6: 0.4751 - recall_6: 0.4038\n",
            "Epoch 8/150\n",
            "62/62 [==============================] - 0s 5ms/step - loss: 0.6946 - accuracy: 0.6352 - precision_6: 0.4636 - recall_6: 0.3286\n",
            "Epoch 9/150\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6795 - accuracy: 0.6564 - precision_6: 0.5061 - recall_6: 0.3897\n",
            "Epoch 10/150\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6788 - accuracy: 0.6303 - precision_6: 0.4628 - recall_6: 0.4085\n",
            "Epoch 11/150\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.6562 - accuracy: 0.6547 - precision_6: 0.5032 - recall_6: 0.3662\n",
            "Epoch 12/150\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6282 - accuracy: 0.6694 - precision_6: 0.5338 - recall_6: 0.3709\n",
            "Epoch 13/150\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.6342 - accuracy: 0.6498 - precision_6: 0.4943 - recall_6: 0.4038\n",
            "Epoch 14/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.6551 - accuracy: 0.6580 - precision_6: 0.5106 - recall_6: 0.3380\n",
            "Epoch 15/150\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6275 - accuracy: 0.6580 - precision_6: 0.5102 - recall_6: 0.3521\n",
            "Epoch 16/150\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6090 - accuracy: 0.6726 - precision_6: 0.5405 - recall_6: 0.3756\n",
            "Epoch 17/150\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6229 - accuracy: 0.6792 - precision_6: 0.5426 - recall_6: 0.4789\n",
            "Epoch 18/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.7085 - precision_6: 0.6104 - recall_6: 0.4413\n",
            "Epoch 19/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5953 - accuracy: 0.6938 - precision_6: 0.5817 - recall_6: 0.4178\n",
            "Epoch 20/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5932 - accuracy: 0.7036 - precision_6: 0.6069 - recall_6: 0.4131\n",
            "Epoch 21/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5963 - accuracy: 0.6971 - precision_6: 0.5849 - recall_6: 0.4366\n",
            "Epoch 22/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.6873 - precision_6: 0.5636 - recall_6: 0.4366\n",
            "Epoch 23/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.6987 - precision_6: 0.6045 - recall_6: 0.3803\n",
            "Epoch 24/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5873 - accuracy: 0.7215 - precision_6: 0.6500 - recall_6: 0.4272\n",
            "Epoch 25/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5991 - accuracy: 0.6857 - precision_6: 0.5581 - recall_6: 0.4507\n",
            "Epoch 26/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.6922 - precision_6: 0.5845 - recall_6: 0.3897\n",
            "Epoch 27/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.7134 - precision_6: 0.6149 - recall_6: 0.4648\n",
            "Epoch 28/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.6987 - precision_6: 0.5933 - recall_6: 0.4178\n",
            "Epoch 29/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.7166 - precision_6: 0.6383 - recall_6: 0.4225\n",
            "Epoch 30/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.7264 - precision_6: 0.6398 - recall_6: 0.4836\n",
            "Epoch 31/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5666 - accuracy: 0.7166 - precision_6: 0.6309 - recall_6: 0.4413\n",
            "Epoch 32/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5691 - accuracy: 0.7085 - precision_6: 0.6049 - recall_6: 0.4601\n",
            "Epoch 33/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5622 - accuracy: 0.7248 - precision_6: 0.6692 - recall_6: 0.4085\n",
            "Epoch 34/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.7166 - precision_6: 0.6140 - recall_6: 0.4930\n",
            "Epoch 35/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.7313 - precision_6: 0.6500 - recall_6: 0.4883\n",
            "Epoch 36/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5561 - accuracy: 0.7199 - precision_6: 0.6323 - recall_6: 0.4601\n",
            "Epoch 37/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.7134 - precision_6: 0.6082 - recall_6: 0.4883\n",
            "Epoch 38/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.7248 - precision_6: 0.6447 - recall_6: 0.4601\n",
            "Epoch 39/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7199 - precision_6: 0.6289 - recall_6: 0.4695\n",
            "Epoch 40/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.7296 - precision_6: 0.6516 - recall_6: 0.4742\n",
            "Epoch 41/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5432 - accuracy: 0.7296 - precision_6: 0.6516 - recall_6: 0.4742\n",
            "Epoch 42/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5470 - accuracy: 0.7231 - precision_6: 0.6424 - recall_6: 0.4554\n",
            "Epoch 43/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.7215 - precision_6: 0.6346 - recall_6: 0.4648\n",
            "Epoch 44/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7296 - precision_6: 0.6536 - recall_6: 0.4695\n",
            "Epoch 45/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7362 - precision_6: 0.6491 - recall_6: 0.5211\n",
            "Epoch 46/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.7394 - precision_6: 0.6732 - recall_6: 0.4836\n",
            "Epoch 47/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5401 - accuracy: 0.7427 - precision_6: 0.6797 - recall_6: 0.4883\n",
            "Epoch 48/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5695 - accuracy: 0.7134 - precision_6: 0.6095 - recall_6: 0.4836\n",
            "Epoch 49/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7394 - precision_6: 0.6606 - recall_6: 0.5117\n",
            "Epoch 50/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7313 - precision_6: 0.6429 - recall_6: 0.5070\n",
            "Epoch 51/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.7541 - precision_6: 0.6824 - recall_6: 0.5446\n",
            "Epoch 52/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7410 - precision_6: 0.6627 - recall_6: 0.5164\n",
            "Epoch 53/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5368 - accuracy: 0.7215 - precision_6: 0.6382 - recall_6: 0.4554\n",
            "Epoch 54/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7345 - precision_6: 0.6562 - recall_6: 0.4930\n",
            "Epoch 55/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5446 - accuracy: 0.7329 - precision_6: 0.6450 - recall_6: 0.5117\n",
            "Epoch 56/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5424 - accuracy: 0.7345 - precision_6: 0.6623 - recall_6: 0.4789\n",
            "Epoch 57/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7427 - precision_6: 0.6667 - recall_6: 0.5164\n",
            "Epoch 58/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7459 - precision_6: 0.6727 - recall_6: 0.5211\n",
            "Epoch 59/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7394 - precision_6: 0.6646 - recall_6: 0.5023\n",
            "Epoch 60/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5267 - accuracy: 0.7362 - precision_6: 0.6604 - recall_6: 0.4930\n",
            "Epoch 61/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5230 - accuracy: 0.7459 - precision_6: 0.6610 - recall_6: 0.5493\n",
            "Epoch 62/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7524 - precision_6: 0.7103 - recall_6: 0.4836\n",
            "Epoch 63/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7362 - precision_6: 0.6564 - recall_6: 0.5023\n",
            "Epoch 64/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5293 - accuracy: 0.7378 - precision_6: 0.6566 - recall_6: 0.5117\n",
            "Epoch 65/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7345 - precision_6: 0.6866 - recall_6: 0.4319\n",
            "Epoch 66/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5437 - accuracy: 0.7264 - precision_6: 0.6316 - recall_6: 0.5070\n",
            "Epoch 67/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7378 - precision_6: 0.6585 - recall_6: 0.5070\n",
            "Epoch 68/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7443 - precision_6: 0.6818 - recall_6: 0.4930\n",
            "Epoch 69/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.7345 - precision_6: 0.6471 - recall_6: 0.5164\n",
            "Epoch 70/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.7443 - precision_6: 0.6647 - recall_6: 0.5305\n",
            "Epoch 71/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5129 - accuracy: 0.7541 - precision_6: 0.6914 - recall_6: 0.5258\n",
            "Epoch 72/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5129 - accuracy: 0.7508 - precision_6: 0.6744 - recall_6: 0.5446\n",
            "Epoch 73/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7573 - precision_6: 0.6882 - recall_6: 0.5493\n",
            "Epoch 74/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7573 - precision_6: 0.6928 - recall_6: 0.5399\n",
            "Epoch 75/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5378 - accuracy: 0.7329 - precision_6: 0.6561 - recall_6: 0.4836\n",
            "Epoch 76/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7557 - precision_6: 0.6933 - recall_6: 0.5305\n",
            "Epoch 77/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.7476 - precision_6: 0.6667 - recall_6: 0.5446\n",
            "Epoch 78/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7573 - precision_6: 0.6839 - recall_6: 0.5587\n",
            "Epoch 79/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7524 - precision_6: 0.6943 - recall_6: 0.5117\n",
            "Epoch 80/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7199 - precision_6: 0.6108 - recall_6: 0.5305\n",
            "Epoch 81/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7329 - precision_6: 0.6601 - recall_6: 0.4742\n",
            "Epoch 82/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7606 - precision_6: 0.7012 - recall_6: 0.5399\n",
            "Epoch 83/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.7427 - precision_6: 0.6571 - recall_6: 0.5399\n",
            "Epoch 84/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7394 - precision_6: 0.6688 - recall_6: 0.4930\n",
            "Epoch 85/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.7264 - precision_6: 0.6316 - recall_6: 0.5070\n",
            "Epoch 86/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.7492 - precision_6: 0.6725 - recall_6: 0.5399\n",
            "Epoch 87/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7394 - precision_6: 0.6646 - recall_6: 0.5023\n",
            "Epoch 88/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.7606 - precision_6: 0.7171 - recall_6: 0.5117\n",
            "Epoch 89/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7671 - precision_6: 0.6989 - recall_6: 0.5775\n",
            "Epoch 90/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7622 - precision_6: 0.6851 - recall_6: 0.5822\n",
            "Epoch 91/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5093 - accuracy: 0.7410 - precision_6: 0.6800 - recall_6: 0.4789\n",
            "Epoch 92/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4940 - accuracy: 0.7541 - precision_6: 0.6761 - recall_6: 0.5587\n",
            "Epoch 93/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.7557 - precision_6: 0.7032 - recall_6: 0.5117\n",
            "Epoch 94/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.7476 - precision_6: 0.6543 - recall_6: 0.5775\n",
            "Epoch 95/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4983 - accuracy: 0.7524 - precision_6: 0.7047 - recall_6: 0.4930\n",
            "Epoch 96/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7427 - precision_6: 0.6647 - recall_6: 0.5211\n",
            "Epoch 97/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5149 - accuracy: 0.7394 - precision_6: 0.6667 - recall_6: 0.4977\n",
            "Epoch 98/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.7150 - precision_6: 0.6000 - recall_6: 0.5352\n",
            "Epoch 99/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5135 - accuracy: 0.7394 - precision_6: 0.6568 - recall_6: 0.5211\n",
            "Epoch 100/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7671 - precision_6: 0.7083 - recall_6: 0.5587\n",
            "Epoch 101/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5076 - accuracy: 0.7476 - precision_6: 0.6812 - recall_6: 0.5117\n",
            "Epoch 102/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7573 - precision_6: 0.7162 - recall_6: 0.4977\n",
            "Epoch 103/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5063 - accuracy: 0.7606 - precision_6: 0.6897 - recall_6: 0.5634\n",
            "Epoch 104/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.7508 - precision_6: 0.6948 - recall_6: 0.5023\n",
            "Epoch 105/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.7687 - precision_6: 0.7178 - recall_6: 0.5493\n",
            "Epoch 106/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7557 - precision_6: 0.6780 - recall_6: 0.5634\n",
            "Epoch 107/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4985 - accuracy: 0.7736 - precision_6: 0.7102 - recall_6: 0.5869\n",
            "Epoch 108/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7720 - precision_6: 0.7517 - recall_6: 0.5117\n",
            "Epoch 109/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4953 - accuracy: 0.7638 - precision_6: 0.7152 - recall_6: 0.5305\n",
            "Epoch 110/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4856 - accuracy: 0.7736 - precision_6: 0.7256 - recall_6: 0.5587\n",
            "Epoch 111/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4910 - accuracy: 0.7508 - precision_6: 0.6724 - recall_6: 0.5493\n",
            "Epoch 112/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4962 - accuracy: 0.7638 - precision_6: 0.7000 - recall_6: 0.5587\n",
            "Epoch 113/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4947 - accuracy: 0.7557 - precision_6: 0.6957 - recall_6: 0.5258\n",
            "Epoch 114/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4933 - accuracy: 0.7557 - precision_6: 0.6842 - recall_6: 0.5493\n",
            "Epoch 115/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4968 - accuracy: 0.7638 - precision_6: 0.7179 - recall_6: 0.5258\n",
            "Epoch 116/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7508 - precision_6: 0.6852 - recall_6: 0.5211\n",
            "Epoch 117/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4911 - accuracy: 0.7508 - precision_6: 0.6667 - recall_6: 0.5634\n",
            "Epoch 118/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.7622 - precision_6: 0.7107 - recall_6: 0.5305\n",
            "Epoch 119/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.5090 - accuracy: 0.7443 - precision_6: 0.6687 - recall_6: 0.5211\n",
            "Epoch 120/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4763 - accuracy: 0.7655 - precision_6: 0.6994 - recall_6: 0.5681\n",
            "Epoch 121/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7687 - precision_6: 0.7205 - recall_6: 0.5446\n",
            "Epoch 122/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.7541 - precision_6: 0.6914 - recall_6: 0.5258\n",
            "Epoch 123/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.7443 - precision_6: 0.6707 - recall_6: 0.5164\n",
            "Epoch 124/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.7557 - precision_6: 0.6800 - recall_6: 0.5587\n",
            "Epoch 125/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.7801 - precision_6: 0.7566 - recall_6: 0.5399\n",
            "Epoch 126/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7410 - precision_6: 0.6588 - recall_6: 0.5258\n",
            "Epoch 127/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.7557 - precision_6: 0.6842 - recall_6: 0.5493\n",
            "Epoch 128/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7476 - precision_6: 0.6859 - recall_6: 0.5023\n",
            "Epoch 129/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7524 - precision_6: 0.6848 - recall_6: 0.5305\n",
            "Epoch 130/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.7573 - precision_6: 0.6860 - recall_6: 0.5540\n",
            "Epoch 131/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7671 - precision_6: 0.7160 - recall_6: 0.5446\n",
            "Epoch 132/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7459 - precision_6: 0.6770 - recall_6: 0.5117\n",
            "Epoch 133/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7541 - precision_6: 0.6782 - recall_6: 0.5540\n",
            "Epoch 134/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7345 - precision_6: 0.6582 - recall_6: 0.4883\n",
            "Epoch 135/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.7655 - precision_6: 0.7226 - recall_6: 0.5258\n",
            "Epoch 136/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4968 - accuracy: 0.7655 - precision_6: 0.6906 - recall_6: 0.5869\n",
            "Epoch 137/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.7476 - precision_6: 0.6835 - recall_6: 0.5070\n",
            "Epoch 138/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.7524 - precision_6: 0.6943 - recall_6: 0.5117\n",
            "Epoch 139/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7704 - precision_6: 0.7143 - recall_6: 0.5634\n",
            "Epoch 140/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7524 - precision_6: 0.6805 - recall_6: 0.5399\n",
            "Epoch 141/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.7590 - precision_6: 0.6923 - recall_6: 0.5493\n",
            "Epoch 142/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.7704 - precision_6: 0.7195 - recall_6: 0.5540\n",
            "Epoch 143/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.7606 - precision_6: 0.6988 - recall_6: 0.5446\n",
            "Epoch 144/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7541 - precision_6: 0.6845 - recall_6: 0.5399\n",
            "Epoch 145/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.7541 - precision_6: 0.6782 - recall_6: 0.5540\n",
            "Epoch 146/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.7687 - precision_6: 0.7261 - recall_6: 0.5352\n",
            "Epoch 147/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.7590 - precision_6: 0.6946 - recall_6: 0.5446\n",
            "Epoch 148/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4788 - accuracy: 0.7655 - precision_6: 0.6994 - recall_6: 0.5681\n",
            "Epoch 149/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7622 - precision_6: 0.7030 - recall_6: 0.5446\n",
            "Epoch 150/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7541 - precision_6: 0.6890 - recall_6: 0.5305\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.5541 - accuracy: 0.7143 - precision_6: 0.6170 - recall_6: 0.5273\n",
            "Accuracy: 71.43%\n",
            "Precision: 61.70%\n",
            "Recall: 52.73%\n"
          ]
        }
      ],
      "source": [
        "augmented_train_data = final_df\n",
        "\n",
        "# original dataset train data is stored in train data\n",
        "# original dataset test data is stored in test data\n",
        "\n",
        "# augmented dataset train data is stored in final_df\n",
        "# augmented dataset test data is stored in test data\n",
        "\n",
        "if 'id' in train_data.columns:\n",
        "  train_data = train_data.drop(columns=['id'])\n",
        "\n",
        "if 'id' in test_data.columns:\n",
        "  test_data = test_data.drop(columns=['id'])\n",
        "\n",
        "if 'id' in final_df.columns:\n",
        "  augmented_train_data = final_df.drop(columns=['id'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "experiments = []\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "train_percentages=[]\n",
        "test_percentages=[]\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras.metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "# data = pd.read_csv('path/to/dataset.csv')\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "## UNCOMMENT THIS\n",
        "# test_size = 0.2\n",
        "# train_data, test_data = train_test_split(eval_original_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = train_data.iloc[:, 1:], train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "train_X, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_data)\n",
        "# print(test_y)\n",
        "print(train_y)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "experiments.append(\"Original dataset\")\n",
        "accuracies.append(accuracy)\n",
        "precisions.append(precision)\n",
        "recalls.append(recall)\n",
        "\n"
      ],
      "id": "8j2Gng6G0RcV"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "woXDJmn21s5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80ebd53-8bec-4ebe-910a-c29983a96335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      1.0\n",
            "1      0.0\n",
            "2      1.0\n",
            "3      0.0\n",
            "4      1.0\n",
            "      ... \n",
            "609    0.0\n",
            "610    1.0\n",
            "611    0.0\n",
            "612    0.0\n",
            "613    1.0\n",
            "Name: Outcome, Length: 1228, dtype: float64\n",
            "Epoch 1/150\n",
            "123/123 [==============================] - 1s 2ms/step - loss: 1.2539 - accuracy: 0.6710 - precision_7: 0.5026 - recall_7: 0.7224\n",
            "Epoch 2/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5947 - accuracy: 0.7410 - precision_7: 0.6052 - recall_7: 0.6290\n",
            "Epoch 3/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5436 - accuracy: 0.7557 - precision_7: 0.6314 - recall_7: 0.6314\n",
            "Epoch 4/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7573 - precision_7: 0.6423 - recall_7: 0.6044\n",
            "Epoch 5/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7671 - precision_7: 0.6571 - recall_7: 0.6216\n",
            "Epoch 6/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7663 - precision_7: 0.6554 - recall_7: 0.6216\n",
            "Epoch 7/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.7704 - precision_7: 0.6640 - recall_7: 0.6216\n",
            "Epoch 8/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.7761 - precision_7: 0.6675 - recall_7: 0.6462\n",
            "Epoch 9/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.7866 - precision_7: 0.6913 - recall_7: 0.6437\n",
            "Epoch 10/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4554 - accuracy: 0.7858 - precision_7: 0.6967 - recall_7: 0.6265\n",
            "Epoch 11/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.8005 - precision_7: 0.7132 - recall_7: 0.6658\n",
            "Epoch 12/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.8151 - precision_7: 0.7381 - recall_7: 0.6855\n",
            "Epoch 13/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.8111 - precision_7: 0.7297 - recall_7: 0.6830\n",
            "Epoch 14/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4307 - accuracy: 0.8054 - precision_7: 0.7270 - recall_7: 0.6609\n",
            "Epoch 15/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4270 - accuracy: 0.8070 - precision_7: 0.7310 - recall_7: 0.6609\n",
            "Epoch 16/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8208 - precision_7: 0.7429 - recall_7: 0.7027\n",
            "Epoch 17/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8192 - precision_7: 0.7441 - recall_7: 0.6929\n",
            "Epoch 18/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8265 - precision_7: 0.7622 - recall_7: 0.6929\n",
            "Epoch 19/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8282 - precision_7: 0.7634 - recall_7: 0.6978\n",
            "Epoch 20/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.4021 - accuracy: 0.8355 - precision_7: 0.7704 - recall_7: 0.7174\n",
            "Epoch 21/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3983 - accuracy: 0.8331 - precision_7: 0.7686 - recall_7: 0.7101\n",
            "Epoch 22/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3953 - accuracy: 0.8339 - precision_7: 0.7707 - recall_7: 0.7101\n",
            "Epoch 23/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.8355 - precision_7: 0.7662 - recall_7: 0.7248\n",
            "Epoch 24/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3815 - accuracy: 0.8404 - precision_7: 0.7844 - recall_7: 0.7150\n",
            "Epoch 25/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8420 - precision_7: 0.7656 - recall_7: 0.7543\n",
            "Epoch 26/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3801 - accuracy: 0.8371 - precision_7: 0.7661 - recall_7: 0.7322\n",
            "Epoch 27/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8436 - precision_7: 0.7792 - recall_7: 0.7371\n",
            "Epoch 28/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.8461 - precision_7: 0.7766 - recall_7: 0.7518\n",
            "Epoch 29/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8493 - precision_7: 0.7861 - recall_7: 0.7494\n",
            "Epoch 30/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8502 - precision_7: 0.7852 - recall_7: 0.7543\n",
            "Epoch 31/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8436 - precision_7: 0.7749 - recall_7: 0.7445\n",
            "Epoch 32/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8485 - precision_7: 0.7715 - recall_7: 0.7715\n",
            "Epoch 33/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3637 - accuracy: 0.8583 - precision_7: 0.7920 - recall_7: 0.7764\n",
            "Epoch 34/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3671 - accuracy: 0.8493 - precision_7: 0.7906 - recall_7: 0.7420\n",
            "Epoch 35/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.8518 - precision_7: 0.7848 - recall_7: 0.7617\n",
            "Epoch 36/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8518 - precision_7: 0.7848 - recall_7: 0.7617\n",
            "Epoch 37/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3610 - accuracy: 0.8518 - precision_7: 0.7877 - recall_7: 0.7568\n",
            "Epoch 38/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.8591 - precision_7: 0.8063 - recall_7: 0.7568\n",
            "Epoch 39/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.8567 - precision_7: 0.7969 - recall_7: 0.7617\n",
            "Epoch 40/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3625 - accuracy: 0.8518 - precision_7: 0.7863 - recall_7: 0.7592\n",
            "Epoch 41/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8428 - precision_7: 0.7716 - recall_7: 0.7469\n",
            "Epoch 42/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3588 - accuracy: 0.8485 - precision_7: 0.7742 - recall_7: 0.7666\n",
            "Epoch 43/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.8510 - precision_7: 0.7917 - recall_7: 0.7469\n",
            "Epoch 44/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8591 - precision_7: 0.7910 - recall_7: 0.7813\n",
            "Epoch 45/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.8526 - precision_7: 0.7912 - recall_7: 0.7543\n",
            "Epoch 46/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3534 - accuracy: 0.8510 - precision_7: 0.7843 - recall_7: 0.7592\n",
            "Epoch 47/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8477 - precision_7: 0.7895 - recall_7: 0.7371\n",
            "Epoch 48/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3483 - accuracy: 0.8510 - precision_7: 0.7745 - recall_7: 0.7764\n",
            "Epoch 49/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3487 - accuracy: 0.8632 - precision_7: 0.8072 - recall_7: 0.7715\n",
            "Epoch 50/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8567 - precision_7: 0.7969 - recall_7: 0.7617\n",
            "Epoch 51/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3484 - accuracy: 0.8534 - precision_7: 0.7888 - recall_7: 0.7617\n",
            "Epoch 52/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3427 - accuracy: 0.8575 - precision_7: 0.8005 - recall_7: 0.7592\n",
            "Epoch 53/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.8559 - precision_7: 0.8042 - recall_7: 0.7469\n",
            "Epoch 54/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.8567 - precision_7: 0.7924 - recall_7: 0.7690\n",
            "Epoch 55/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3455 - accuracy: 0.8550 - precision_7: 0.7913 - recall_7: 0.7641\n",
            "Epoch 56/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8518 - precision_7: 0.7953 - recall_7: 0.7445\n",
            "Epoch 57/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3474 - accuracy: 0.8583 - precision_7: 0.7935 - recall_7: 0.7740\n",
            "Epoch 58/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8607 - precision_7: 0.8073 - recall_7: 0.7617\n",
            "Epoch 59/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3517 - accuracy: 0.8461 - precision_7: 0.7899 - recall_7: 0.7297\n",
            "Epoch 60/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8575 - precision_7: 0.7990 - recall_7: 0.7617\n",
            "Epoch 61/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8567 - precision_7: 0.7954 - recall_7: 0.7641\n",
            "Epoch 62/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8632 - precision_7: 0.8056 - recall_7: 0.7740\n",
            "Epoch 63/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3357 - accuracy: 0.8575 - precision_7: 0.7990 - recall_7: 0.7617\n",
            "Epoch 64/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8632 - precision_7: 0.8104 - recall_7: 0.7666\n",
            "Epoch 65/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8591 - precision_7: 0.8000 - recall_7: 0.7666\n",
            "Epoch 66/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8616 - precision_7: 0.8110 - recall_7: 0.7592\n",
            "Epoch 67/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.8624 - precision_7: 0.8036 - recall_7: 0.7740\n",
            "Epoch 68/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8599 - precision_7: 0.8052 - recall_7: 0.7617\n",
            "Epoch 69/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8575 - precision_7: 0.8053 - recall_7: 0.7518\n",
            "Epoch 70/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8583 - precision_7: 0.7980 - recall_7: 0.7666\n",
            "Epoch 71/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3341 - accuracy: 0.8575 - precision_7: 0.8005 - recall_7: 0.7592\n",
            "Epoch 72/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3263 - accuracy: 0.8673 - precision_7: 0.8228 - recall_7: 0.7641\n",
            "Epoch 73/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8624 - precision_7: 0.8067 - recall_7: 0.7690\n",
            "Epoch 74/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3348 - accuracy: 0.8697 - precision_7: 0.8175 - recall_7: 0.7813\n",
            "Epoch 75/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3294 - accuracy: 0.8648 - precision_7: 0.8098 - recall_7: 0.7740\n",
            "Epoch 76/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8607 - precision_7: 0.8026 - recall_7: 0.7690\n",
            "Epoch 77/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8681 - precision_7: 0.8165 - recall_7: 0.7764\n",
            "Epoch 78/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3353 - accuracy: 0.8705 - precision_7: 0.8212 - recall_7: 0.7789\n",
            "Epoch 79/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8616 - precision_7: 0.8078 - recall_7: 0.7641\n",
            "Epoch 80/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8705 - precision_7: 0.8147 - recall_7: 0.7887\n",
            "Epoch 81/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8689 - precision_7: 0.8106 - recall_7: 0.7887\n",
            "Epoch 82/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8681 - precision_7: 0.8182 - recall_7: 0.7740\n",
            "Epoch 83/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8632 - precision_7: 0.8025 - recall_7: 0.7789\n",
            "Epoch 84/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3282 - accuracy: 0.8624 - precision_7: 0.8099 - recall_7: 0.7641\n",
            "Epoch 85/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8575 - precision_7: 0.8021 - recall_7: 0.7568\n",
            "Epoch 86/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8673 - precision_7: 0.8297 - recall_7: 0.7543\n",
            "Epoch 87/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8632 - precision_7: 0.8056 - recall_7: 0.7740\n",
            "Epoch 88/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.8697 - precision_7: 0.8276 - recall_7: 0.7666\n",
            "Epoch 89/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3190 - accuracy: 0.8624 - precision_7: 0.8067 - recall_7: 0.7690\n",
            "Epoch 90/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8583 - precision_7: 0.8026 - recall_7: 0.7592\n",
            "Epoch 91/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3204 - accuracy: 0.8640 - precision_7: 0.8093 - recall_7: 0.7715\n",
            "Epoch 92/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3205 - accuracy: 0.8648 - precision_7: 0.8179 - recall_7: 0.7617\n",
            "Epoch 93/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3147 - accuracy: 0.8738 - precision_7: 0.8298 - recall_7: 0.7789\n",
            "Epoch 94/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8705 - precision_7: 0.8263 - recall_7: 0.7715\n",
            "Epoch 95/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8738 - precision_7: 0.8333 - recall_7: 0.7740\n",
            "Epoch 96/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8713 - precision_7: 0.8217 - recall_7: 0.7813\n",
            "Epoch 97/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8640 - precision_7: 0.8125 - recall_7: 0.7666\n",
            "Epoch 98/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8770 - precision_7: 0.8441 - recall_7: 0.7715\n",
            "Epoch 99/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8624 - precision_7: 0.8083 - recall_7: 0.7666\n",
            "Epoch 100/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.8754 - precision_7: 0.8396 - recall_7: 0.7715\n",
            "Epoch 101/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8795 - precision_7: 0.8453 - recall_7: 0.7789\n",
            "Epoch 102/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8705 - precision_7: 0.8263 - recall_7: 0.7715\n",
            "Epoch 103/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8713 - precision_7: 0.8268 - recall_7: 0.7740\n",
            "Epoch 104/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8721 - precision_7: 0.8342 - recall_7: 0.7666\n",
            "Epoch 105/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8705 - precision_7: 0.8229 - recall_7: 0.7764\n",
            "Epoch 106/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8705 - precision_7: 0.8316 - recall_7: 0.7641\n",
            "Epoch 107/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8705 - precision_7: 0.8298 - recall_7: 0.7666\n",
            "Epoch 108/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.8673 - precision_7: 0.8228 - recall_7: 0.7641\n",
            "Epoch 109/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3095 - accuracy: 0.8762 - precision_7: 0.8346 - recall_7: 0.7813\n",
            "Epoch 110/150\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 0.3015 - accuracy: 0.8713 - precision_7: 0.8356 - recall_7: 0.7617\n",
            "Epoch 111/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8705 - precision_7: 0.8212 - recall_7: 0.7789\n",
            "Epoch 112/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.8705 - precision_7: 0.8351 - recall_7: 0.7592\n",
            "Epoch 113/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3085 - accuracy: 0.8689 - precision_7: 0.8220 - recall_7: 0.7715\n",
            "Epoch 114/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3055 - accuracy: 0.8746 - precision_7: 0.8338 - recall_7: 0.7764\n",
            "Epoch 115/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3137 - accuracy: 0.8697 - precision_7: 0.8241 - recall_7: 0.7715\n",
            "Epoch 116/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3082 - accuracy: 0.8705 - precision_7: 0.8212 - recall_7: 0.7789\n",
            "Epoch 117/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8673 - precision_7: 0.8245 - recall_7: 0.7617\n",
            "Epoch 118/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.8762 - precision_7: 0.8382 - recall_7: 0.7764\n",
            "Epoch 119/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3171 - accuracy: 0.8705 - precision_7: 0.8316 - recall_7: 0.7641\n",
            "Epoch 120/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3051 - accuracy: 0.8697 - precision_7: 0.8159 - recall_7: 0.7838\n",
            "Epoch 121/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3027 - accuracy: 0.8795 - precision_7: 0.8435 - recall_7: 0.7813\n",
            "Epoch 122/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.8681 - precision_7: 0.8267 - recall_7: 0.7617\n",
            "Epoch 123/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3031 - accuracy: 0.8730 - precision_7: 0.8383 - recall_7: 0.7641\n",
            "Epoch 124/150\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 0.3028 - accuracy: 0.8762 - precision_7: 0.8346 - recall_7: 0.7813\n",
            "Epoch 125/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3095 - accuracy: 0.8779 - precision_7: 0.8464 - recall_7: 0.7715\n",
            "Epoch 126/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3025 - accuracy: 0.8689 - precision_7: 0.8237 - recall_7: 0.7690\n",
            "Epoch 127/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.2980 - accuracy: 0.8746 - precision_7: 0.8373 - recall_7: 0.7715\n",
            "Epoch 128/150\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 0.3015 - accuracy: 0.8762 - precision_7: 0.8400 - recall_7: 0.7740\n",
            "Epoch 129/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.2987 - accuracy: 0.8746 - precision_7: 0.8303 - recall_7: 0.7813\n",
            "Epoch 130/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3089 - accuracy: 0.8648 - precision_7: 0.8146 - recall_7: 0.7666\n",
            "Epoch 131/150\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 0.2988 - accuracy: 0.8664 - precision_7: 0.8257 - recall_7: 0.7568\n",
            "Epoch 132/150\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 0.2955 - accuracy: 0.8836 - precision_7: 0.8455 - recall_7: 0.7936\n",
            "Epoch 133/150\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 0.3035 - accuracy: 0.8673 - precision_7: 0.8245 - recall_7: 0.7617\n",
            "Epoch 134/150\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 0.2984 - accuracy: 0.8713 - precision_7: 0.8285 - recall_7: 0.7715\n",
            "Epoch 135/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3066 - accuracy: 0.8746 - precision_7: 0.8186 - recall_7: 0.7985\n",
            "Epoch 136/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8713 - precision_7: 0.8268 - recall_7: 0.7740\n",
            "Epoch 137/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.2942 - accuracy: 0.8730 - precision_7: 0.8311 - recall_7: 0.7740\n",
            "Epoch 138/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.2978 - accuracy: 0.8738 - precision_7: 0.8369 - recall_7: 0.7690\n",
            "Epoch 139/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.2929 - accuracy: 0.8705 - precision_7: 0.8280 - recall_7: 0.7690\n",
            "Epoch 140/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.8779 - precision_7: 0.8355 - recall_7: 0.7862\n",
            "Epoch 141/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.2891 - accuracy: 0.8746 - precision_7: 0.8303 - recall_7: 0.7813\n",
            "Epoch 142/150\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.8754 - precision_7: 0.8451 - recall_7: 0.7641\n",
            "Epoch 143/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8713 - precision_7: 0.8251 - recall_7: 0.7764\n",
            "Epoch 144/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.8721 - precision_7: 0.8324 - recall_7: 0.7690\n",
            "Epoch 145/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.8787 - precision_7: 0.8413 - recall_7: 0.7813\n",
            "Epoch 146/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.2996 - accuracy: 0.8803 - precision_7: 0.8421 - recall_7: 0.7862\n",
            "Epoch 147/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.8746 - precision_7: 0.8391 - recall_7: 0.7690\n",
            "Epoch 148/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.8721 - precision_7: 0.8324 - recall_7: 0.7690\n",
            "Epoch 149/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.8697 - precision_7: 0.8276 - recall_7: 0.7666\n",
            "Epoch 150/150\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.2900 - accuracy: 0.8721 - precision_7: 0.8307 - recall_7: 0.7715\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.5237 - accuracy: 0.7532 - precision_7: 0.6667 - recall_7: 0.6182\n",
            "Accuracy: 75.32%\n",
            "Precision: 66.67%\n",
            "Recall: 61.82%\n"
          ]
        }
      ],
      "source": [
        "# test_size = 5/9.0\n",
        "# train_data, test_data = train_test_split(eval_augmented_df, test_size=test_size, shuffle=False)\n",
        "\n",
        "# Separate the target variable from the features in the train and test sets\n",
        "# train_X, train_y = augmented_train_data.iloc[:, 1:], augmented_train_data.iloc[:, 0]\n",
        "# test_X, test_y = test_data.iloc[:, 1:], test_data.iloc[:, 0]\n",
        "\n",
        "\n",
        "# last column\n",
        "train_X, train_y = augmented_train_data.iloc[:, :-1], augmented_train_data.iloc[:, -1]\n",
        "test_X, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
        "\n",
        "print(train_y)\n",
        "# print(test_data)\n",
        "# Encode the target variable\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.transform(test_y)\n",
        "\n",
        "# Create the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=train_X.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "# Fit the model to the train set\n",
        "model.fit(train_X, train_y, epochs=150, batch_size=10)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, accuracy, precision, recall = model.evaluate(test_X, test_y)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))\n",
        "print('Precision: %.2f%%' % (precision*100))\n",
        "print('Recall: %.2f%%' % (recall*100))\n",
        "\n",
        "\n",
        "experiments.append(\"Augmented dataset\")\n",
        "accuracies.append(accuracy)\n",
        "precisions.append(precision)\n",
        "recalls.append(recall)\n"
      ],
      "id": "woXDJmn21s5Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experimental Results Summary"
      ],
      "metadata": {
        "id": "SYWi6swLnJY5"
      },
      "id": "SYWi6swLnJY5"
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"Accuracy\": accuracies,\n",
        "    \"Precision\": precisions,\n",
        "    \"Recall\": recalls\n",
        "}\n",
        "\n",
        "# Create a Pandas DataFrame from the dictionary\n",
        "results = pd.DataFrame(data, index=experiments)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H15UjTM1h-C",
        "outputId": "349ae9e6-db4d-4237-e7ee-e6cca02783d8"
      },
      "id": "3H15UjTM1h-C",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   Accuracy  Precision    Recall\n",
            "Original dataset   0.714286   0.617021  0.527273\n",
            "Augmented dataset  0.753247   0.666667  0.618182\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}